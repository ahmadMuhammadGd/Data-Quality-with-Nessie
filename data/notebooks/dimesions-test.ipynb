{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/28 01:17:49 WARN Utils: Your hostname, ahmad-HP-ZBook-17 resolves to a loopback address: 127.0.1.1; using 192.168.1.35 instead (on interface wlo1)\n",
      "24/08/28 01:17:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/28 01:17:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DIMs\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------+----------+-------------+------------------+--------+----+--------------+---+--------+------+-----------+--------------+----------------+------------+-----+------------+----+--------+\n",
      "|           Order_ID|    Date|              Status|Fulfilment|ORDERS_Channel|ship_service_level|Category|Size|Courier_Status|Qty|currency|Amount|  ship_city|    ship_state|ship_postal_code|ship_country|  B2B|fulfilled_by| New|PendingS|\n",
      "+-------------------+--------+--------------------+----------+-------------+------------------+--------+----+--------------+---+--------+------+-----------+--------------+----------------+------------+-----+------------+----+--------+\n",
      "|405-8078784-5731545|04-30-22|           Cancelled|  Merchant|    Amazon.in|          Standard| T-shirt|   S|    On the Way|  0|     INR|647.62|     MUMBAI|   MAHARASHTRA|          400081|          IN|false|   Easy Ship|NULL|    NULL|\n",
      "|171-9198151-1101146|04-30-22|Shipped - Deliver...|  Merchant|    Amazon.in|          Standard|   Shirt| 3XL|       Shipped|  1|     INR| 406.0|  BENGALURU|     KARNATAKA|          560085|          IN|false|   Easy Ship|NULL|    NULL|\n",
      "|404-0687676-7273146|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited|   Shirt|  XL|       Shipped|  1|     INR| 329.0|NAVI MUMBAI|   MAHARASHTRA|          410210|          IN| true|        NULL|NULL|    NULL|\n",
      "|403-9615377-8133951|04-30-22|           Cancelled|  Merchant|    Amazon.in|          Standard| Blazzer|   L|    On the Way|  0|     INR|753.33| PUDUCHERRY|    PUDUCHERRY|          605008|          IN|false|   Easy Ship|NULL|    NULL|\n",
      "|407-1069790-7240320|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited|Trousers| 3XL|       Shipped|  1|     INR| 574.0|    CHENNAI|    TAMIL NADU|          600073|          IN|false|        NULL|NULL|    NULL|\n",
      "|404-1490984-4578765|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited| T-shirt|  XL|       Shipped|  1|     INR| 824.0|  GHAZIABAD| UTTAR PRADESH|          201102|          IN|false|        NULL|NULL|    NULL|\n",
      "|408-5748499-6859555|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited| T-shirt|   L|       Shipped|  1|     INR| 653.0| CHANDIGARH|    CHANDIGARH|          160036|          IN|false|        NULL|NULL|    NULL|\n",
      "|406-7807733-3785945|04-30-22|Shipped - Deliver...|  Merchant|    Amazon.in|          Standard|   Shirt|   S|       Shipped|  1|     INR| 399.0|  HYDERABAD|     TELANGANA|          500032|          IN|false|   Easy Ship|NULL|    NULL|\n",
      "|407-5443024-5233168|04-30-22|           Cancelled|    Amazon|    Amazon.in|         Expedited| T-shirt| 3XL|     Cancelled|  0|    NULL|  NULL|  HYDERABAD|     TELANGANA|          500008|          IN|false|        NULL|NULL|    NULL|\n",
      "|402-4393761-0311520|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited|   Shirt| XXL|       Shipped|  1|     INR| 363.0|    Chennai|    TAMIL NADU|          600041|          IN|false|        NULL|NULL|    NULL|\n",
      "|407-5633625-6970741|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited|   Shirt|   S|       Shipped|  1|     INR| 685.0|    CHENNAI|    TAMIL NADU|          600073|          IN|false|        NULL|NULL|    NULL|\n",
      "|171-4638481-6326716|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited|   Shirt|  XS|       Shipped|  1|     INR| 364.0|      NOIDA| UTTAR PRADESH|          201303|          IN|false|        NULL|NULL|    NULL|\n",
      "|405-5513694-8146768|04-30-22|Shipped - Deliver...|  Merchant|    Amazon.in|          Standard|   Shirt|  XS|       Shipped|  1|     INR| 399.0|  Amravati.|   MAHARASHTRA|          444606|          IN|false|   Easy Ship|NULL|    NULL|\n",
      "|408-7955685-3083534|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited| T-shirt|  XS|       Shipped|  1|     INR| 657.0|     MUMBAI|   MAHARASHTRA|          400053|          IN|false|        NULL|NULL|    NULL|\n",
      "|408-1298370-1920302|04-30-22|Shipped - Deliver...|  Merchant|    Amazon.in|          Standard| T-shirt|   L|       Shipped|  1|     INR| 771.0|     MUMBAI|   MAHARASHTRA|          400053|          IN|false|   Easy Ship|NULL|    NULL|\n",
      "|403-4965581-9520319|04-30-22|Shipped - Deliver...|  Merchant|    Amazon.in|          Standard|   Shirt| 6XL|       Shipped|  1|     INR| 544.0|   GUNTAKAL|ANDHRA PRADESH|          515801|          IN|false|   Easy Ship|NULL|    NULL|\n",
      "|406-9379318-6555504|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited|   Shirt| XXL|       Shipped|  1|     INR| 329.0|     JAIPUR|     RAJASTHAN|          302020|          IN|false|        NULL|NULL|    NULL|\n",
      "|405-9013803-8009918|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited|   Shirt|  XL|       Shipped|  1|     INR| 399.0|  NEW DELHI|         DELHI|          110074|          IN|false|        NULL|NULL|    NULL|\n",
      "|402-4030358-5835511|04-30-22|Shipped - Deliver...|  Merchant|    Amazon.in|          Standard|   Shirt| XXL|       Shipped|  1|     INR| 458.0|    Gurgaon|       HARYANA|          122004|          IN|false|   Easy Ship|NULL|    NULL|\n",
      "|405-5957858-1051546|04-30-22|             Shipped|    Amazon|    Amazon.in|         Expedited| T-shirt|  XS|       Shipped|  1|     INR| 886.0|  BENGALURU|     KARNATAKA|          560017|          IN|false|        NULL|NULL|    NULL|\n",
      "+-------------------+--------+--------------------+----------+-------------+------------------+--------+----+--------------+---+--------+------+-----------+--------------+----------------+------------+-----+------------+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"../original_dataset/Amazon Sale Report.csv\").drop('index')\n",
    "new_columns = [col.replace(\" \", \"_\").replace(\"-\", \"_\") for col in df.columns]\n",
    "df = df.toDF(*new_columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Order_ID',\n",
       " 'Date',\n",
       " 'Status',\n",
       " 'Fulfilment',\n",
       " 'ORDERS_Channel',\n",
       " 'ship_service_level',\n",
       " 'Category',\n",
       " 'Size',\n",
       " 'Courier_Status',\n",
       " 'Qty',\n",
       " 'currency',\n",
       " 'Amount',\n",
       " 'ship_city',\n",
       " 'ship_state',\n",
       " 'ship_postal_code',\n",
       " 'ship_country',\n",
       " 'B2B',\n",
       " 'fulfilled_by',\n",
       " 'New',\n",
       " 'PendingS']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr \n",
    "df= df.withColumn('Date', expr(f\"to_date({'Date'}, 'mm-dd-yy')\")).withColumnRenamed(\"date\", \"order_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"source_table\")\n",
    "source_name = \"source_table\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets normalize this table\n",
    "['Order_ID', # fact_dim\n",
    " 'Date',    # date_dim\n",
    " 'Status',  # shipping_dim (SCD)\n",
    " 'Fulfilment',  # shipping_dim \n",
    " 'ORDERS_Channel', # ORDERS_channel_dim\n",
    " 'ship_service_level', # shipping_dim\n",
    " 'Category',    #product_dim\n",
    " 'Size',    #product_dim\n",
    " 'Courier_Status',\n",
    " 'Qty', #fact_dim\n",
    " 'currency',    #currency_dim\n",
    " 'Amount',  #fact_dim\n",
    " 'ship_city',   #location_dim\n",
    " 'ship_state',  #location_dim\n",
    " 'ship_postal_code', #location_dim\n",
    " 'ship_country', #location_dim\n",
    " 'B2B', # fact_dim\n",
    " 'fulfilled_by',    # shipping_dim \n",
    " 'New',    \n",
    " 'PendingS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# location dim test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/28 01:47:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:47:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:47:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:47:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:47:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:47:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------+--------------------+----------------+\n",
      "|location_id|ship_country|ship_state|           ship_city|ship_postal_code|\n",
      "+-----------+------------+----------+--------------------+----------------+\n",
      "|          1|        NULL|      NULL|                NULL|            NULL|\n",
      "|          2|          IN|     DELHI|           NEW DELHI|          110001|\n",
      "|          3|          IN|     DELHI|               Delhi|          110001|\n",
      "|          4|          IN|     DELHI|           New Delhi|          110001|\n",
      "|          5|          IN|     delhi|           NEW DELHI|          110001|\n",
      "|          6|          IN|     Delhi|           New Delhi|          110001|\n",
      "|          7|          IN|     DELHI|           NEW DELHI|          110002|\n",
      "|          8|          IN|     Delhi|           New Delhi|          110002|\n",
      "|          9|          IN|     DELHI|               delhi|          110002|\n",
      "|         10|          IN|     DELHI|           NEW DELHI|          110003|\n",
      "|         11|          IN|     DELHI|           New Delhi|          110003|\n",
      "|         12|          IN| New Delhi|           NEW DELHI|          110003|\n",
      "|         13|          IN|     DELHI|           New delhi|          110003|\n",
      "|         14|          IN|     Delhi|           New Delhi|          110003|\n",
      "|         15|          IN|     DELHI|           NEW DELHI|          110004|\n",
      "|         16|          IN|     DELHI|           New delhi|          110005|\n",
      "|         17|          IN|     DELHI|           NEW DELHI|          110005|\n",
      "|         18|          IN|     DELHI|           New Delhi|          110005|\n",
      "|         19|          IN|     Delhi|joshi road karol ...|          110005|\n",
      "|         20|          IN|     DELHI|           new delhi|          110005|\n",
      "+-----------+------------+----------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    WITH location_dim AS\n",
    "    (\n",
    "        SELECT\n",
    "            DISTINCT \n",
    "            ship_country,\n",
    "            ship_state,\n",
    "            ship_city,\n",
    "            ship_postal_code\n",
    "        FROM\n",
    "            {source_name}\n",
    "    )\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER (ORDER BY ship_postal_code) AS location_id,\n",
    "        ship_country,\n",
    "        ship_state,\n",
    "        ship_city,\n",
    "        ship_postal_code\n",
    "    FROM\n",
    "        location_dim\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# date dim test query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/28 01:51:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:51:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:51:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/08/28 01:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+-----+----+---+----+\n",
      "|date_id|order_date|quarter|month|week|day|year|\n",
      "+-------+----------+-------+-----+----+---+----+\n",
      "|      1|2022-01-01|      1|    1|  52|  1|2022|\n",
      "|      2|2022-01-02|      1|    1|  52|  2|2022|\n",
      "|      3|2022-01-03|      1|    1|   1|  3|2022|\n",
      "|      4|2022-01-04|      1|    1|   1|  4|2022|\n",
      "|      5|2022-01-05|      1|    1|   1|  5|2022|\n",
      "|      6|2022-01-06|      1|    1|   1|  6|2022|\n",
      "|      7|2022-01-07|      1|    1|   1|  7|2022|\n",
      "|      8|2022-01-08|      1|    1|   1|  8|2022|\n",
      "|      9|2022-01-09|      1|    1|   1|  9|2022|\n",
      "|     10|2022-01-10|      1|    1|   2| 10|2022|\n",
      "|     11|2022-01-11|      1|    1|   2| 11|2022|\n",
      "|     12|2022-01-12|      1|    1|   2| 12|2022|\n",
      "|     13|2022-01-13|      1|    1|   2| 13|2022|\n",
      "|     14|2022-01-14|      1|    1|   2| 14|2022|\n",
      "|     15|2022-01-15|      1|    1|   2| 15|2022|\n",
      "|     16|2022-01-16|      1|    1|   2| 16|2022|\n",
      "|     17|2022-01-17|      1|    1|   3| 17|2022|\n",
      "|     18|2022-01-18|      1|    1|   3| 18|2022|\n",
      "|     19|2022-01-19|      1|    1|   3| 19|2022|\n",
      "|     20|2022-01-20|      1|    1|   3| 20|2022|\n",
      "+-------+----------+-------+-----+----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    WITH date_dim AS (\n",
    "        SELECT\n",
    "            DISTINCT \n",
    "            order_date\n",
    "        FROM\n",
    "            {source_name}\n",
    "        WHERE\n",
    "            order_date IS NOT NULL\n",
    "    )\n",
    "    SELECT\n",
    "        ROW_NUMBER() OVER (ORDER BY order_date) AS date_id,\n",
    "        order_date,\n",
    "        EXTRACT(QUARTER FROM order_date) AS quarter,\n",
    "        EXTRACT(MONTH FROM order_date) AS month,\n",
    "        EXTRACT(WEEK FROM order_date) AS week,\n",
    "        EXTRACT(DAY FROM order_date) AS day,\n",
    "        EXTRACT(YEAR FROM order_date) AS year\n",
    "    FROM\n",
    "        date_dim\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shipping dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:==========================================================(5 + 0) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+------------+\n",
      "|              Status|Fulfilment|ship_service_level|fulfilled_by|\n",
      "+--------------------+----------+------------------+------------+\n",
      "|Shipped - Lost in...|  Merchant|          Standard|   Easy Ship|\n",
      "|Shipped - Rejecte...|  Merchant|          Standard|   Easy Ship|\n",
      "|Shipped - Returni...|  Merchant|          Standard|   Easy Ship|\n",
      "|Shipped - Returne...|  Merchant|          Standard|   Easy Ship|\n",
      "|Shipped - Deliver...|  Merchant|          Standard|   Easy Ship|\n",
      "|Shipped - Out for...|  Merchant|          Standard|   Easy Ship|\n",
      "|           Cancelled|  Merchant|          Standard|   Easy Ship|\n",
      "| Shipped - Picked Up|  Merchant|          Standard|   Easy Ship|\n",
      "|   Shipped - Damaged|  Merchant|          Standard|   Easy Ship|\n",
      "|Pending - Waiting...|  Merchant|          Standard|   Easy Ship|\n",
      "|             Pending|  Merchant|          Standard|   Easy Ship|\n",
      "|             Shipped|    Amazon|         Expedited|        NULL|\n",
      "|           Cancelled|    Amazon|         Expedited|        NULL|\n",
      "|             Shipped|    Amazon|          Standard|        NULL|\n",
      "|           Cancelled|    Amazon|          Standard|        NULL|\n",
      "|             Pending|    Amazon|         Expedited|        NULL|\n",
      "|            Shipping|    Amazon|          Standard|        NULL|\n",
      "|             Pending|    Amazon|          Standard|        NULL|\n",
      "+--------------------+----------+------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shipping_dim_df = spark.sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT\n",
    "        Status,\n",
    "        Fulfilment,\n",
    "        ship_service_level,\n",
    "        fulfilled_by\n",
    "    FROM\n",
    "        {source_name}\n",
    "    \"\"\"\n",
    ")\n",
    "shipping_dim_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
