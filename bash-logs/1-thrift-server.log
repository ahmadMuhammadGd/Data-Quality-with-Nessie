Exporting variables in /config/dremio.env
Exporting variables in /config/dwh.env
Exporting variables in /config/minio.env
Exporting variables in /config/nessie.env
Exporting variables in /config/paths.env
NESSIE_URI: http://nessie:19120/api/v1
BRANCH_MAIN: main
MINIO_ACCESS_KEY: admin
MINIO_SECRET_KEY: password
MINIO_ENDPOINT: http://172.18.0.3:9000
MINIO_ICEBERG_S3_BUCKET: s3://warehouse
SPARK_HOME: /opt/spark
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/me/.ivy2/cache
The jars for the packages stored in: /home/me/.ivy2/jars
org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency
org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-c4e18cdd-9e4c-4087-a829-bdb4de614e9a;1.0
	confs: [default]
	found org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central
	found org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.92.1 in central
downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.2/iceberg-spark-runtime-3.5_2.12-1.5.2.jar ...
	[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2!iceberg-spark-runtime-3.5_2.12.jar (591ms)
downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.5_2.12/0.92.1/nessie-spark-extensions-3.5_2.12-0.92.1.jar ...
	[SUCCESSFUL ] org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.92.1!nessie-spark-extensions-3.5_2.12.jar (71ms)
:: resolution report :: resolve 1750ms :: artifacts dl 671ms
	:: modules in use:
	org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]
	org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.92.1 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-c4e18cdd-9e4c-4087-a829-bdb4de614e9a
	confs: [default]
	2 artifacts copied, 0 already retrieved (43039kB/92ms)
24/09/28 15:57:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/09/28 15:57:12 INFO HiveThriftServer2: Started daemon with process name: 336@1809a5016469
24/09/28 15:57:12 INFO SignalUtils: Registering signal handler for TERM
24/09/28 15:57:12 INFO SignalUtils: Registering signal handler for HUP
24/09/28 15:57:12 INFO SignalUtils: Registering signal handler for INT
24/09/28 15:57:12 INFO HiveThriftServer2: Starting SparkContext
24/09/28 15:57:12 INFO HiveConf: Found configuration file null
24/09/28 15:57:12 INFO SparkContext: Running Spark version 3.5.2
24/09/28 15:57:12 INFO SparkContext: OS info Linux, 6.8.0-1014-azure, amd64
24/09/28 15:57:12 INFO SparkContext: Java version 11.0.24
24/09/28 15:57:12 INFO ResourceUtils: ==============================================================
24/09/28 15:57:12 INFO ResourceUtils: No custom resources configured for spark.driver.
24/09/28 15:57:12 INFO ResourceUtils: ==============================================================
24/09/28 15:57:12 INFO SparkContext: Submitted application: SparkSQL::172.18.0.8
24/09/28 15:57:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/09/28 15:57:12 INFO ResourceProfile: Limiting resource is cpu
24/09/28 15:57:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/09/28 15:57:12 INFO SecurityManager: Changing view acls to: me
24/09/28 15:57:12 INFO SecurityManager: Changing modify acls to: me
24/09/28 15:57:12 INFO SecurityManager: Changing view acls groups to: 
24/09/28 15:57:12 INFO SecurityManager: Changing modify acls groups to: 
24/09/28 15:57:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: me; groups with view permissions: EMPTY; users with modify permissions: me; groups with modify permissions: EMPTY
24/09/28 15:57:13 INFO Utils: Successfully started service 'sparkDriver' on port 39555.
24/09/28 15:57:13 INFO SparkEnv: Registering MapOutputTracker
24/09/28 15:57:13 INFO SparkEnv: Registering BlockManagerMaster
24/09/28 15:57:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/09/28 15:57:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/09/28 15:57:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/09/28 15:57:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-517554e2-bf35-4f7b-9e26-be94be0219aa
24/09/28 15:57:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/09/28 15:57:13 INFO SparkEnv: Registering OutputCommitCoordinator
24/09/28 15:57:13 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/09/28 15:57:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/09/28 15:57:13 INFO SparkContext: Added JAR file:///home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar at spark://1809a5016469:39555/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1727539032612
24/09/28 15:57:13 INFO SparkContext: Added JAR file:///home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar at spark://1809a5016469:39555/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar with timestamp 1727539032612
24/09/28 15:57:13 INFO SparkContext: Added file file:///home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar at file:///home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1727539032612
24/09/28 15:57:13 INFO Utils: Copying /home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar to /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar
24/09/28 15:57:13 INFO SparkContext: Added file file:///home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar at file:///home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar with timestamp 1727539032612
24/09/28 15:57:13 INFO Utils: Copying /home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar to /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar
24/09/28 15:57:13 INFO Executor: Starting executor ID driver on host 1809a5016469
24/09/28 15:57:13 INFO Executor: OS info Linux, 6.8.0-1014-azure, amd64
24/09/28 15:57:13 INFO Executor: Java version 11.0.24
24/09/28 15:57:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/09/28 15:57:13 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4f1afe8f for default.
24/09/28 15:57:13 INFO Executor: Fetching file:///home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1727539032612
24/09/28 15:57:13 INFO Utils: /home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar has been previously copied to /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar
24/09/28 15:57:13 INFO Executor: Fetching file:///home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar with timestamp 1727539032612
24/09/28 15:57:13 INFO Utils: /home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar has been previously copied to /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar
24/09/28 15:57:13 INFO Executor: Fetching spark://1809a5016469:39555/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1727539032612
24/09/28 15:57:13 INFO TransportClientFactory: Successfully created connection to 1809a5016469/172.18.0.8:39555 after 44 ms (0 ms spent in bootstraps)
24/09/28 15:57:13 INFO Utils: Fetching spark://1809a5016469:39555/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar to /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/fetchFileTemp11407978480468489187.tmp
24/09/28 15:57:14 INFO Utils: /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/fetchFileTemp11407978480468489187.tmp has been previously copied to /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar
24/09/28 15:57:14 INFO Executor: Adding file:/tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar to class loader default
24/09/28 15:57:14 INFO Executor: Fetching spark://1809a5016469:39555/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar with timestamp 1727539032612
24/09/28 15:57:14 INFO Utils: Fetching spark://1809a5016469:39555/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar to /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/fetchFileTemp15912953109038411174.tmp
24/09/28 15:57:14 INFO Utils: /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/fetchFileTemp15912953109038411174.tmp has been previously copied to /tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar
24/09/28 15:57:14 INFO Executor: Adding file:/tmp/spark-2d35199d-db16-4b9d-9aa2-d83961baf37d/userFiles-b2e8c340-f92e-45cb-a10d-a000d8cdd058/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar to class loader default
24/09/28 15:57:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34757.
24/09/28 15:57:14 INFO NettyBlockTransferService: Server created on 1809a5016469:34757
24/09/28 15:57:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/09/28 15:57:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 1809a5016469, 34757, None)
24/09/28 15:57:14 INFO BlockManagerMasterEndpoint: Registering block manager 1809a5016469:34757 with 434.4 MiB RAM, BlockManagerId(driver, 1809a5016469, 34757, None)
24/09/28 15:57:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 1809a5016469, 34757, None)
24/09/28 15:57:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 1809a5016469, 34757, None)
24/09/28 15:57:14 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/09/28 15:57:14 INFO SharedState: Warehouse path is 'file:/opt/spark/spark-warehouse'.
24/09/28 15:57:15 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/09/28 15:57:16 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/opt/spark/spark-warehouse
24/09/28 15:57:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 15:57:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 15:57:16 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 15:57:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 15:57:16 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/09/28 15:57:16 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/09/28 15:57:22 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/09/28 15:57:28 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 15:57:28 INFO ObjectStore: Initialized ObjectStore
24/09/28 15:57:28 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/09/28 15:57:28 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.8
24/09/28 15:57:28 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
24/09/28 15:57:29 INFO HiveMetaStore: Added admin role in metastore
24/09/28 15:57:29 INFO HiveMetaStore: Added public role in metastore
24/09/28 15:57:29 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/09/28 15:57:29 INFO HiveMetaStore: 0: get_database: default
24/09/28 15:57:29 INFO audit: ugi=me	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 15:57:29 INFO HiveUtils: Initializing execution hive, version 2.3.9
24/09/28 15:57:29 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/opt/spark/spark-warehouse
24/09/28 15:57:29 INFO SessionManager: Operation log root directory is created: /tmp/me/operation_logs
24/09/28 15:57:29 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
24/09/28 15:57:29 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
24/09/28 15:57:29 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
24/09/28 15:57:29 INFO AbstractService: Service:OperationManager is inited.
24/09/28 15:57:29 INFO AbstractService: Service:SessionManager is inited.
24/09/28 15:57:29 INFO AbstractService: Service: CLIService is inited.
24/09/28 15:57:29 INFO AbstractService: Service:ThriftBinaryCLIService is inited.
24/09/28 15:57:29 INFO AbstractService: Service: HiveServer2 is inited.
24/09/28 15:57:29 INFO AbstractService: Service:OperationManager is started.
24/09/28 15:57:29 INFO AbstractService: Service:SessionManager is started.
24/09/28 15:57:29 INFO AbstractService: Service: CLIService is started.
24/09/28 15:57:29 INFO AbstractService: Service:ThriftBinaryCLIService is started.
24/09/28 15:57:29 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads
24/09/28 15:57:29 INFO AbstractService: Service:HiveServer2 is started.
24/09/28 15:57:29 INFO HiveThriftServer2: HiveThriftServer2 started
24/09/28 16:01:57 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:01:57 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/04cd04fd-7e83-42e6-9de9-878b32d672f2
24/09/28 16:01:57 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 687e7239-bb80-44a1-83fc-8f36110884ce
24/09/28 16:01:57 INFO SparkExecuteStatementOperation: Running query with 687e7239-bb80-44a1-83fc-8f36110884ce
24/09/28 16:01:58 INFO HiveMetaStore: 1: get_database: global_temp
24/09/28 16:01:58 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/09/28 16:01:58 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:01:58 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:01:58 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:01:58 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:01:58 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:01:58 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:01:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/09/28 16:01:58 INFO HiveMetaStore: 1: get_database: default
24/09/28 16:01:58 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:01:58 INFO DAGScheduler: Asked to cancel job group 687e7239-bb80-44a1-83fc-8f36110884ce
24/09/28 16:01:58 INFO SparkExecuteStatementOperation: Close statement with 687e7239-bb80-44a1-83fc-8f36110884ce
24/09/28 16:01:58 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 283685f4-2283-451e-981a-3ae04bf63d69
24/09/28 16:01:58 INFO SparkExecuteStatementOperation: Running query with 283685f4-2283-451e-981a-3ae04bf63d69
24/09/28 16:01:58 INFO HiveMetaStore: 2: get_databases: *
24/09/28 16:01:58 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:01:58 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:01:58 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:01:58 INFO HiveMetaStore: 2: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:01:58 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:01:58 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:01:58 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:01:59 INFO CodeGenerator: Code generated in 274.966269 ms
24/09/28 16:01:59 INFO CodeGenerator: Code generated in 7.861814 ms
24/09/28 16:01:59 INFO CodeGenerator: Code generated in 12.026629 ms
24/09/28 16:01:59 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:01:59 INFO DAGScheduler: Asked to cancel job group 283685f4-2283-451e-981a-3ae04bf63d69
24/09/28 16:01:59 INFO SparkExecuteStatementOperation: Close statement with 283685f4-2283-451e-981a-3ae04bf63d69
24/09/28 16:01:59 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:01:59 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:01:59 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:01:59 INFO HiveMetaStore: 1: Cleaning up thread local RawStore...
24/09/28 16:01:59 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:01:59 INFO HiveMetaStore: 1: Done cleaning up thread local RawStore
24/09/28 16:01:59 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:01:59 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:01:59 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:01:59 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/9991512c-2db3-48b4-a55b-166ff5b807a9
24/09/28 16:01:59 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with dab13681-8ba9-4421-8cae-518c34572b1c
24/09/28 16:01:59 INFO SparkExecuteStatementOperation: Running query with dab13681-8ba9-4421-8cae-518c34572b1c
24/09/28 16:01:59 INFO HiveMetaStore: 3: get_database: default
24/09/28 16:01:59 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:01:59 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:01:59 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:01:59 INFO HiveMetaStore: 3: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:01:59 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:01:59 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:01:59 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:01:59 INFO DAGScheduler: Asked to cancel job group dab13681-8ba9-4421-8cae-518c34572b1c
24/09/28 16:01:59 INFO SparkExecuteStatementOperation: Close statement with dab13681-8ba9-4421-8cae-518c34572b1c
24/09/28 16:01:59 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 580a4b01-54bb-4b9a-80e1-89553fe9eb5f
24/09/28 16:01:59 INFO SparkExecuteStatementOperation: Running query with 580a4b01-54bb-4b9a-80e1-89553fe9eb5f
24/09/28 16:02:00 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:00 INFO DAGScheduler: Asked to cancel job group 580a4b01-54bb-4b9a-80e1-89553fe9eb5f
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Close statement with 580a4b01-54bb-4b9a-80e1-89553fe9eb5f
24/09/28 16:02:00 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:00 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:00 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:00 INFO HiveMetaStore: 3: Cleaning up thread local RawStore...
24/09/28 16:02:00 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:00 INFO HiveMetaStore: 3: Done cleaning up thread local RawStore
24/09/28 16:02:00 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:00 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:00 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:00 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/3a518100-4627-41e8-8620-ce5e330891f7
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 77382c51-879a-4dc5-9548-d813e9c479e2
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Running query with 77382c51-879a-4dc5-9548-d813e9c479e2
24/09/28 16:02:00 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:02:00 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:00 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:00 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:00 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:00 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:00 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:00 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:00 INFO DAGScheduler: Asked to cancel job group 77382c51-879a-4dc5-9548-d813e9c479e2
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Close statement with 77382c51-879a-4dc5-9548-d813e9c479e2
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 414817d9-6591-466a-946c-a7a8e1a09217
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Running query with 414817d9-6591-466a-946c-a7a8e1a09217
24/09/28 16:02:00 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:00 INFO DAGScheduler: Asked to cancel job group 414817d9-6591-466a-946c-a7a8e1a09217
24/09/28 16:02:00 ERROR SparkExecuteStatementOperation: Error executing query with 414817d9-6591-466a-946c-a7a8e1a09217, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#12, tableName#13, isTemporary#14, information#15]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6be87e22, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 68962baa-fb30-4ee5-84ef-d22ae050eae2
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Running query with 68962baa-fb30-4ee5-84ef-d22ae050eae2
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:02:00 INFO DAGScheduler: Asked to cancel job group 68962baa-fb30-4ee5-84ef-d22ae050eae2
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Close statement with 68962baa-fb30-4ee5-84ef-d22ae050eae2
24/09/28 16:02:00 INFO DAGScheduler: Asked to cancel job group 414817d9-6591-466a-946c-a7a8e1a09217
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Close statement with 414817d9-6591-466a-946c-a7a8e1a09217
24/09/28 16:02:00 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:00 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:00 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:00 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:02:00 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:00 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:02:00 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:00 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:00 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:00 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/e27a46f4-c911-418c-b96e-93d4201b8e8e
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 6ec313a1-212c-4be8-9887-0a6c82459b95
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Running query with 6ec313a1-212c-4be8-9887-0a6c82459b95
24/09/28 16:02:00 INFO HiveMetaStore: 5: get_database: default
24/09/28 16:02:00 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:00 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:00 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:00 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:00 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:00 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:00 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:00 INFO DAGScheduler: Asked to cancel job group 6ec313a1-212c-4be8-9887-0a6c82459b95
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Close statement with 6ec313a1-212c-4be8-9887-0a6c82459b95
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.amazon_orders_silver"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with a3ae5f7e-78b0-4330-a58c-14b5266d6c4a
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Running query with a3ae5f7e-78b0-4330-a58c-14b5266d6c4a
24/09/28 16:02:00 INFO CodeGenerator: Code generated in 10.245894 ms
24/09/28 16:02:00 INFO CodeGenerator: Code generated in 8.805004 ms
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.amazon_orders_silver"} */
create or replace view nessie.gold.amazon_orders_silver
  
  
  as
    
    -- partition_by='MONTH(ingestion_date)',


SELECT
    *
FROM
    nessie.silver.amazon_orders
' with d11bdcee-486a-41ce-850e-2ed76663383f
24/09/28 16:02:00 INFO SparkExecuteStatementOperation: Running query with d11bdcee-486a-41ce-850e-2ed76663383f
24/09/28 16:02:00 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:00 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:02:01 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=130aadf477fb6595977bab6e9b8d65e0ea5963b93936f4b9abcab8f2294bf3c5}'
24/09/28 16:02:02 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:02:02 INFO V2ScanRelationPushDown: 
Output: Order_ID#40, Order_Date#41, Order_Status#42, Fulfilment#43, ORDERS_Channel#44, ship_service_level#45, Category#46, Size#47, Courier_Status#48, Qty#49, Currency#50, Amount#51, Ship_City#52, Ship_State#53, Ship_Postal_Code#54, Ship_Country#55, B2B#56, Fulfilled_By#57, New#58, PendingS#59, Ingestion_Date#60
         
24/09/28 16:02:02 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 8111912379027843168 created at 2024-09-28T15:59:27.211+00:00 with filter true
24/09/28 16:02:02 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/28 16:02:02 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/28 16:02:02 INFO NessieIcebergClient: Committed 'gold.amazon_orders_silver' against 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=9583bd54307f21d4ba60204a2e156be312bf88a9501f8e3560a24ce0c70baf1e}', expected commit-id was '130aadf477fb6595977bab6e9b8d65e0ea5963b93936f4b9abcab8f2294bf3c5'
24/09/28 16:02:02 INFO BaseViewOperations: Successfully committed to view gold.amazon_orders_silver in 132 ms
24/09/28 16:02:02 INFO DAGScheduler: Asked to cancel job group d11bdcee-486a-41ce-850e-2ed76663383f
24/09/28 16:02:02 INFO SparkExecuteStatementOperation: Close statement with d11bdcee-486a-41ce-850e-2ed76663383f
24/09/28 16:02:02 INFO DAGScheduler: Asked to cancel job group a3ae5f7e-78b0-4330-a58c-14b5266d6c4a
24/09/28 16:02:02 INFO SparkExecuteStatementOperation: Close statement with a3ae5f7e-78b0-4330-a58c-14b5266d6c4a
24/09/28 16:02:02 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:02 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:02 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/28 16:02:02 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:02 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/28 16:02:02 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:02 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:02 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:02 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/6c0e45f5-8b09-4b56-a48b-e7107556ad52
24/09/28 16:02:02 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 93db7a2b-6c23-44c6-99ee-6eb37a3d1f25
24/09/28 16:02:02 INFO SparkExecuteStatementOperation: Running query with 93db7a2b-6c23-44c6-99ee-6eb37a3d1f25
24/09/28 16:02:02 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:02:02 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:02 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:02 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:02 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:02 INFO DAGScheduler: Asked to cancel job group 93db7a2b-6c23-44c6-99ee-6eb37a3d1f25
24/09/28 16:02:02 INFO SparkExecuteStatementOperation: Close statement with 93db7a2b-6c23-44c6-99ee-6eb37a3d1f25
24/09/28 16:02:02 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:02 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:02 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:02:02 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:02 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:02:02 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:02 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:15 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:15 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/9bdb5d3b-0db6-482e-a96b-99ac9599e8ab
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with ab5b4631-18d4-4cd7-af91-dc056c223f83
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Running query with ab5b4631-18d4-4cd7-af91-dc056c223f83
24/09/28 16:02:15 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:02:15 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:15 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:15 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:15 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:15 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:15 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:15 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:15 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:15 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/d59e95f2-cdee-43a9-a64d-8e1c4715ec42
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with d04f47a4-bf5e-4f4d-8943-f589f9286d36
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Running query with d04f47a4-bf5e-4f4d-8943-f589f9286d36
24/09/28 16:02:15 INFO DAGScheduler: Asked to cancel job group ab5b4631-18d4-4cd7-af91-dc056c223f83
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Close statement with ab5b4631-18d4-4cd7-af91-dc056c223f83
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with ff4eaa45-10d0-4d0b-854b-0102516c13c5
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Running query with ff4eaa45-10d0-4d0b-854b-0102516c13c5
24/09/28 16:02:15 INFO HiveMetaStore: 5: get_database: default
24/09/28 16:02:15 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:15 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:15 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:15 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/2c961326-3bf4-45f6-9cd3-51ccb4e296f9
24/09/28 16:02:15 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/ab2b43ee-3057-48b3-88a8-b84b20de743f
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 514293d2-e1ad-4bed-95cc-634c1d4dd7e6
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Running query with 514293d2-e1ad-4bed-95cc-634c1d4dd7e6
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 5c1d8149-36bc-4f08-9f63-442060c2ca2a
24/09/28 16:02:15 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:15 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:15 INFO SparkExecuteStatementOperation: Running query with 5c1d8149-36bc-4f08-9f63-442060c2ca2a
24/09/28 16:02:15 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:15 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:15 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:15 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:15 INFO HiveMetaStore: 7: get_databases: *
24/09/28 16:02:15 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group d04f47a4-bf5e-4f4d-8943-f589f9286d36
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with d04f47a4-bf5e-4f4d-8943-f589f9286d36
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 6e13efc1-ba30-4ff1-8b7b-e61dcdb3c6c7
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 6e13efc1-ba30-4ff1-8b7b-e61dcdb3c6c7
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 7: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 8: get_databases: *
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:02:16 INFO HiveMetaStore: 8: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group ff4eaa45-10d0-4d0b-854b-0102516c13c5
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with ff4eaa45-10d0-4d0b-854b-0102516c13c5
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO HiveMetaStore: 3: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:16 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:16 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:16 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/1875db2e-4789-4b68-b9d7-b95dba40ab31
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 3: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with acf58cba-eb77-4298-8425-6f292d7d850e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with acf58cba-eb77-4298-8425-6f292d7d850e
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 6e13efc1-ba30-4ff1-8b7b-e61dcdb3c6c7
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 6e13efc1-ba30-4ff1-8b7b-e61dcdb3c6c7
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:16 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:16 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 5c1d8149-36bc-4f08-9f63-442060c2ca2a
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 5c1d8149-36bc-4f08-9f63-442060c2ca2a
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 28bb101d-fac3-4a1f-848c-c0a4ceb4397e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 28bb101d-fac3-4a1f-848c-c0a4ceb4397e
24/09/28 16:02:16 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:16 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/88fd7ace-4e20-4d15-a2a2-5963cfa78dec
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 3e82aa80-3ae0-4c25-8053-69ed70bb0d2f
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 3e82aa80-3ae0-4c25-8053-69ed70bb0d2f
24/09/28 16:02:16 INFO HiveMetaStore: 9: get_databases: *
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:02:16 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 514293d2-e1ad-4bed-95cc-634c1d4dd7e6
24/09/28 16:02:16 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/273cc185-6aff-4273-94b8-8b6f2413c94f
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 514293d2-e1ad-4bed-95cc-634c1d4dd7e6
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 6388050f-7de3-4fbe-8265-378b2734d39d
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 6388050f-7de3-4fbe-8265-378b2734d39d
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with b330f15b-7bc9-463f-8833-ac6da0b1b221
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with b330f15b-7bc9-463f-8833-ac6da0b1b221
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 9: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 1: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 28bb101d-fac3-4a1f-848c-c0a4ceb4397e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 28bb101d-fac3-4a1f-848c-c0a4ceb4397e
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 3: Cleaning up thread local RawStore...
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:16 INFO HiveMetaStore: 3: Done cleaning up thread local RawStore
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:16 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group acf58cba-eb77-4298-8425-6f292d7d850e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with acf58cba-eb77-4298-8425-6f292d7d850e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 9e58a2c2-07d1-401a-9268-51dfaa0e7a9e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 9e58a2c2-07d1-401a-9268-51dfaa0e7a9e
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:16 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/7c8f3350-6284-46e5-b254-4a8d066d6c91
24/09/28 16:02:16 INFO HiveMetaStore: 10: get_databases: *
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 93f2f684-9863-40eb-8c3b-a851aa4088f2
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 93f2f684-9863-40eb-8c3b-a851aa4088f2
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group b330f15b-7bc9-463f-8833-ac6da0b1b221
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with b330f15b-7bc9-463f-8833-ac6da0b1b221
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 84a0657d-8d92-4301-99bf-3db240ccfb33
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 84a0657d-8d92-4301-99bf-3db240ccfb33
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 10: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 5: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 9e58a2c2-07d1-401a-9268-51dfaa0e7a9e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 9e58a2c2-07d1-401a-9268-51dfaa0e7a9e
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:16 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:16 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 6388050f-7de3-4fbe-8265-378b2734d39d
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 6388050f-7de3-4fbe-8265-378b2734d39d
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO HiveMetaStore: 11: get_databases: *
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:02:16 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:16 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/2018086a-92f1-4fca-a164-02a22a173890
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 3e82aa80-3ae0-4c25-8053-69ed70bb0d2f
24/09/28 16:02:16 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 3e82aa80-3ae0-4c25-8053-69ed70bb0d2f
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:16 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with fab75593-0a40-45e5-b26c-a19b5524f69e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with fab75593-0a40-45e5-b26c-a19b5524f69e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 1583db3d-e3f0-438e-a9df-a347ddb83b72
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 11: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 1583db3d-e3f0-438e-a9df-a347ddb83b72
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/b0ba0836-f386-4da5-a5c2-55ad7b1198a8
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 16585529-6602-4663-83f5-a018a3eb43cb
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 16585529-6602-4663-83f5-a018a3eb43cb
24/09/28 16:02:16 INFO HiveMetaStore: 3: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 3: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:02:16 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 84a0657d-8d92-4301-99bf-3db240ccfb33
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 84a0657d-8d92-4301-99bf-3db240ccfb33
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 93f2f684-9863-40eb-8c3b-a851aa4088f2
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 93f2f684-9863-40eb-8c3b-a851aa4088f2
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 55a2fdf7-eb62-4482-a517-8063b5af293f
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 55a2fdf7-eb62-4482-a517-8063b5af293f
24/09/28 16:02:16 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 1: Cleaning up thread local RawStore...
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:16 INFO HiveMetaStore: 1: Done cleaning up thread local RawStore
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:16 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 1583db3d-e3f0-438e-a9df-a347ddb83b72
24/09/28 16:02:16 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 1583db3d-e3f0-438e-a9df-a347ddb83b72
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:16 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/a8eba300-7012-4b21-96e6-7e9582f7d4ba
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 16585529-6602-4663-83f5-a018a3eb43cb
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 16585529-6602-4663-83f5-a018a3eb43cb
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:16 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with d2df34ba-d3b4-4514-8977-20808f5bd2a6
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with d2df34ba-d3b4-4514-8977-20808f5bd2a6
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 1c108ca9-01ad-443f-a86e-545bebdede67
24/09/28 16:02:16 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 1c108ca9-01ad-443f-a86e-545bebdede67
24/09/28 16:02:16 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:16 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:16 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/7cab7fcc-0e9f-4eaa-b247-fae70fca9077
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with d670ce4c-1a7e-4b6d-8557-71963633d68a
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with d670ce4c-1a7e-4b6d-8557-71963633d68a
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 1: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group fab75593-0a40-45e5-b26c-a19b5524f69e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with fab75593-0a40-45e5-b26c-a19b5524f69e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 84bf95d5-7f87-4b48-95e2-c4ffe293fb2e
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 84bf95d5-7f87-4b48-95e2-c4ffe293fb2e
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 55a2fdf7-eb62-4482-a517-8063b5af293f
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 55a2fdf7-eb62-4482-a517-8063b5af293f
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:16 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO HiveMetaStore: 3: Cleaning up thread local RawStore...
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:16 INFO HiveMetaStore: 3: Done cleaning up thread local RawStore
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 84bf95d5-7f87-4b48-95e2-c4ffe293fb2e
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:16 ERROR SparkExecuteStatementOperation: Error executing query with 84bf95d5-7f87-4b48-95e2-c4ffe293fb2e, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#142, tableName#143, isTemporary#144, information#145]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6084334d, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO HiveMetaStore: 5: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group d2df34ba-d3b4-4514-8977-20808f5bd2a6
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with d2df34ba-d3b4-4514-8977-20808f5bd2a6
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 61ddafb6-93c3-4da1-bdde-ec400c72ea43
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 61ddafb6-93c3-4da1-bdde-ec400c72ea43
24/09/28 16:02:16 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:16 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/75383705-b442-497a-b39b-8ed33bef8ece
24/09/28 16:02:16 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 53d07fdf-13b7-4c1d-920c-b68088877acc
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 9286291e-dfde-4d18-bf4f-46a82591e657
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 9286291e-dfde-4d18-bf4f-46a82591e657
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:16 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:16 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group 1c108ca9-01ad-443f-a86e-545bebdede67
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with 1c108ca9-01ad-443f-a86e-545bebdede67
24/09/28 16:02:16 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:16 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:16 INFO DAGScheduler: Asked to cancel job group d670ce4c-1a7e-4b6d-8557-71963633d68a
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Close statement with d670ce4c-1a7e-4b6d-8557-71963633d68a
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 53d07fdf-13b7-4c1d-920c-b68088877acc
24/09/28 16:02:16 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:16 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:16 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 5c71a95d-c0c9-41fe-949b-ad56d8406de9
24/09/28 16:02:16 INFO HiveMetaStore: 3: get_database: default
24/09/28 16:02:16 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:16 INFO SparkExecuteStatementOperation: Running query with 5c71a95d-c0c9-41fe-949b-ad56d8406de9
24/09/28 16:02:17 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:17 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:17 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/b5285d93-88c3-4929-b903-10ff62f9400e
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 5c71a95d-c0c9-41fe-949b-ad56d8406de9
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with e398200f-c7fa-4af6-8810-89451bdab030
24/09/28 16:02:17 ERROR SparkExecuteStatementOperation: Error executing query with 5c71a95d-c0c9-41fe-949b-ad56d8406de9, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#155, tableName#156, isTemporary#157, information#158]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@59a814d2, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with e398200f-c7fa-4af6-8810-89451bdab030
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:17 INFO HiveMetaStore: 3: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with a873c6ff-485a-4fa7-818a-4c037c43e403
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with a873c6ff-485a-4fa7-818a-4c037c43e403
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 9286291e-dfde-4d18-bf4f-46a82591e657
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 9286291e-dfde-4d18-bf4f-46a82591e657
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with eb47fbb4-ccbe-48b5-8cc5-252eebeef467
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with eb47fbb4-ccbe-48b5-8cc5-252eebeef467
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:17 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:17 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group eb47fbb4-ccbe-48b5-8cc5-252eebeef467
24/09/28 16:02:17 ERROR SparkExecuteStatementOperation: Error executing query with eb47fbb4-ccbe-48b5-8cc5-252eebeef467, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#166, tableName#167, isTemporary#168, information#169]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@40ff09bf, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group e398200f-c7fa-4af6-8810-89451bdab030
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with e398200f-c7fa-4af6-8810-89451bdab030
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 2eee7818-65c1-462b-bd8b-b23d0b7024ef
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 2eee7818-65c1-462b-bd8b-b23d0b7024ef
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 96e33fdc-3ca9-4498-abbf-70f5883887c9
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 96e33fdc-3ca9-4498-abbf-70f5883887c9
24/09/28 16:02:17 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 2eee7818-65c1-462b-bd8b-b23d0b7024ef
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:02:17 ERROR SparkExecuteStatementOperation: Error executing query with 2eee7818-65c1-462b-bd8b-b23d0b7024ef, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#172, tableName#173, isTemporary#174, information#175]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@12713560, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 61ddafb6-93c3-4da1-bdde-ec400c72ea43
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 61ddafb6-93c3-4da1-bdde-ec400c72ea43
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 53d07fdf-13b7-4c1d-920c-b68088877acc
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 53d07fdf-13b7-4c1d-920c-b68088877acc
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 84bf95d5-7f87-4b48-95e2-c4ffe293fb2e
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 84bf95d5-7f87-4b48-95e2-c4ffe293fb2e
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:17 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 1: Cleaning up thread local RawStore...
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 8d9e9bbb-b21e-41ae-af6b-52552b7092ba
24/09/28 16:02:17 INFO HiveMetaStore: 1: Done cleaning up thread local RawStore
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 8d9e9bbb-b21e-41ae-af6b-52552b7092ba
24/09/28 16:02:17 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:17 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/421ec0de-c8af-4875-a9a9-956ff4c65a64
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group a873c6ff-485a-4fa7-818a-4c037c43e403
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with afe9b498-fb7a-44f9-887c-f72ab5831c7e
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with a873c6ff-485a-4fa7-818a-4c037c43e403
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with afe9b498-fb7a-44f9-887c-f72ab5831c7e
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 5c71a95d-c0c9-41fe-949b-ad56d8406de9
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 5c71a95d-c0c9-41fe-949b-ad56d8406de9
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:17 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:17 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:02:17 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:17 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/82c6778e-c6ad-4fea-a279-3f06f8f9fdf0
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with e26faeda-7628-476e-b5db-fe91cbf8a415
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with e26faeda-7628-476e-b5db-fe91cbf8a415
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 96e33fdc-3ca9-4498-abbf-70f5883887c9
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 96e33fdc-3ca9-4498-abbf-70f5883887c9
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group eb47fbb4-ccbe-48b5-8cc5-252eebeef467
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with eb47fbb4-ccbe-48b5-8cc5-252eebeef467
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 3: Cleaning up thread local RawStore...
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:17 INFO HiveMetaStore: 3: Done cleaning up thread local RawStore
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:17 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:17 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 5: get_database: default
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 8d9e9bbb-b21e-41ae-af6b-52552b7092ba
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 8d9e9bbb-b21e-41ae-af6b-52552b7092ba
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 2eee7818-65c1-462b-bd8b-b23d0b7024ef
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 2eee7818-65c1-462b-bd8b-b23d0b7024ef
24/09/28 16:02:17 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/6697b868-fe20-427f-a88a-1e530b519170
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group afe9b498-fb7a-44f9-887c-f72ab5831c7e
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with afe9b498-fb7a-44f9-887c-f72ab5831c7e
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with c42f58eb-713f-4fa0-8b93-d62bbfb6ebcd
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with c42f58eb-713f-4fa0-8b93-d62bbfb6ebcd
24/09/28 16:02:17 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with edfd3fa8-7937-4939-b8fc-40e2e3f1c2c8
24/09/28 16:02:17 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with edfd3fa8-7937-4939-b8fc-40e2e3f1c2c8
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:17 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:17 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group edfd3fa8-7937-4939-b8fc-40e2e3f1c2c8
24/09/28 16:02:17 ERROR SparkExecuteStatementOperation: Error executing query with edfd3fa8-7937-4939-b8fc-40e2e3f1c2c8, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#205, tableName#206, isTemporary#207, information#208]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@60090ba, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO HiveMetaStore: 3: get_database: default
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:17 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:17 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/ba714513-b241-4716-9905-c6c6245c0f04
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 312452f2-1cc6-41fe-8377-02067b45a508
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 312452f2-1cc6-41fe-8377-02067b45a508
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group e26faeda-7628-476e-b5db-fe91cbf8a415
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with e26faeda-7628-476e-b5db-fe91cbf8a415
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.shipping_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with 3111f57e-dd3b-4f68-bc41-a4c2ebac3ac4
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 3111f57e-dd3b-4f68-bc41-a4c2ebac3ac4
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 1324fb1c-a1a6-4782-a2d2-3eb842f86dd5
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 1324fb1c-a1a6-4782-a2d2-3eb842f86dd5
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:17 INFO HiveMetaStore: 3: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group c42f58eb-713f-4fa0-8b93-d62bbfb6ebcd
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with c42f58eb-713f-4fa0-8b93-d62bbfb6ebcd
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.date_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with 8c7ba360-9308-4d50-99ed-9c7ca6f698cc
24/09/28 16:02:17 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:17 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/0e4a78a9-2209-4177-9bb8-0b621aa19d00
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 8c7ba360-9308-4d50-99ed-9c7ca6f698cc
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 99bf4bed-aa2e-4f43-9a51-ed6c37757cfd
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 99bf4bed-aa2e-4f43-9a51-ed6c37757cfd
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:17 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 1: get_database: default
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 312452f2-1cc6-41fe-8377-02067b45a508
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 312452f2-1cc6-41fe-8377-02067b45a508
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.location_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with 5531e1c8-d92a-4029-bd5a-0847e4aa5f60
24/09/28 16:02:17 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.date_dim"} */

  
    
        create or replace table nessie.gold.date_dim
      
      
    using iceberg
      
      
      partitioned by (MONTH(full_date))
      
      
      

      as
      

WITH date_series AS (
    SELECT
        src.order_date AS full_date,
        MIN(ingestion_date) AS ingestion_date
    FROM
        nessie.gold.amazon_orders_silver AS src
    
    
    
    GROUP BY
        src.order_date
),
date_attributes AS (
    SELECT
        
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
        full_date AS full_date,
        EXTRACT (YEAR FROM full_date) AS year,
        EXTRACT (MONTH FROM full_date) AS month,
        EXTRACT (DAY FROM full_date) AS day,
        CASE 
            WHEN EXTRACT(MONTH FROM full_date) IN (1, 2, 12) THEN 'Winter'
            WHEN EXTRACT(MONTH FROM full_date) IN (3, 4, 5) THEN 'Spring'
            WHEN EXTRACT(MONTH FROM full_date) IN (6, 7, 8) THEN 'Summer'
            ELSE 'Fall'
        END AS season,
        WEEKOFYEAR(full_date) AS week_of_year,
        ingestion_date
    FROM
        date_series
)

SELECT 
    *
FROM 
    date_attributes
  ' with 5f3efea0-10c7-49e4-a095-52dde7e1bbfd
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.shipping_dim"} */

  
    
        create or replace table nessie.gold.shipping_dim
      
      
    using iceberg
      
      
      
      
      
      

      as
      


WITH src AS (
    SELECT
        src.order_status AS shipping_status,
        src.Fulfilment,
        src.ship_service_level,
        src.fulfilled_by,
        MIN(src.ingestion_date) AS ingestion_date
    FROM 
        nessie.gold.amazon_orders_silver AS src
    
    
    
    
    GROUP BY
        src.order_status,
        src.Fulfilment,
        src.ship_service_level,
        src.fulfilled_by
)

SELECT
    
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
    src.*
FROM
    src
  ' with d2e40d84-fbb3-47ae-a558-bd85b8565db1
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 5531e1c8-d92a-4029-bd5a-0847e4aa5f60
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with d2e40d84-fbb3-47ae-a558-bd85b8565db1
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 5f3efea0-10c7-49e4-a095-52dde7e1bbfd
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:17 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 1324fb1c-a1a6-4782-a2d2-3eb842f86dd5
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 1324fb1c-a1a6-4782-a2d2-3eb842f86dd5
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group edfd3fa8-7937-4939-b8fc-40e2e3f1c2c8
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with edfd3fa8-7937-4939-b8fc-40e2e3f1c2c8
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 99bf4bed-aa2e-4f43-9a51-ed6c37757cfd
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 99bf4bed-aa2e-4f43-9a51-ed6c37757cfd
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:17 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.currency_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with 0c66a6fe-3c96-4b1f-8a8b-1fb34083cd80
24/09/28 16:02:17 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 0c66a6fe-3c96-4b1f-8a8b-1fb34083cd80
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.location_dim"} */

  
    
        create or replace table nessie.gold.location_dim
      
      
    using iceberg
      
      
      partitioned by (ship_country,ship_state)
      
      
      

      as
      


WITH src AS (
    SELECT
        src.ship_country,
        src.ship_state,
        src.ship_city,
        src.ship_postal_code,
        MIN(src.ingestion_date) AS ingestion_date
    FROM 
        nessie.gold.amazon_orders_silver AS src
    
    
    
    
    
    GROUP BY
        src.ship_country,
        src.ship_state,
        src.ship_city,
        src.ship_postal_code
)

SELECT
    
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
    src.*
FROM
    src
  ' with 38e223d6-2244-492e-9fac-6840c6c153ed
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 38e223d6-2244-492e-9fac-6840c6c153ed
24/09/28 16:02:17 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:17 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/5f021a63-4248-438a-bb2d-0ada4bbec969
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 92ae83ee-ea14-4b4d-9a50-24ab5ea59bad
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 92ae83ee-ea14-4b4d-9a50-24ab5ea59bad
24/09/28 16:02:17 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:02:17 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:17 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:17 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.currency_dim"} */

  
    
        create or replace table nessie.gold.currency_dim
      
      
    using iceberg
      
      
      
      
      
      

      as
      

WITH src AS (
    SELECT
        src.currency,
        MIN(src.ingestion_date) AS ingestion_date
    FROM 
        nessie.gold.amazon_orders_silver AS src
    
    
    
    
    
    GROUP BY
        src.currency
)

SELECT
    
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
    src.*
FROM
    src
  ' with 9260e527-4330-4712-bd2c-549cc9188c22
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 9260e527-4330-4712-bd2c-549cc9188c22
24/09/28 16:02:17 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:17 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:17 INFO DAGScheduler: Asked to cancel job group 92ae83ee-ea14-4b4d-9a50-24ab5ea59bad
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Close statement with 92ae83ee-ea14-4b4d-9a50-24ab5ea59bad
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.product_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with 8ff27c76-22fb-4531-9961-3bfc9207cf06
24/09/28 16:02:17 INFO SparkExecuteStatementOperation: Running query with 8ff27c76-22fb-4531-9961-3bfc9207cf06
24/09/28 16:02:17 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:17 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:17 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.product_dim"} */

  
    
        create or replace table nessie.gold.product_dim
      
      
    using iceberg
      
      
      
      
      
      

      as
      


WITH src AS (
    SELECT
        src.Category,
        src.size,
        MIN(src.ingestion_date) AS ingestion_date
    FROM 
        nessie.gold.amazon_orders_silver AS src
    
    
    
    
    
    GROUP BY
        src.Category,
        src.size
)

SELECT
    
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
    src.*
FROM
    src
  ' with c589dc51-e7a9-4f01-997f-f3e012a517a9
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Running query with c589dc51-e7a9-4f01-997f-f3e012a517a9
24/09/28 16:02:18 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:18 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:02:18 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=9583bd54307f21d4ba60204a2e156be312bf88a9501f8e3560a24ce0c70baf1e}'
24/09/28 16:02:18 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:02:18 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:02:18 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=9583bd54307f21d4ba60204a2e156be312bf88a9501f8e3560a24ce0c70baf1e}'
24/09/28 16:02:18 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=9583bd54307f21d4ba60204a2e156be312bf88a9501f8e3560a24ce0c70baf1e}'
24/09/28 16:02:18 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:02:18 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:02:18 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:02:18 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:02:18 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=9583bd54307f21d4ba60204a2e156be312bf88a9501f8e3560a24ce0c70baf1e}'
24/09/28 16:02:18 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:02:18 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:02:18 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=9583bd54307f21d4ba60204a2e156be312bf88a9501f8e3560a24ce0c70baf1e}'
24/09/28 16:02:18 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 9260e527-4330-4712-bd2c-549cc9188c22
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group d2e40d84-fbb3-47ae-a558-bd85b8565db1
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 38e223d6-2244-492e-9fac-6840c6c153ed
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group c589dc51-e7a9-4f01-997f-f3e012a517a9
24/09/28 16:02:18 ERROR SparkExecuteStatementOperation: Error executing query with 38e223d6-2244-492e-9fac-6840c6c153ed, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `src`.`ingestion_date` cannot be resolved. Did you mean one of the following? [`src`.`Ingestion_Date`, `src`.`Category`, `src`.`Currency`, `src`.`New`, `src`.`Order_Date`].; line 26 pos 12;
'ReplaceTableAsSelect [identity(ship_country), identity(ship_state)], TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@5f727516, gold.location_dim
+- 'Project [row_number() windowspecdefinition(scalar-subquery#276 [] ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#279, src.*]
   :  +- 'Project [unresolvedalias(null, None)]
   :     +- OneRowRelation
   +- 'SubqueryAlias src
      +- 'Aggregate [ship_country#373, ship_state#371, ship_city#370, ship_postal_code#372], [ship_country#373, ship_state#371, ship_city#370, ship_postal_code#372, 'MIN('src.ingestion_date) AS ingestion_date#280]
         +- SubqueryAlias src
            +- SubqueryAlias nessie.gold.amazon_orders_silver
               +- Project [cast(Order_ID#422 as string) AS Order_ID#358, cast(Order_Date#423 as date) AS Order_Date#359, cast(Order_Status#424 as string) AS Order_Status#360, cast(Fulfilment#425 as string) AS Fulfilment#361, cast(ORDERS_Channel#426 as string) AS ORDERS_Channel#362, cast(ship_service_level#427 as string) AS ship_service_level#363, cast(Category#428 as string) AS Category#364, cast(Size#429 as string) AS Size#365, cast(Courier_Status#430 as string) AS Courier_Status#366, cast(Qty#431 as int) AS Qty#367, cast(Currency#432 as string) AS Currency#368, cast(Amount#433 as double) AS Amount#369, cast(Ship_City#434 as string) AS Ship_City#370, cast(Ship_State#435 as string) AS Ship_State#371, cast(Ship_Postal_Code#436 as int) AS Ship_Postal_Code#372, cast(Ship_Country#437 as string) AS Ship_Country#373, cast(B2B#438 as boolean) AS B2B#374, cast(Fulfilled_By#439 as string) AS Fulfilled_By#375, cast(New#440 as string) AS New#376, cast(PendingS#441 as string) AS PendingS#377, cast(Ingestion_Date#442 as timestamp) AS Ingestion_Date#378]
                  +- Project [Order_ID#422, Order_Date#423, Order_Status#424, Fulfilment#425, ORDERS_Channel#426, ship_service_level#427, Category#428, Size#429, Courier_Status#430, Qty#431, Currency#432, Amount#433, Ship_City#434, Ship_State#435, Ship_Postal_Code#436, Ship_Country#437, B2B#438, Fulfilled_By#439, New#440, PendingS#441, Ingestion_Date#442]
                     +- SubqueryAlias nessie.silver.amazon_orders
                        +- RelationV2[Order_ID#422, Order_Date#423, Order_Status#424, Fulfilment#425, ORDERS_Channel#426, ship_service_level#427, Category#428, Size#429, Courier_Status#430, Qty#431, Currency#432, Amount#433, Ship_City#434, Ship_State#435, Ship_Postal_Code#436, Ship_Country#437, B2B#438, Fulfilled_By#439, New#440, PendingS#441, Ingestion_Date#442] nessie.silver.amazon_orders nessie.silver.amazon_orders

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 ERROR SparkExecuteStatementOperation: Error executing query with 9260e527-4330-4712-bd2c-549cc9188c22, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `src`.`ingestion_date` cannot be resolved. Did you mean one of the following? [`src`.`Ingestion_Date`, `src`.`Category`, `src`.`Currency`, `src`.`New`, `src`.`Order_Date`].; line 22 pos 12;
'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@3e918428, gold.currency_dim
+- 'Project [row_number() windowspecdefinition(scalar-subquery#283 [] ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#284, src.*]
   :  +- 'Project [unresolvedalias(null, None)]
   :     +- OneRowRelation
   +- 'SubqueryAlias src
      +- 'Aggregate [currency#326], [currency#326, 'MIN('src.ingestion_date) AS ingestion_date#285]
         +- SubqueryAlias src
            +- SubqueryAlias nessie.gold.amazon_orders_silver
               +- Project [cast(Order_ID#443 as string) AS Order_ID#316, cast(Order_Date#444 as date) AS Order_Date#317, cast(Order_Status#445 as string) AS Order_Status#318, cast(Fulfilment#446 as string) AS Fulfilment#319, cast(ORDERS_Channel#447 as string) AS ORDERS_Channel#320, cast(ship_service_level#448 as string) AS ship_service_level#321, cast(Category#449 as string) AS Category#322, cast(Size#450 as string) AS Size#323, cast(Courier_Status#451 as string) AS Courier_Status#324, cast(Qty#452 as int) AS Qty#325, cast(Currency#453 as string) AS Currency#326, cast(Amount#454 as double) AS Amount#327, cast(Ship_City#455 as string) AS Ship_City#328, cast(Ship_State#456 as string) AS Ship_State#329, cast(Ship_Postal_Code#457 as int) AS Ship_Postal_Code#330, cast(Ship_Country#458 as string) AS Ship_Country#331, cast(B2B#459 as boolean) AS B2B#332, cast(Fulfilled_By#460 as string) AS Fulfilled_By#333, cast(New#461 as string) AS New#334, cast(PendingS#462 as string) AS PendingS#335, cast(Ingestion_Date#463 as timestamp) AS Ingestion_Date#336]
                  +- Project [Order_ID#443, Order_Date#444, Order_Status#445, Fulfilment#446, ORDERS_Channel#447, ship_service_level#448, Category#449, Size#450, Courier_Status#451, Qty#452, Currency#453, Amount#454, Ship_City#455, Ship_State#456, Ship_Postal_Code#457, Ship_Country#458, B2B#459, Fulfilled_By#460, New#461, PendingS#462, Ingestion_Date#463]
                     +- SubqueryAlias nessie.silver.amazon_orders
                        +- RelationV2[Order_ID#443, Order_Date#444, Order_Status#445, Fulfilment#446, ORDERS_Channel#447, ship_service_level#448, Category#449, Size#450, Courier_Status#451, Qty#452, Currency#453, Amount#454, Ship_City#455, Ship_State#456, Ship_Postal_Code#457, Ship_Country#458, B2B#459, Fulfilled_By#460, New#461, PendingS#462, Ingestion_Date#463] nessie.silver.amazon_orders nessie.silver.amazon_orders

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 ERROR SparkExecuteStatementOperation: Error executing query with c589dc51-e7a9-4f01-997f-f3e012a517a9, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `src`.`ingestion_date` cannot be resolved. Did you mean one of the following? [`src`.`Ingestion_Date`, `src`.`Category`, `src`.`Currency`, `src`.`New`, `src`.`Order_Date`].; line 24 pos 12;
'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@4eca14e7, gold.product_dim
+- 'Project [row_number() windowspecdefinition(scalar-subquery#304 [] ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#305, src.*]
   :  +- 'Project [unresolvedalias(null, None)]
   :     +- OneRowRelation
   +- 'SubqueryAlias src
      +- 'Aggregate [Category#386, size#387], [Category#386, size#387, 'MIN('src.ingestion_date) AS ingestion_date#306]
         +- SubqueryAlias src
            +- SubqueryAlias nessie.gold.amazon_orders_silver
               +- Project [cast(Order_ID#500 as string) AS Order_ID#380, cast(Order_Date#501 as date) AS Order_Date#381, cast(Order_Status#502 as string) AS Order_Status#382, cast(Fulfilment#503 as string) AS Fulfilment#383, cast(ORDERS_Channel#504 as string) AS ORDERS_Channel#384, cast(ship_service_level#505 as string) AS ship_service_level#385, cast(Category#506 as string) AS Category#386, cast(Size#507 as string) AS Size#387, cast(Courier_Status#508 as string) AS Courier_Status#388, cast(Qty#509 as int) AS Qty#389, cast(Currency#510 as string) AS Currency#390, cast(Amount#511 as double) AS Amount#391, cast(Ship_City#512 as string) AS Ship_City#392, cast(Ship_State#513 as string) AS Ship_State#393, cast(Ship_Postal_Code#514 as int) AS Ship_Postal_Code#394, cast(Ship_Country#515 as string) AS Ship_Country#395, cast(B2B#516 as boolean) AS B2B#396, cast(Fulfilled_By#517 as string) AS Fulfilled_By#397, cast(New#518 as string) AS New#398, cast(PendingS#519 as string) AS PendingS#399, cast(Ingestion_Date#520 as timestamp) AS Ingestion_Date#400]
                  +- Project [Order_ID#500, Order_Date#501, Order_Status#502, Fulfilment#503, ORDERS_Channel#504, ship_service_level#505, Category#506, Size#507, Courier_Status#508, Qty#509, Currency#510, Amount#511, Ship_City#512, Ship_State#513, Ship_Postal_Code#514, Ship_Country#515, B2B#516, Fulfilled_By#517, New#518, PendingS#519, Ingestion_Date#520]
                     +- SubqueryAlias nessie.silver.amazon_orders
                        +- RelationV2[Order_ID#500, Order_Date#501, Order_Status#502, Fulfilment#503, ORDERS_Channel#504, ship_service_level#505, Category#506, Size#507, Courier_Status#508, Qty#509, Currency#510, Amount#511, Ship_City#512, Ship_State#513, Ship_Postal_Code#514, Ship_Country#515, B2B#516, Fulfilled_By#517, New#518, PendingS#519, Ingestion_Date#520] nessie.silver.amazon_orders nessie.silver.amazon_orders

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 ERROR SparkExecuteStatementOperation: Error executing query with d2e40d84-fbb3-47ae-a558-bd85b8565db1, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `src`.`ingestion_date` cannot be resolved. Did you mean one of the following? [`src`.`Ingestion_Date`, `src`.`Category`, `src`.`Currency`, `src`.`New`, `src`.`Order_Date`].; line 26 pos 12;
'ReplaceTableAsSelect TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@6664cfce, gold.shipping_dim
+- 'Project [row_number() windowspecdefinition(scalar-subquery#275 [] ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#277, src.*]
   :  +- 'Project [unresolvedalias(null, None)]
   :     +- OneRowRelation
   +- 'SubqueryAlias src
      +- 'Aggregate [order_status#339, Fulfilment#340, ship_service_level#342, fulfilled_by#354], [order_status#339 AS shipping_status#278, Fulfilment#340, ship_service_level#342, fulfilled_by#354, 'MIN('src.ingestion_date) AS ingestion_date#281]
         +- SubqueryAlias src
            +- SubqueryAlias nessie.gold.amazon_orders_silver
               +- Project [cast(Order_ID#401 as string) AS Order_ID#337, cast(Order_Date#402 as date) AS Order_Date#338, cast(Order_Status#403 as string) AS Order_Status#339, cast(Fulfilment#404 as string) AS Fulfilment#340, cast(ORDERS_Channel#405 as string) AS ORDERS_Channel#341, cast(ship_service_level#406 as string) AS ship_service_level#342, cast(Category#407 as string) AS Category#343, cast(Size#408 as string) AS Size#344, cast(Courier_Status#409 as string) AS Courier_Status#345, cast(Qty#410 as int) AS Qty#346, cast(Currency#411 as string) AS Currency#347, cast(Amount#412 as double) AS Amount#348, cast(Ship_City#413 as string) AS Ship_City#349, cast(Ship_State#414 as string) AS Ship_State#350, cast(Ship_Postal_Code#415 as int) AS Ship_Postal_Code#351, cast(Ship_Country#416 as string) AS Ship_Country#352, cast(B2B#417 as boolean) AS B2B#353, cast(Fulfilled_By#418 as string) AS Fulfilled_By#354, cast(New#419 as string) AS New#355, cast(PendingS#420 as string) AS PendingS#356, cast(Ingestion_Date#421 as timestamp) AS Ingestion_Date#357]
                  +- Project [Order_ID#401, Order_Date#402, Order_Status#403, Fulfilment#404, ORDERS_Channel#405, ship_service_level#406, Category#407, Size#408, Courier_Status#409, Qty#410, Currency#411, Amount#412, Ship_City#413, Ship_State#414, Ship_Postal_Code#415, Ship_Country#416, B2B#417, Fulfilled_By#418, New#419, PendingS#420, Ingestion_Date#421]
                     +- SubqueryAlias nessie.silver.amazon_orders
                        +- RelationV2[Order_ID#401, Order_Date#402, Order_Status#403, Fulfilment#404, ORDERS_Channel#405, ship_service_level#406, Category#407, Size#408, Courier_Status#409, Qty#410, Currency#411, Amount#412, Ship_City#413, Ship_State#414, Ship_Postal_Code#415, Ship_Country#416, B2B#417, Fulfilled_By#418, New#419, PendingS#420, Ingestion_Date#421] nessie.silver.amazon_orders nessie.silver.amazon_orders

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 5f3efea0-10c7-49e4-a095-52dde7e1bbfd
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group c589dc51-e7a9-4f01-997f-f3e012a517a9
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 9260e527-4330-4712-bd2c-549cc9188c22
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 38e223d6-2244-492e-9fac-6840c6c153ed
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with c589dc51-e7a9-4f01-997f-f3e012a517a9
24/09/28 16:02:18 ERROR SparkExecuteStatementOperation: Error executing query with 5f3efea0-10c7-49e4-a095-52dde7e1bbfd, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ingestion_date` cannot be resolved. Did you mean one of the following? [`Ingestion_Date`, `Category`, `Currency`, `New`, `Order_Date`].; line 22 pos 12;
'ReplaceTableAsSelect [MONTH(full_date)], TableSpec(Map(),Some(iceberg),Map(),None,None,None,false), true, false
:- ResolvedIdentifier org.apache.iceberg.spark.SparkCatalog@36043ad7, gold.date_dim
+- 'Project [*]
   +- 'SubqueryAlias date_attributes
      +- 'Project [row_number() windowspecdefinition(scalar-subquery#301 [] ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS id#302, 'full_date AS full_date#303, 'extract(YEAR, 'full_date) AS year#307, 'extract(MONTH, 'full_date) AS month#308, 'extract(DAY, 'full_date) AS day#309, CASE WHEN 'extract(MONTH, 'full_date) IN (1,2,12) THEN Winter WHEN 'extract(MONTH, 'full_date) IN (3,4,5) THEN Spring WHEN 'extract(MONTH, 'full_date) IN (6,7,8) THEN Summer ELSE Fall END AS season#310, 'WEEKOFYEAR('full_date) AS week_of_year#311, 'ingestion_date]
         :  +- 'Project [unresolvedalias(null, None)]
         :     +- OneRowRelation
         +- 'SubqueryAlias date_series
            +- 'Aggregate [order_date#480], [order_date#480 AS full_date#299, 'MIN('ingestion_date) AS ingestion_date#300]
               +- SubqueryAlias src
                  +- SubqueryAlias nessie.gold.amazon_orders_silver
                     +- Project [cast(Order_ID#526 as string) AS Order_ID#479, cast(Order_Date#527 as date) AS Order_Date#480, cast(Order_Status#528 as string) AS Order_Status#481, cast(Fulfilment#529 as string) AS Fulfilment#482, cast(ORDERS_Channel#530 as string) AS ORDERS_Channel#483, cast(ship_service_level#531 as string) AS ship_service_level#484, cast(Category#532 as string) AS Category#485, cast(Size#533 as string) AS Size#486, cast(Courier_Status#534 as string) AS Courier_Status#487, cast(Qty#535 as int) AS Qty#488, cast(Currency#536 as string) AS Currency#489, cast(Amount#537 as double) AS Amount#490, cast(Ship_City#538 as string) AS Ship_City#491, cast(Ship_State#539 as string) AS Ship_State#492, cast(Ship_Postal_Code#540 as int) AS Ship_Postal_Code#493, cast(Ship_Country#541 as string) AS Ship_Country#494, cast(B2B#542 as boolean) AS B2B#495, cast(Fulfilled_By#543 as string) AS Fulfilled_By#496, cast(New#544 as string) AS New#497, cast(PendingS#545 as string) AS PendingS#498, cast(Ingestion_Date#546 as timestamp) AS Ingestion_Date#499]
                        +- Project [Order_ID#526, Order_Date#527, Order_Status#528, Fulfilment#529, ORDERS_Channel#530, ship_service_level#531, Category#532, Size#533, Courier_Status#534, Qty#535, Currency#536, Amount#537, Ship_City#538, Ship_State#539, Ship_Postal_Code#540, Ship_Country#541, B2B#542, Fulfilled_By#543, New#544, PendingS#545, Ingestion_Date#546]
                           +- SubqueryAlias nessie.silver.amazon_orders
                              +- RelationV2[Order_ID#526, Order_Date#527, Order_Status#528, Fulfilment#529, ORDERS_Channel#530, ship_service_level#531, Category#532, Size#533, Courier_Status#534, Qty#535, Currency#536, Amount#537, Ship_City#538, Ship_State#539, Ship_Postal_Code#540, Ship_Country#541, B2B#542, Fulfilled_By#543, New#544, PendingS#545, Ingestion_Date#546] nessie.silver.amazon_orders nessie.silver.amazon_orders

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:299)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:297)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:297)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:243)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:243)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 9260e527-4330-4712-bd2c-549cc9188c22
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 8ff27c76-22fb-4531-9961-3bfc9207cf06
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 8ff27c76-22fb-4531-9961-3bfc9207cf06
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 38e223d6-2244-492e-9fac-6840c6c153ed
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 5531e1c8-d92a-4029-bd5a-0847e4aa5f60
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 5531e1c8-d92a-4029-bd5a-0847e4aa5f60
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 0c66a6fe-3c96-4b1f-8a8b-1fb34083cd80
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 0c66a6fe-3c96-4b1f-8a8b-1fb34083cd80
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group d2e40d84-fbb3-47ae-a558-bd85b8565db1
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with d2e40d84-fbb3-47ae-a558-bd85b8565db1
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 3111f57e-dd3b-4f68-bc41-a4c2ebac3ac4
24/09/28 16:02:18 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:18 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 3111f57e-dd3b-4f68-bc41-a4c2ebac3ac4
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:18 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 1: Cleaning up thread local RawStore...
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:18 INFO HiveMetaStore: 1: Done cleaning up thread local RawStore
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/28 16:02:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:18 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:18 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/b6cd2795-521a-42bb-9e07-3151b9e14aee
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 5f3efea0-10c7-49e4-a095-52dde7e1bbfd
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 5f3efea0-10c7-49e4-a095-52dde7e1bbfd
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 8c7ba360-9308-4d50-99ed-9c7ca6f698cc
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 8c7ba360-9308-4d50-99ed-9c7ca6f698cc
24/09/28 16:02:18 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/cf4de47b-35a4-4a83-be81-340df3a76dff
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 389b1843-b233-49bd-813a-e14a7b4f4261
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 839cec03-9634-4796-ac17-c6bfa6c0794d
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Running query with 389b1843-b233-49bd-813a-e14a7b4f4261
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Running query with 839cec03-9634-4796-ac17-c6bfa6c0794d
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 3: Cleaning up thread local RawStore...
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:18 INFO HiveMetaStore: 3: Done cleaning up thread local RawStore
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:18 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/34c55f5b-c613-4c17-9029-3f0ce882ecc5
24/09/28 16:02:18 INFO HiveMetaStore: 5: get_database: default
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 4cbbaca2-e3ab-4814-bb5e-1c95e91e9ff7
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Running query with 4cbbaca2-e3ab-4814-bb5e-1c95e91e9ff7
24/09/28 16:02:18 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:18 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/613dbec5-08a8-482c-b0c1-4c798ed4dab3
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:18 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 1aa4ed0d-a0ed-45c1-a053-4e4792df6e07
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Running query with 1aa4ed0d-a0ed-45c1-a053-4e4792df6e07
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 3: get_database: default
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:18 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:02:18 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/e00e0804-e03b-403a-87ad-705056d38c76
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with fc4f4b2d-d71a-41ae-8a81-bbb2925edf71
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Running query with fc4f4b2d-d71a-41ae-8a81-bbb2925edf71
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 839cec03-9634-4796-ac17-c6bfa6c0794d
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 839cec03-9634-4796-ac17-c6bfa6c0794d
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:18 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:18 INFO HiveMetaStore: 3: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 1: get_database: default
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 4cbbaca2-e3ab-4814-bb5e-1c95e91e9ff7
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 4cbbaca2-e3ab-4814-bb5e-1c95e91e9ff7
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO HiveMetaStore: 3: Cleaning up thread local RawStore...
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:18 INFO HiveMetaStore: 3: Done cleaning up thread local RawStore
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 389b1843-b233-49bd-813a-e14a7b4f4261
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 389b1843-b233-49bd-813a-e14a7b4f4261
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 1: Cleaning up thread local RawStore...
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:18 INFO HiveMetaStore: 1: Done cleaning up thread local RawStore
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:18 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group fc4f4b2d-d71a-41ae-8a81-bbb2925edf71
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with fc4f4b2d-d71a-41ae-8a81-bbb2925edf71
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:18 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:02:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:02:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:02:18 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:18 INFO DAGScheduler: Asked to cancel job group 1aa4ed0d-a0ed-45c1-a053-4e4792df6e07
24/09/28 16:02:18 INFO SparkExecuteStatementOperation: Close statement with 1aa4ed0d-a0ed-45c1-a053-4e4792df6e07
24/09/28 16:02:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:02:19 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:02:19 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:02:19 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:02:19 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:02:19 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:02:19 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:02:19 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.r24/09/28 16:05:13 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:13 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/ec54c720-9435-47f6-aecc-b9f82aed6f7c
24/09/28 16:05:13 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 2efaf089-699f-4fc9-9e6a-4946766303e4
24/09/28 16:05:13 INFO SparkExecuteStatementOperation: Running query with 2efaf089-699f-4fc9-9e6a-4946766303e4
24/09/28 16:05:13 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:13 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:13 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:13 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:13 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:13 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:13 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:13 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:13 INFO DAGScheduler: Asked to cancel job group 2efaf089-699f-4fc9-9e6a-4946766303e4
24/09/28 16:05:13 INFO SparkExecuteStatementOperation: Close statement with 2efaf089-699f-4fc9-9e6a-4946766303e4
24/09/28 16:05:13 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with a3524a20-c377-4f8d-8ef2-5c95e44529a0
24/09/28 16:05:13 INFO SparkExecuteStatementOperation: Running query with a3524a20-c377-4f8d-8ef2-5c95e44529a0
24/09/28 16:05:14 INFO HiveMetaStore: 12: get_databases: *
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:05:14 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:14 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:14 INFO HiveMetaStore: 12: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:14 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:14 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:14 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:05:14 INFO DAGScheduler: Asked to cancel job group a3524a20-c377-4f8d-8ef2-5c95e44529a0
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Close statement with a3524a20-c377-4f8d-8ef2-5c95e44529a0
24/09/28 16:05:14 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:14 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:14 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:14 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:14 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:14 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:14 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:14 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/c47911dd-288b-4242-aa4f-561589965864
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 9a3b9058-2897-4a46-808d-095a66d90ab9
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Running query with 9a3b9058-2897-4a46-808d-095a66d90ab9
24/09/28 16:05:14 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:14 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:14 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:14 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:14 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:14 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:14 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:14 INFO DAGScheduler: Asked to cancel job group 9a3b9058-2897-4a46-808d-095a66d90ab9
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Close statement with 9a3b9058-2897-4a46-808d-095a66d90ab9
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 541395d9-0c30-4414-9093-c87ae7510b2f
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Running query with 541395d9-0c30-4414-9093-c87ae7510b2f
24/09/28 16:05:14 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:14 INFO DAGScheduler: Asked to cancel job group 541395d9-0c30-4414-9093-c87ae7510b2f
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Close statement with 541395d9-0c30-4414-9093-c87ae7510b2f
24/09/28 16:05:14 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:14 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:14 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:14 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:14 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:14 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:14 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:14 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/b2f133fc-ca94-462f-9a03-921b44bfff2b
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 8239d5b7-3932-4889-a223-1b6d6bbdcea4
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Running query with 8239d5b7-3932-4889-a223-1b6d6bbdcea4
24/09/28 16:05:14 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:14 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:14 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:14 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:14 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:14 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:14 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:14 INFO DAGScheduler: Asked to cancel job group 8239d5b7-3932-4889-a223-1b6d6bbdcea4
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Close statement with 8239d5b7-3932-4889-a223-1b6d6bbdcea4
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with ba2f4ca3-e612-406c-98b8-d215d3a80965
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Running query with ba2f4ca3-e612-406c-98b8-d215d3a80965
24/09/28 16:05:14 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:14 INFO DAGScheduler: Asked to cancel job group ba2f4ca3-e612-406c-98b8-d215d3a80965
24/09/28 16:05:14 ERROR SparkExecuteStatementOperation: Error executing query with ba2f4ca3-e612-406c-98b8-d215d3a80965, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#569, tableName#570, isTemporary#571, information#572]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1e9a3072, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with e72fcfad-4390-4876-9274-a4f1b9635ba1
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Running query with e72fcfad-4390-4876-9274-a4f1b9635ba1
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:05:14 INFO DAGScheduler: Asked to cancel job group e72fcfad-4390-4876-9274-a4f1b9635ba1
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Close statement with e72fcfad-4390-4876-9274-a4f1b9635ba1
24/09/28 16:05:14 INFO DAGScheduler: Asked to cancel job group ba2f4ca3-e612-406c-98b8-d215d3a80965
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Close statement with ba2f4ca3-e612-406c-98b8-d215d3a80965
24/09/28 16:05:14 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:14 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:14 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:14 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:14 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:14 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:14 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:14 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/3c5f6e17-6ea9-4d63-848c-9fd6ab1165a4
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with f41abef0-7db6-4c11-bc4e-6dd623e1d6fd
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Running query with f41abef0-7db6-4c11-bc4e-6dd623e1d6fd
24/09/28 16:05:14 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:14 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:14 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:14 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:14 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:14 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:14 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:14 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:14 INFO DAGScheduler: Asked to cancel job group f41abef0-7db6-4c11-bc4e-6dd623e1d6fd
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Close statement with f41abef0-7db6-4c11-bc4e-6dd623e1d6fd
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.currency_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with 4940fe65-27fb-41ac-8b57-bffdb9326e61
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Running query with 4940fe65-27fb-41ac-8b57-bffdb9326e61
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.currency_dim"} */

  
    
        create or replace table nessie.gold.currency_dim
      
      
    using iceberg
      
      
      
      
      
      

      as
      

WITH src AS (
    SELECT
        src.currency,
        MIN(src.ingestion_date) AS ingestion_date
    FROM 
        nessie.gold.amazon_orders_silver AS src
    
    
    
    
    
    GROUP BY
        src.currency
)

SELECT
    
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
    src.*
FROM
    src
  ' with 66bdddc1-d005-4ff0-944f-9f59bf399f90
24/09/28 16:05:14 INFO SparkExecuteStatementOperation: Running query with 66bdddc1-d005-4ff0-944f-9f59bf399f90
24/09/28 16:05:14 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:14 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:14 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:15 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:05:15 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=9583bd54307f21d4ba60204a2e156be312bf88a9501f8e3560a24ce0c70baf1e}'
24/09/28 16:05:15 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:05:15 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/28 16:05:15 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/28 16:05:15 INFO SparkScanBuilder: Skipping aggregate pushdown: group by aggregation push down is not supported
24/09/28 16:05:15 INFO V2ScanRelationPushDown: 
Output: Currency#632, Ingestion_Date#642
         
24/09/28 16:05:15 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 8111912379027843168 created at 2024-09-28T15:59:27.211+00:00 with filter true
24/09/28 16:05:15 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/28 16:05:15 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/28 16:05:15 INFO SparkWrite: Requesting 0 bytes advisory partition size for table gold.currency_dim
24/09/28 16:05:15 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table gold.currency_dim
24/09/28 16:05:15 INFO SparkWrite: Requesting [] as write ordering for table gold.currency_dim
24/09/28 16:05:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
24/09/28 16:05:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.4 MiB)
24/09/28 16:05:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:15 INFO SparkContext: Created broadcast 0 from broadcast at SparkBatch.java:79
24/09/28 16:05:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	scala.collection.immutable.List.map(List.scala:293)
	org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	scala.collection.immutable.List.map(List.scala:293)
	org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	scala.collection.immutable.List.map(List.scala:293)
	org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	scala.collection.immutable.List.map(List.scala:293)
	org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	scala.collection.immutable.List.map(List.scala:293)
	org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/28 16:05:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/28 16:05:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:15 INFO SparkContext: Created broadcast 1 from broadcast at SparkBatch.java:79
24/09/28 16:05:15 INFO CodeGenerator: Code generated in 82.486312 ms
24/09/28 16:05:16 INFO DAGScheduler: Registering RDD 3 (run at AccessController.java:0) as input to shuffle 0
24/09/28 16:05:16 INFO DAGScheduler: Got map stage job 0 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:16 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (run at AccessController.java:0)
24/09/28 16:05:16 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:05:16 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:16 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 39.9 KiB, free 434.3 MiB)
24/09/28 16:05:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 434.3 MiB)
24/09/28 16:05:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 1809a5016469:34757 (size: 17.4 KiB, free: 434.4 MiB)
24/09/28 16:05:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/09/28 16:05:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 16163 bytes) 
24/09/28 16:05:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/09/28 16:05:16 INFO CodeGenerator: Code generated in 73.198839 ms
24/09/28 16:05:16 INFO CodeGenerator: Code generated in 11.171582 ms
24/09/28 16:05:16 INFO CodeGenerator: Code generated in 7.528693 ms
24/09/28 16:05:16 INFO CodeGenerator: Code generated in 14.174247 ms
24/09/28 16:05:16 INFO CodeGenerator: Code generated in 11.953232 ms
24/09/28 16:05:16 INFO VectorizedSparkParquetReaders: Enabling arrow.enable_unsafe_memory_access
24/09/28 16:05:16 INFO VectorizedSparkParquetReaders: Disabling arrow.enable_null_check_for_get
24/09/28 16:05:16 INFO BaseAllocator: Debug mode disabled. Enable with the VM option -Darrow.memory.debug.allocator=true.
24/09/28 16:05:16 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type
24/09/28 16:05:16 INFO CheckAllocator: Using DefaultAllocationManager at memory/DefaultAllocationManagerFactory.class
24/09/28 16:05:17 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:05:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5486 bytes result sent to driver
24/09/28 16:05:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1220 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/09/28 16:05:17 INFO DAGScheduler: ShuffleMapStage 0 (run at AccessController.java:0) finished in 1.301 s
24/09/28 16:05:17 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:17 INFO DAGScheduler: running: Set()
24/09/28 16:05:17 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:17 INFO DAGScheduler: failed: Set()
24/09/28 16:05:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:17 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/28 16:05:17 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 44.42415 ms
24/09/28 16:05:17 INFO DAGScheduler: Registering RDD 6 (run at AccessController.java:0) as input to shuffle 1
24/09/28 16:05:17 INFO DAGScheduler: Got map stage job 1 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:17 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (run at AccessController.java:0)
24/09/28 16:05:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
24/09/28 16:05:17 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:17 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[6] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 40.0 KiB, free 434.2 MiB)
24/09/28 16:05:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 18.0 KiB, free 434.2 MiB)
24/09/28 16:05:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 1809a5016469:34757 (size: 18.0 KiB, free: 434.4 MiB)
24/09/28 16:05:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[6] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/09/28 16:05:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/28 16:05:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
24/09/28 16:05:17 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 17.517437 ms
24/09/28 16:05:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 8023 bytes result sent to driver
24/09/28 16:05:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 92 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/09/28 16:05:17 INFO DAGScheduler: ShuffleMapStage 2 (run at AccessController.java:0) finished in 0.116 s
24/09/28 16:05:17 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:17 INFO DAGScheduler: running: Set()
24/09/28 16:05:17 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:17 INFO DAGScheduler: failed: Set()
24/09/28 16:05:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 14.260608 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 12.435291 ms
24/09/28 16:05:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:05:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.2 MiB)
24/09/28 16:05:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 1809a5016469:34757 (size: 3.1 KiB, free: 434.4 MiB)
24/09/28 16:05:17 INFO SparkContext: Created broadcast 4 from broadcast at SparkWrite.java:193
24/09/28 16:05:17 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.currency_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/28 16:05:17 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/28 16:05:17 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 1809a5016469:34757 in memory (size: 17.4 KiB, free: 434.4 MiB)
24/09/28 16:05:17 INFO DAGScheduler: Got job 2 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:17 INFO DAGScheduler: Final stage: ResultStage 5 (run at AccessController.java:0)
24/09/28 16:05:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
24/09/28 16:05:17 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:17 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[10] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 47.8 KiB, free 434.2 MiB)
24/09/28 16:05:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 21.8 KiB, free 434.2 MiB)
24/09/28 16:05:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 1809a5016469:34757 (size: 21.8 KiB, free: 434.4 MiB)
24/09/28 16:05:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:17 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/09/28 16:05:17 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/28 16:05:17 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
24/09/28 16:05:17 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 9.98062 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 12.964338 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 5.937413 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 6.463695 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 4.476466 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 5.010122 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 4.850453 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 4.282994 ms
24/09/28 16:05:17 INFO CodeGenerator: Code generated in 5.629427 ms
24/09/28 16:05:18 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 1809a5016469:34757 in memory (size: 18.0 KiB, free: 434.4 MiB)
24/09/28 16:05:18 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/28 16:05:18 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 5.0)
24/09/28 16:05:18 INFO Executor: Finished task 0.0 in stage 5.0 (TID 2). 11924 bytes result sent to driver
24/09/28 16:05:18 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 2) in 425 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:18 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/09/28 16:05:18 INFO DAGScheduler: ResultStage 5 (run at AccessController.java:0) finished in 0.440 s
24/09/28 16:05:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:05:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/09/28 16:05:18 INFO DAGScheduler: Job 2 finished: run at AccessController.java:0, took 0.458829 s
24/09/28 16:05:18 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.currency_dim, format=PARQUET) is committing.
24/09/28 16:05:18 INFO SparkWrite: Committing append with 1 new data files to table gold.currency_dim
24/09/28 16:05:18 INFO SnapshotProducer: Committed snapshot 5660865275238950643 (MergeAppend)
24/09/28 16:05:18 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.currency_dim, snapshotId=5660865275238950643, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.168441693S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=1}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=924}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=924}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1727539033730, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/28 16:05:18 INFO SparkWrite: Committed in 177 ms
24/09/28 16:05:18 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.currency_dim, format=PARQUET) committed.
24/09/28 16:05:18 INFO NessieIcebergClient: Committed 'gold.currency_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=e342a626063649326bbb7830dfe4a5a342c894f153f69f6bc526f22a10e27b18}', expected commit-id was '9583bd54307f21d4ba60204a2e156be312bf88a9501f8e3560a24ce0c70baf1e'
24/09/28 16:05:18 INFO BaseMetastoreTableOperations: Successfully committed to table gold.currency_dim in 40 ms
24/09/28 16:05:18 INFO DAGScheduler: Asked to cancel job group 66bdddc1-d005-4ff0-944f-9f59bf399f90
24/09/28 16:05:18 INFO SparkExecuteStatementOperation: Close statement with 66bdddc1-d005-4ff0-944f-9f59bf399f90
24/09/28 16:05:18 INFO DAGScheduler: Asked to cancel job group 4940fe65-27fb-41ac-8b57-bffdb9326e61
24/09/28 16:05:18 INFO SparkExecuteStatementOperation: Close statement with 4940fe65-27fb-41ac-8b57-bffdb9326e61
24/09/28 16:05:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:18 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:18 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:18 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:18 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/14a1e5c0-e119-48a4-94ca-bce4e2066b45
24/09/28 16:05:18 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 7f7cef1e-121a-4374-b56d-78046a7a4583
24/09/28 16:05:18 INFO SparkExecuteStatementOperation: Running query with 7f7cef1e-121a-4374-b56d-78046a7a4583
24/09/28 16:05:18 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:18 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:18 INFO DAGScheduler: Asked to cancel job group 7f7cef1e-121a-4374-b56d-78046a7a4583
24/09/28 16:05:18 INFO SparkExecuteStatementOperation: Close statement with 7f7cef1e-121a-4374-b56d-78046a7a4583
24/09/28 16:05:18 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:18 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:18 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:18 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:18 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:18 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:18 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:40 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:40 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/85d705a7-f82b-4710-983f-4a0ec1d2b814
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 766f420e-e8e4-4114-b812-0fa7f0d4c178
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Running query with 766f420e-e8e4-4114-b812-0fa7f0d4c178
24/09/28 16:05:40 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:40 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:40 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:40 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:40 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:40 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:40 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:40 INFO DAGScheduler: Asked to cancel job group 766f420e-e8e4-4114-b812-0fa7f0d4c178
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Close statement with 766f420e-e8e4-4114-b812-0fa7f0d4c178
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 031d9d8b-6322-4762-9e4a-5b346edc232f
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Running query with 031d9d8b-6322-4762-9e4a-5b346edc232f
24/09/28 16:05:40 INFO HiveMetaStore: 13: get_databases: *
24/09/28 16:05:40 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:05:40 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:40 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:40 INFO HiveMetaStore: 13: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:40 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:40 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:05:40 INFO DAGScheduler: Asked to cancel job group 031d9d8b-6322-4762-9e4a-5b346edc232f
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Close statement with 031d9d8b-6322-4762-9e4a-5b346edc232f
24/09/28 16:05:40 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:40 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:40 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:40 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:40 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:40 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:40 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:40 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:40 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/0d3f6d73-98f8-40b9-83ed-b79bd37efe9c
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 960df096-a941-464e-99d7-f3a2178f442f
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Running query with 960df096-a941-464e-99d7-f3a2178f442f
24/09/28 16:05:40 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:40 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:40 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:40 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:40 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:40 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:40 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:40 INFO DAGScheduler: Asked to cancel job group 960df096-a941-464e-99d7-f3a2178f442f
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Close statement with 960df096-a941-464e-99d7-f3a2178f442f
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 6ed1469f-099f-4a90-9a17-ac69a6df5f78
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Running query with 6ed1469f-099f-4a90-9a17-ac69a6df5f78
24/09/28 16:05:40 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:40 INFO DAGScheduler: Asked to cancel job group 6ed1469f-099f-4a90-9a17-ac69a6df5f78
24/09/28 16:05:40 INFO SparkExecuteStatementOperation: Close statement with 6ed1469f-099f-4a90-9a17-ac69a6df5f78
24/09/28 16:05:40 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:40 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:40 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:40 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:40 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:40 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:40 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:40 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:40 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/c1c99d79-2667-468c-9944-48dd8178c00e
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 19eb80f2-0fa0-4331-b146-948ca20d233b
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Running query with 19eb80f2-0fa0-4331-b146-948ca20d233b
24/09/28 16:05:41 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:41 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:41 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:41 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:41 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:41 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:41 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:41 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:41 INFO DAGScheduler: Asked to cancel job group 19eb80f2-0fa0-4331-b146-948ca20d233b
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Close statement with 19eb80f2-0fa0-4331-b146-948ca20d233b
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 1aeebaa0-73c0-43e1-8eb1-3311446544a9
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Running query with 1aeebaa0-73c0-43e1-8eb1-3311446544a9
24/09/28 16:05:41 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:41 INFO DAGScheduler: Asked to cancel job group 1aeebaa0-73c0-43e1-8eb1-3311446544a9
24/09/28 16:05:41 ERROR SparkExecuteStatementOperation: Error executing query with 1aeebaa0-73c0-43e1-8eb1-3311446544a9, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#676, tableName#677, isTemporary#678, information#679]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@22f33700, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 6fa5a77e-8cc5-44ba-ad60-a8644dd8f56c
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Running query with 6fa5a77e-8cc5-44ba-ad60-a8644dd8f56c
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:05:41 INFO DAGScheduler: Asked to cancel job group 6fa5a77e-8cc5-44ba-ad60-a8644dd8f56c
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Close statement with 6fa5a77e-8cc5-44ba-ad60-a8644dd8f56c
24/09/28 16:05:41 INFO DAGScheduler: Asked to cancel job group 1aeebaa0-73c0-43e1-8eb1-3311446544a9
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Close statement with 1aeebaa0-73c0-43e1-8eb1-3311446544a9
24/09/28 16:05:41 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:41 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:41 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:41 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:41 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:41 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:41 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:41 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:41 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:41 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/d0a685e4-7009-433b-905e-ee12d9ad319d
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 34c90a74-2eb4-4589-a505-5115633a7334
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Running query with 34c90a74-2eb4-4589-a505-5115633a7334
24/09/28 16:05:41 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:41 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:41 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:41 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:41 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:41 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:41 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:41 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:41 INFO DAGScheduler: Asked to cancel job group 34c90a74-2eb4-4589-a505-5115633a7334
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Close statement with 34c90a74-2eb4-4589-a505-5115633a7334
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.shipping_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with a6fa5c9c-60c3-4041-953f-ae4eba4da788
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Running query with a6fa5c9c-60c3-4041-953f-ae4eba4da788
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.shipping_dim"} */

  
    
        create or replace table nessie.gold.shipping_dim
      
      
    using iceberg
      
      
      
      
      
      

      as
      


WITH src AS (
    SELECT
        src.order_status AS shipping_status,
        src.Fulfilment,
        src.ship_service_level,
        src.fulfilled_by,
        MIN(src.ingestion_date) AS ingestion_date
    FROM 
        nessie.gold.amazon_orders_silver AS src
    
    
    
    
    GROUP BY
        src.order_status,
        src.Fulfilment,
        src.ship_service_level,
        src.fulfilled_by
)

SELECT
    
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
    src.*
FROM
    src
  ' with 2b3cba48-494a-42be-85b3-ec3a15c41183
24/09/28 16:05:41 INFO SparkExecuteStatementOperation: Running query with 2b3cba48-494a-42be-85b3-ec3a15c41183
24/09/28 16:05:41 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:41 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:41 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:41 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:05:41 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=e342a626063649326bbb7830dfe4a5a342c894f153f69f6bc526f22a10e27b18}'
24/09/28 16:05:41 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:05:41 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/28 16:05:41 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/28 16:05:41 INFO SparkScanBuilder: Skipping aggregate pushdown: group by aggregation push down is not supported
24/09/28 16:05:41 INFO V2ScanRelationPushDown: 
Output: Order_Status#732, Fulfilment#733, ship_service_level#735, Fulfilled_By#747, Ingestion_Date#750
         
24/09/28 16:05:41 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 8111912379027843168 created at 2024-09-28T15:59:27.211+00:00 with filter true
24/09/28 16:05:41 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/28 16:05:41 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/28 16:05:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table gold.shipping_dim
24/09/28 16:05:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table gold.shipping_dim
24/09/28 16:05:41 INFO SparkWrite: Requesting [] as write ordering for table gold.shipping_dim
24/09/28 16:05:41 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:05:41 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)
24/09/28 16:05:41 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:41 INFO SparkContext: Created broadcast 6 from broadcast at SparkBatch.java:79
24/09/28 16:05:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:41 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:05:41 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)
24/09/28 16:05:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:41 INFO SparkContext: Created broadcast 7 from broadcast at SparkBatch.java:79
24/09/28 16:05:41 INFO CodeGenerator: Code generated in 42.751407 ms
24/09/28 16:05:41 INFO DAGScheduler: Registering RDD 14 (run at AccessController.java:0) as input to shuffle 2
24/09/28 16:05:41 INFO DAGScheduler: Got map stage job 3 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:41 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (run at AccessController.java:0)
24/09/28 16:05:41 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:05:41 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:41 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:41 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 45.3 KiB, free 434.1 MiB)
24/09/28 16:05:41 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.1 MiB)
24/09/28 16:05:41 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:41 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 1809a5016469:34757 (size: 18.6 KiB, free: 434.3 MiB)
24/09/28 16:05:41 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:41 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 1809a5016469:34757 in memory (size: 21.8 KiB, free: 434.4 MiB)
24/09/28 16:05:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:41 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/09/28 16:05:41 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 3) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 16368 bytes) 
24/09/28 16:05:41 INFO Executor: Running task 0.0 in stage 6.0 (TID 3)
24/09/28 16:05:41 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 1809a5016469:34757 in memory (size: 3.1 KiB, free: 434.4 MiB)
24/09/28 16:05:41 INFO CodeGenerator: Code generated in 76.798077 ms
24/09/28 16:05:41 INFO CodeGenerator: Code generated in 10.81112 ms
24/09/28 16:05:41 INFO CodeGenerator: Code generated in 6.205313 ms
24/09/28 16:05:41 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:05:41 INFO Executor: Finished task 0.0 in stage 6.0 (TID 3). 5400 bytes result sent to driver
24/09/28 16:05:41 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 3) in 163 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:41 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/09/28 16:05:41 INFO DAGScheduler: ShuffleMapStage 6 (run at AccessController.java:0) finished in 0.213 s
24/09/28 16:05:41 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:41 INFO DAGScheduler: running: Set()
24/09/28 16:05:41 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:41 INFO DAGScheduler: failed: Set()
24/09/28 16:05:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:41 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/28 16:05:41 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
24/09/28 16:05:41 INFO CodeGenerator: Code generated in 17.339239 ms
24/09/28 16:05:41 INFO DAGScheduler: Registering RDD 17 (run at AccessController.java:0) as input to shuffle 3
24/09/28 16:05:41 INFO DAGScheduler: Got map stage job 4 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:41 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (run at AccessController.java:0)
24/09/28 16:05:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
24/09/28 16:05:41 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:41 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[17] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:41 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 43.6 KiB, free 434.2 MiB)
24/09/28 16:05:41 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 434.1 MiB)
24/09/28 16:05:41 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 1809a5016469:34757 (size: 18.9 KiB, free: 434.3 MiB)
24/09/28 16:05:41 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[17] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:41 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
24/09/28 16:05:41 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/28 16:05:41 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)
24/09/28 16:05:41 INFO ShuffleBlockFetcherIterator: Getting 1 (440.0 B) non-empty blocks including 1 (440.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/09/28 16:05:41 INFO CodeGenerator: Code generated in 32.165653 ms
24/09/28 16:05:41 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 7937 bytes result sent to driver
24/09/28 16:05:41 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 58 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:41 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/09/28 16:05:41 INFO DAGScheduler: ShuffleMapStage 8 (run at AccessController.java:0) finished in 0.069 s
24/09/28 16:05:41 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:41 INFO DAGScheduler: running: Set()
24/09/28 16:05:42 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:42 INFO DAGScheduler: failed: Set()
24/09/28 16:05:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:42 INFO CodeGenerator: Code generated in 8.505246 ms
24/09/28 16:05:42 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/28 16:05:42 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.1 MiB)
24/09/28 16:05:42 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 1809a5016469:34757 (size: 3.2 KiB, free: 434.3 MiB)
24/09/28 16:05:42 INFO SparkContext: Created broadcast 10 from broadcast at SparkWrite.java:193
24/09/28 16:05:42 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.shipping_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/28 16:05:42 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/28 16:05:42 INFO DAGScheduler: Got job 5 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:42 INFO DAGScheduler: Final stage: ResultStage 11 (run at AccessController.java:0)
24/09/28 16:05:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
24/09/28 16:05:42 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:42 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[21] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:42 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 50.9 KiB, free 434.1 MiB)
24/09/28 16:05:42 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 22.5 KiB, free 434.0 MiB)
24/09/28 16:05:42 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 1809a5016469:34757 (size: 22.5 KiB, free: 434.3 MiB)
24/09/28 16:05:42 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:42 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
24/09/28 16:05:42 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 5) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/28 16:05:42 INFO Executor: Running task 0.0 in stage 11.0 (TID 5)
24/09/28 16:05:42 INFO ShuffleBlockFetcherIterator: Getting 1 (276.0 B) non-empty blocks including 1 (276.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/09/28 16:05:42 INFO CodeGenerator: Code generated in 10.332301 ms
24/09/28 16:05:42 INFO CodeGenerator: Code generated in 6.552621 ms
24/09/28 16:05:42 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:42 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/28 16:05:42 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 1809a5016469:34757 in memory (size: 18.9 KiB, free: 434.3 MiB)
24/09/28 16:05:42 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 1809a5016469:34757 in memory (size: 18.6 KiB, free: 434.4 MiB)
24/09/28 16:05:42 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 11.0)
24/09/28 16:05:42 INFO Executor: Finished task 0.0 in stage 11.0 (TID 5). 12170 bytes result sent to driver
24/09/28 16:05:42 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 5) in 125 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:42 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/09/28 16:05:42 INFO DAGScheduler: ResultStage 11 (run at AccessController.java:0) finished in 0.152 s
24/09/28 16:05:42 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:05:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
24/09/28 16:05:42 INFO DAGScheduler: Job 5 finished: run at AccessController.java:0, took 0.185818 s
24/09/28 16:05:42 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.shipping_dim, format=PARQUET) is committing.
24/09/28 16:05:42 INFO SparkWrite: Committing append with 1 new data files to table gold.shipping_dim
24/09/28 16:05:42 INFO SnapshotProducer: Committed snapshot 238474889566213799 (MergeAppend)
24/09/28 16:05:42 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.shipping_dim, snapshotId=238474889566213799, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.122248223S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=3}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2097}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=2097}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1727539033730, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/28 16:05:42 INFO SparkWrite: Committed in 123 ms
24/09/28 16:05:42 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.shipping_dim, format=PARQUET) committed.
24/09/28 16:05:42 INFO NessieIcebergClient: Committed 'gold.shipping_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=f23c9a38f674076821d7a5d0624a244ecc03a19d4f3af27ca73caf13dd7204a2}', expected commit-id was 'e342a626063649326bbb7830dfe4a5a342c894f153f69f6bc526f22a10e27b18'
24/09/28 16:05:42 INFO BaseMetastoreTableOperations: Successfully committed to table gold.shipping_dim in 38 ms
24/09/28 16:05:42 INFO DAGScheduler: Asked to cancel job group 2b3cba48-494a-42be-85b3-ec3a15c41183
24/09/28 16:05:42 INFO SparkExecuteStatementOperation: Close statement with 2b3cba48-494a-42be-85b3-ec3a15c41183
24/09/28 16:05:42 INFO DAGScheduler: Asked to cancel job group a6fa5c9c-60c3-4041-953f-ae4eba4da788
24/09/28 16:05:42 INFO SparkExecuteStatementOperation: Close statement with a6fa5c9c-60c3-4041-953f-ae4eba4da788
24/09/28 16:05:42 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:42 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:42 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:42 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:42 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:42 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:42 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:42 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:42 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:42 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/52fde570-56b8-49e4-95fa-e6edd4d0a230
24/09/28 16:05:42 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 69ad47fd-00c2-44c1-9170-ab0a8a731c67
24/09/28 16:05:42 INFO SparkExecuteStatementOperation: Running query with 69ad47fd-00c2-44c1-9170-ab0a8a731c67
24/09/28 16:05:42 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:42 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:42 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:42 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:42 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:42 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:42 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:42 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:42 INFO DAGScheduler: Asked to cancel job group 69ad47fd-00c2-44c1-9170-ab0a8a731c67
24/09/28 16:05:42 INFO SparkExecuteStatementOperation: Close statement with 69ad47fd-00c2-44c1-9170-ab0a8a731c67
24/09/28 16:05:42 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:42 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:42 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:42 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:42 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:42 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:42 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:42 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:47 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:47 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/7f6d2133-77c5-4276-a8b9-7a921da02751
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with cf76f798-4d26-4839-8922-89c442e37a7c
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Running query with cf76f798-4d26-4839-8922-89c442e37a7c
24/09/28 16:05:47 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:47 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:47 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:47 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:47 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:47 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:47 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:47 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:47 INFO DAGScheduler: Asked to cancel job group cf76f798-4d26-4839-8922-89c442e37a7c
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Close statement with cf76f798-4d26-4839-8922-89c442e37a7c
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 7e152517-ffab-4628-b204-bc111010ec03
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Running query with 7e152517-ffab-4628-b204-bc111010ec03
24/09/28 16:05:47 INFO HiveMetaStore: 14: get_databases: *
24/09/28 16:05:47 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:05:47 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:47 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:47 INFO HiveMetaStore: 14: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:47 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:47 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:47 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:05:47 INFO DAGScheduler: Asked to cancel job group 7e152517-ffab-4628-b204-bc111010ec03
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Close statement with 7e152517-ffab-4628-b204-bc111010ec03
24/09/28 16:05:47 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:47 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:47 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:47 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:47 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:47 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:47 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:47 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:47 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:47 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/5c51815e-c875-4a22-8ede-3a109d774577
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with cb7201c6-f58f-4cfd-99dd-b3f1fd7f1c2a
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Running query with cb7201c6-f58f-4cfd-99dd-b3f1fd7f1c2a
24/09/28 16:05:47 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:47 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:47 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:47 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:47 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:47 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:47 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:47 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:47 INFO DAGScheduler: Asked to cancel job group cb7201c6-f58f-4cfd-99dd-b3f1fd7f1c2a
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Close statement with cb7201c6-f58f-4cfd-99dd-b3f1fd7f1c2a
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 41a78928-7872-4465-b2b9-064553f2c397
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Running query with 41a78928-7872-4465-b2b9-064553f2c397
24/09/28 16:05:47 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:47 INFO DAGScheduler: Asked to cancel job group 41a78928-7872-4465-b2b9-064553f2c397
24/09/28 16:05:47 INFO SparkExecuteStatementOperation: Close statement with 41a78928-7872-4465-b2b9-064553f2c397
24/09/28 16:05:47 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:47 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:47 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:47 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:47 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:47 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:47 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:47 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:48 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:48 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/09baf417-782b-427e-a1e1-bdb6a5339ee2
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 91f05866-25f7-4c2c-b5ff-f36b005352f8
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Running query with 91f05866-25f7-4c2c-b5ff-f36b005352f8
24/09/28 16:05:48 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:48 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:48 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:48 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:48 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:48 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:48 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:48 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:48 INFO DAGScheduler: Asked to cancel job group 91f05866-25f7-4c2c-b5ff-f36b005352f8
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Close statement with 91f05866-25f7-4c2c-b5ff-f36b005352f8
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 2b76ab1f-c383-4e37-a473-bc11d7d67a20
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Running query with 2b76ab1f-c383-4e37-a473-bc11d7d67a20
24/09/28 16:05:48 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:48 INFO DAGScheduler: Asked to cancel job group 2b76ab1f-c383-4e37-a473-bc11d7d67a20
24/09/28 16:05:48 ERROR SparkExecuteStatementOperation: Error executing query with 2b76ab1f-c383-4e37-a473-bc11d7d67a20, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#790, tableName#791, isTemporary#792, information#793]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@71d93ca8, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 4224ebd1-e90a-453a-b8de-5a2f4d08a95a
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Running query with 4224ebd1-e90a-453a-b8de-5a2f4d08a95a
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:05:48 INFO DAGScheduler: Asked to cancel job group 4224ebd1-e90a-453a-b8de-5a2f4d08a95a
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Close statement with 4224ebd1-e90a-453a-b8de-5a2f4d08a95a
24/09/28 16:05:48 INFO DAGScheduler: Asked to cancel job group 2b76ab1f-c383-4e37-a473-bc11d7d67a20
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Close statement with 2b76ab1f-c383-4e37-a473-bc11d7d67a20
24/09/28 16:05:48 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:48 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:48 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:48 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:48 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:48 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:48 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:48 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:48 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:48 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/cee325ac-8ff6-4b59-af0a-9c9e46bb7e86
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with c25da7c4-6fd2-4ff4-a6c0-c158428bd3e8
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Running query with c25da7c4-6fd2-4ff4-a6c0-c158428bd3e8
24/09/28 16:05:48 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:48 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:48 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:48 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:48 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:48 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:48 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:48 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:48 INFO DAGScheduler: Asked to cancel job group c25da7c4-6fd2-4ff4-a6c0-c158428bd3e8
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Close statement with c25da7c4-6fd2-4ff4-a6c0-c158428bd3e8
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.date_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with 00dec2a7-bbe3-4ed1-824a-1ecbba49170f
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Running query with 00dec2a7-bbe3-4ed1-824a-1ecbba49170f
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.date_dim"} */

  
    
        create or replace table nessie.gold.date_dim
      
      
    using iceberg
      
      
      partitioned by (MONTH(full_date))
      
      
      

      as
      

WITH date_series AS (
    SELECT
        src.order_date AS full_date,
        MIN(ingestion_date) AS ingestion_date
    FROM
        nessie.gold.amazon_orders_silver AS src
    
    
    
    GROUP BY
        src.order_date
),
date_attributes AS (
    SELECT
        
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
        full_date AS full_date,
        EXTRACT (YEAR FROM full_date) AS year,
        EXTRACT (MONTH FROM full_date) AS month,
        EXTRACT (DAY FROM full_date) AS day,
        CASE 
            WHEN EXTRACT(MONTH FROM full_date) IN (1, 2, 12) THEN 'Winter'
            WHEN EXTRACT(MONTH FROM full_date) IN (3, 4, 5) THEN 'Spring'
            WHEN EXTRACT(MONTH FROM full_date) IN (6, 7, 8) THEN 'Summer'
            ELSE 'Fall'
        END AS season,
        WEEKOFYEAR(full_date) AS week_of_year,
        ingestion_date
    FROM
        date_series
)

SELECT 
    *
FROM 
    date_attributes
  ' with 213e9dc5-54c2-4575-873f-4aabdc98503e
24/09/28 16:05:48 INFO SparkExecuteStatementOperation: Running query with 213e9dc5-54c2-4575-873f-4aabdc98503e
24/09/28 16:05:48 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:48 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:48 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:48 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:05:48 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=f23c9a38f674076821d7a5d0624a244ecc03a19d4f3af27ca73caf13dd7204a2}'
24/09/28 16:05:48 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:05:48 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/28 16:05:48 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/28 16:05:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:48 INFO SparkScanBuilder: Skipping aggregate pushdown: group by aggregation push down is not supported
24/09/28 16:05:48 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:48 INFO V2ScanRelationPushDown: 
Output: Order_Date#851, Ingestion_Date#870
         
24/09/28 16:05:48 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 8111912379027843168 created at 2024-09-28T15:59:27.211+00:00 with filter true
24/09/28 16:05:48 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 1809a5016469:34757 in memory (size: 22.5 KiB, free: 434.4 MiB)
24/09/28 16:05:48 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/28 16:05:48 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/28 16:05:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:48 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 1809a5016469:34757 in memory (size: 3.2 KiB, free: 434.4 MiB)
24/09/28 16:05:48 INFO SparkWrite: Requesting 402653184 bytes advisory partition size for table gold.date_dim
24/09/28 16:05:48 INFO SparkWrite: Requesting ClusteredDistribution(months(full_date)) as write distribution for table gold.date_dim
24/09/28 16:05:48 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:48 INFO SparkWrite: Requesting [] as write ordering for table gold.date_dim
24/09/28 16:05:48 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
24/09/28 16:05:48 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.4 MiB)
24/09/28 16:05:48 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:48 INFO SparkContext: Created broadcast 12 from broadcast at SparkBatch.java:79
24/09/28 16:05:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:48 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/28 16:05:48 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/28 16:05:48 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:48 INFO SparkContext: Created broadcast 13 from broadcast at SparkBatch.java:79
24/09/28 16:05:48 INFO CodeGenerator: Code generated in 29.802883 ms
24/09/28 16:05:48 INFO DAGScheduler: Registering RDD 25 (run at AccessController.java:0) as input to shuffle 4
24/09/28 16:05:48 INFO DAGScheduler: Got map stage job 6 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:48 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (run at AccessController.java:0)
24/09/28 16:05:48 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:05:48 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:48 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[25] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 39.6 KiB, free 434.3 MiB)
24/09/28 16:05:48 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 434.3 MiB)
24/09/28 16:05:48 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 1809a5016469:34757 (size: 17.2 KiB, free: 434.4 MiB)
24/09/28 16:05:48 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:48 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[25] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:48 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
24/09/28 16:05:48 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 6) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 16167 bytes) 
24/09/28 16:05:48 INFO Executor: Running task 0.0 in stage 12.0 (TID 6)
24/09/28 16:05:49 INFO CodeGenerator: Code generated in 27.033893 ms
24/09/28 16:05:49 INFO CodeGenerator: Code generated in 6.940355 ms
24/09/28 16:05:49 INFO CodeGenerator: Code generated in 4.398951 ms
24/09/28 16:05:49 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:05:49 INFO Executor: Finished task 0.0 in stage 12.0 (TID 6). 5400 bytes result sent to driver
24/09/28 16:05:49 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 6) in 120 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:49 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
24/09/28 16:05:49 INFO DAGScheduler: ShuffleMapStage 12 (run at AccessController.java:0) finished in 0.129 s
24/09/28 16:05:49 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:49 INFO DAGScheduler: running: Set()
24/09/28 16:05:49 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:49 INFO DAGScheduler: failed: Set()
24/09/28 16:05:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:49 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/28 16:05:49 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
24/09/28 16:05:49 INFO CodeGenerator: Code generated in 36.714912 ms
24/09/28 16:05:49 INFO DAGScheduler: Registering RDD 28 (run at AccessController.java:0) as input to shuffle 5
24/09/28 16:05:49 INFO DAGScheduler: Got map stage job 7 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:49 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (run at AccessController.java:0)
24/09/28 16:05:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
24/09/28 16:05:49 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:49 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[28] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:49 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 49.1 KiB, free 434.2 MiB)
24/09/28 16:05:49 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 434.2 MiB)
24/09/28 16:05:49 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 1809a5016469:34757 (size: 20.4 KiB, free: 434.4 MiB)
24/09/28 16:05:49 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[28] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:49 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
24/09/28 16:05:49 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 7) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/28 16:05:49 INFO Executor: Running task 0.0 in stage 14.0 (TID 7)
24/09/28 16:05:49 INFO ShuffleBlockFetcherIterator: Getting 1 (66.0 B) non-empty blocks including 1 (66.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
24/09/28 16:05:49 INFO CodeGenerator: Code generated in 31.516066 ms
24/09/28 16:05:49 INFO Executor: Finished task 0.0 in stage 14.0 (TID 7). 7937 bytes result sent to driver
24/09/28 16:05:49 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 7) in 66 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:49 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/09/28 16:05:49 INFO DAGScheduler: ShuffleMapStage 14 (run at AccessController.java:0) finished in 0.082 s
24/09/28 16:05:49 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:49 INFO DAGScheduler: running: Set()
24/09/28 16:05:49 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:49 INFO DAGScheduler: failed: Set()
24/09/28 16:05:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:49 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 1809a5016469:34757 in memory (size: 17.2 KiB, free: 434.4 MiB)
24/09/28 16:05:49 INFO CodeGenerator: Code generated in 30.91201 ms
24/09/28 16:05:49 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 1809a5016469:34757 in memory (size: 20.4 KiB, free: 434.4 MiB)
24/09/28 16:05:49 INFO DAGScheduler: Registering RDD 33 (run at AccessController.java:0) as input to shuffle 6
24/09/28 16:05:49 INFO DAGScheduler: Got map stage job 8 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:49 INFO DAGScheduler: Final stage: ShuffleMapStage 17 (run at AccessController.java:0)
24/09/28 16:05:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
24/09/28 16:05:49 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:49 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[33] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:49 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 53.1 KiB, free 434.3 MiB)
24/09/28 16:05:49 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 23.0 KiB, free 434.3 MiB)
24/09/28 16:05:49 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 1809a5016469:34757 (size: 23.0 KiB, free: 434.4 MiB)
24/09/28 16:05:49 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[33] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:49 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
24/09/28 16:05:49 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 8) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/28 16:05:49 INFO Executor: Running task 0.0 in stage 17.0 (TID 8)
24/09/28 16:05:49 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/28 16:05:49 INFO CodeGenerator: Code generated in 9.287546 ms
24/09/28 16:05:49 INFO CodeGenerator: Code generated in 8.337833 ms
24/09/28 16:05:49 INFO CodeGenerator: Code generated in 17.011077 ms
24/09/28 16:05:49 INFO Executor: Finished task 0.0 in stage 17.0 (TID 8). 9806 bytes result sent to driver
24/09/28 16:05:49 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 8) in 142 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:49 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
24/09/28 16:05:49 INFO DAGScheduler: ShuffleMapStage 17 (run at AccessController.java:0) finished in 0.169 s
24/09/28 16:05:49 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:49 INFO DAGScheduler: running: Set()
24/09/28 16:05:49 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:49 INFO DAGScheduler: failed: Set()
24/09/28 16:05:49 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576
24/09/28 16:05:49 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:05:49 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.2 MiB)
24/09/28 16:05:49 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 1809a5016469:34757 (size: 3.2 KiB, free: 434.4 MiB)
24/09/28 16:05:49 INFO SparkContext: Created broadcast 17 from broadcast at SparkWrite.java:193
24/09/28 16:05:49 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.date_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/28 16:05:49 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/28 16:05:49 INFO DAGScheduler: Got job 9 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:49 INFO DAGScheduler: Final stage: ResultStage 21 (run at AccessController.java:0)
24/09/28 16:05:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
24/09/28 16:05:49 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:49 INFO DAGScheduler: Submitting ResultStage 21 (ShuffledRowRDD[34] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:49 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 10.7 KiB, free 434.2 MiB)
24/09/28 16:05:49 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 434.2 MiB)
24/09/28 16:05:49 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 1809a5016469:34757 (size: 5.7 KiB, free: 434.4 MiB)
24/09/28 16:05:49 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (ShuffledRowRDD[34] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:49 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
24/09/28 16:05:49 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 9) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/28 16:05:49 INFO Executor: Running task 0.0 in stage 21.0 (TID 9)
24/09/28 16:05:49 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
24/09/28 16:05:49 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/28 16:05:49 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 21.0)
24/09/28 16:05:49 INFO Executor: Finished task 0.0 in stage 21.0 (TID 9). 6906 bytes result sent to driver
24/09/28 16:05:49 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 9) in 107 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:49 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
24/09/28 16:05:49 INFO DAGScheduler: ResultStage 21 (run at AccessController.java:0) finished in 0.130 s
24/09/28 16:05:49 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:05:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
24/09/28 16:05:49 INFO DAGScheduler: Job 9 finished: run at AccessController.java:0, took 0.145244 s
24/09/28 16:05:49 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.date_dim, format=PARQUET) is committing.
24/09/28 16:05:49 INFO SparkWrite: Committing append with 1 new data files to table gold.date_dim
24/09/28 16:05:50 INFO SnapshotProducer: Committed snapshot 8761048072987420580 (MergeAppend)
24/09/28 16:05:50 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.date_dim, snapshotId=8761048072987420580, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.246478913S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=1}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2100}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=2100}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1727539033730, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/28 16:05:50 INFO SparkWrite: Committed in 246 ms
24/09/28 16:05:50 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.date_dim, format=PARQUET) committed.
24/09/28 16:05:50 INFO NessieIcebergClient: Committed 'gold.date_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=15d971ee4a162083674e71a5ed378cfb8f77d8911cb8dfc96f784c6c0e51b335}', expected commit-id was 'f23c9a38f674076821d7a5d0624a244ecc03a19d4f3af27ca73caf13dd7204a2'
24/09/28 16:05:50 INFO BaseMetastoreTableOperations: Successfully committed to table gold.date_dim in 38 ms
24/09/28 16:05:50 INFO DAGScheduler: Asked to cancel job group 213e9dc5-54c2-4575-873f-4aabdc98503e
24/09/28 16:05:50 INFO SparkExecuteStatementOperation: Close statement with 213e9dc5-54c2-4575-873f-4aabdc98503e
24/09/28 16:05:50 INFO DAGScheduler: Asked to cancel job group 00dec2a7-bbe3-4ed1-824a-1ecbba49170f
24/09/28 16:05:50 INFO SparkExecuteStatementOperation: Close statement with 00dec2a7-bbe3-4ed1-824a-1ecbba49170f
24/09/28 16:05:50 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:50 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:50 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:50 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:50 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:50 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:50 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:50 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:50 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:50 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/a6c3e7bc-545b-4ca0-8126-6188b7c77ecd
24/09/28 16:05:50 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 2545b48b-b590-4d33-996a-3a87bcd95116
24/09/28 16:05:50 INFO SparkExecuteStatementOperation: Running query with 2545b48b-b590-4d33-996a-3a87bcd95116
24/09/28 16:05:50 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:50 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:50 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:50 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:50 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:50 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:50 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:50 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:50 INFO DAGScheduler: Asked to cancel job group 2545b48b-b590-4d33-996a-3a87bcd95116
24/09/28 16:05:50 INFO SparkExecuteStatementOperation: Close statement with 2545b48b-b590-4d33-996a-3a87bcd95116
24/09/28 16:05:50 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:50 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:50 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:50 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:50 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:50 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:50 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:50 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:52 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:52 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/0d3f5f83-d1f0-468a-a827-ff300acba3b2
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with b20fb760-0178-4855-8568-9ca8e602ffc8
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Running query with b20fb760-0178-4855-8568-9ca8e602ffc8
24/09/28 16:05:52 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:52 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:52 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:52 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:52 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:52 INFO DAGScheduler: Asked to cancel job group b20fb760-0178-4855-8568-9ca8e602ffc8
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Close statement with b20fb760-0178-4855-8568-9ca8e602ffc8
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 6ecf22eb-ed2f-4a4c-b606-594e2908bf61
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Running query with 6ecf22eb-ed2f-4a4c-b606-594e2908bf61
24/09/28 16:05:52 INFO HiveMetaStore: 15: get_databases: *
24/09/28 16:05:52 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:05:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:52 INFO HiveMetaStore: 15: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:52 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:52 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:05:52 INFO DAGScheduler: Asked to cancel job group 6ecf22eb-ed2f-4a4c-b606-594e2908bf61
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Close statement with 6ecf22eb-ed2f-4a4c-b606-594e2908bf61
24/09/28 16:05:52 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:52 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:52 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:52 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:52 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:52 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:52 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:52 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:52 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/7ba22e0d-f275-4cce-a942-9476212948e7
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 49994db6-2a69-45ef-b716-54d5805729f7
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Running query with 49994db6-2a69-45ef-b716-54d5805729f7
24/09/28 16:05:52 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:52 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:52 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:52 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:52 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:52 INFO DAGScheduler: Asked to cancel job group 49994db6-2a69-45ef-b716-54d5805729f7
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Close statement with 49994db6-2a69-45ef-b716-54d5805729f7
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 8e24725f-f1bb-4c34-bd06-8af4d9a64dbb
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Running query with 8e24725f-f1bb-4c34-bd06-8af4d9a64dbb
24/09/28 16:05:52 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:52 INFO DAGScheduler: Asked to cancel job group 8e24725f-f1bb-4c34-bd06-8af4d9a64dbb
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Close statement with 8e24725f-f1bb-4c34-bd06-8af4d9a64dbb
24/09/28 16:05:52 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:52 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:52 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:52 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:52 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:52 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:52 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:52 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:52 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/d32e4a02-9d62-4254-a259-2cbf838129bc
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 5a33e3e2-23ae-4da2-93f8-3044d494dc6a
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Running query with 5a33e3e2-23ae-4da2-93f8-3044d494dc6a
24/09/28 16:05:52 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:52 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:52 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:52 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:52 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:52 INFO DAGScheduler: Asked to cancel job group 5a33e3e2-23ae-4da2-93f8-3044d494dc6a
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Close statement with 5a33e3e2-23ae-4da2-93f8-3044d494dc6a
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 42477c46-f58b-4a5c-8b57-ede22debfca3
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Running query with 42477c46-f58b-4a5c-8b57-ede22debfca3
24/09/28 16:05:52 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:52 INFO DAGScheduler: Asked to cancel job group 42477c46-f58b-4a5c-8b57-ede22debfca3
24/09/28 16:05:52 ERROR SparkExecuteStatementOperation: Error executing query with 42477c46-f58b-4a5c-8b57-ede22debfca3, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#909, tableName#910, isTemporary#911, information#912]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@6fcde8a3, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with d6ec9a12-088e-4731-841c-222697514e1d
24/09/28 16:05:52 INFO SparkExecuteStatementOperation: Running query with d6ec9a12-088e-4731-841c-222697514e1d
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:05:53 INFO DAGScheduler: Asked to cancel job group d6ec9a12-088e-4731-841c-222697514e1d
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Close statement with d6ec9a12-088e-4731-841c-222697514e1d
24/09/28 16:05:53 INFO DAGScheduler: Asked to cancel job group 42477c46-f58b-4a5c-8b57-ede22debfca3
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Close statement with 42477c46-f58b-4a5c-8b57-ede22debfca3
24/09/28 16:05:53 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:53 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:53 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:53 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:53 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:53 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:53 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:53 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:53 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:53 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/fceaf2b9-c5d9-4d61-bfc2-941143525003
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 2b25deb8-f7a3-4f85-8d9d-1367d4d3b282
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Running query with 2b25deb8-f7a3-4f85-8d9d-1367d4d3b282
24/09/28 16:05:53 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:53 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:53 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 1809a5016469:34757 in memory (size: 3.2 KiB, free: 434.4 MiB)
24/09/28 16:05:53 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:53 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:53 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:53 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:53 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:53 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:53 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 1809a5016469:34757 in memory (size: 23.0 KiB, free: 434.4 MiB)
24/09/28 16:05:53 INFO DAGScheduler: Asked to cancel job group 2b25deb8-f7a3-4f85-8d9d-1367d4d3b282
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Close statement with 2b25deb8-f7a3-4f85-8d9d-1367d4d3b282
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.location_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with f64ded5b-0538-4eee-ad23-ca0f5552fb06
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Running query with f64ded5b-0538-4eee-ad23-ca0f5552fb06
24/09/28 16:05:53 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 1809a5016469:34757 in memory (size: 5.7 KiB, free: 434.4 MiB)
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.location_dim"} */

  
    
        create or replace table nessie.gold.location_dim
      
      
    using iceberg
      
      
      partitioned by (ship_country,ship_state)
      
      
      

      as
      


WITH src AS (
    SELECT
        src.ship_country,
        src.ship_state,
        src.ship_city,
        src.ship_postal_code,
        MIN(src.ingestion_date) AS ingestion_date
    FROM 
        nessie.gold.amazon_orders_silver AS src
    
    
    
    
    
    GROUP BY
        src.ship_country,
        src.ship_state,
        src.ship_city,
        src.ship_postal_code
)

SELECT
    
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
    src.*
FROM
    src
  ' with e721e757-d4db-440c-be1d-2359d1d82f39
24/09/28 16:05:53 INFO SparkExecuteStatementOperation: Running query with e721e757-d4db-440c-be1d-2359d1d82f39
24/09/28 16:05:53 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:53 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:53 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:53 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:05:53 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=15d971ee4a162083674e71a5ed378cfb8f77d8911cb8dfc96f784c6c0e51b335}'
24/09/28 16:05:53 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:05:53 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/28 16:05:53 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/28 16:05:53 INFO SparkScanBuilder: Skipping aggregate pushdown: group by aggregation push down is not supported
24/09/28 16:05:53 INFO V2ScanRelationPushDown: 
Output: Ship_City#974, Ship_State#975, Ship_Postal_Code#976, Ship_Country#977, Ingestion_Date#982
         
24/09/28 16:05:53 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 8111912379027843168 created at 2024-09-28T15:59:27.211+00:00 with filter true
24/09/28 16:05:53 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/28 16:05:53 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/28 16:05:53 INFO SparkWrite: Requesting 402653184 bytes advisory partition size for table gold.location_dim
24/09/28 16:05:53 INFO SparkWrite: Requesting ClusteredDistribution(ship_country, ship_state) as write distribution for table gold.location_dim
24/09/28 16:05:53 INFO SparkWrite: Requesting [] as write ordering for table gold.location_dim
24/09/28 16:05:53 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/28 16:05:53 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/28 16:05:53 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:53 INFO SparkContext: Created broadcast 19 from broadcast at SparkBatch.java:79
24/09/28 16:05:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:53 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/28 16:05:53 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/28 16:05:53 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:53 INFO SparkContext: Created broadcast 20 from broadcast at SparkBatch.java:79
24/09/28 16:05:53 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:54 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 216.447169 ms
24/09/28 16:05:54 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:54 INFO DAGScheduler: Registering RDD 38 (run at AccessController.java:0) as input to shuffle 7
24/09/28 16:05:54 INFO DAGScheduler: Got map stage job 10 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:54 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (run at AccessController.java:0)
24/09/28 16:05:54 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:05:54 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:54 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:54 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[38] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 45.0 KiB, free 434.3 MiB)
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 434.3 MiB)
24/09/28 16:05:54 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 1809a5016469:34757 (size: 18.6 KiB, free: 434.4 MiB)
24/09/28 16:05:54 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[38] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:54 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
24/09/28 16:05:54 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 10) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 16363 bytes) 
24/09/28 16:05:54 INFO Executor: Running task 0.0 in stage 22.0 (TID 10)
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 93.524759 ms
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 32.533125 ms
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 20.663138 ms
24/09/28 16:05:54 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:05:54 INFO Executor: Finished task 0.0 in stage 22.0 (TID 10). 5400 bytes result sent to driver
24/09/28 16:05:54 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 10) in 302 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:54 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
24/09/28 16:05:54 INFO DAGScheduler: ShuffleMapStage 22 (run at AccessController.java:0) finished in 0.324 s
24/09/28 16:05:54 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:54 INFO DAGScheduler: running: Set()
24/09/28 16:05:54 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:54 INFO DAGScheduler: failed: Set()
24/09/28 16:05:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:54 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/28 16:05:54 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 31.144766 ms
24/09/28 16:05:54 INFO DAGScheduler: Registering RDD 41 (run at AccessController.java:0) as input to shuffle 8
24/09/28 16:05:54 INFO DAGScheduler: Got map stage job 11 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:54 INFO DAGScheduler: Final stage: ShuffleMapStage 24 (run at AccessController.java:0)
24/09/28 16:05:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
24/09/28 16:05:54 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:54 INFO DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[41] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 43.6 KiB, free 434.2 MiB)
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.0 KiB, free 434.2 MiB)
24/09/28 16:05:54 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 1809a5016469:34757 (size: 19.0 KiB, free: 434.4 MiB)
24/09/28 16:05:54 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[41] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:54 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
24/09/28 16:05:54 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 11) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/28 16:05:54 INFO Executor: Running task 0.0 in stage 24.0 (TID 11)
24/09/28 16:05:54 INFO ShuffleBlockFetcherIterator: Getting 1 (2.2 KiB) non-empty blocks including 1 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 30.643004 ms
24/09/28 16:05:54 INFO Executor: Finished task 0.0 in stage 24.0 (TID 11). 7937 bytes result sent to driver
24/09/28 16:05:54 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 11) in 72 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:54 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
24/09/28 16:05:54 INFO DAGScheduler: ShuffleMapStage 24 (run at AccessController.java:0) finished in 0.091 s
24/09/28 16:05:54 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:54 INFO DAGScheduler: running: Set()
24/09/28 16:05:54 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:54 INFO DAGScheduler: failed: Set()
24/09/28 16:05:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:54 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 1809a5016469:34757 in memory (size: 18.6 KiB, free: 434.4 MiB)
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 35.003431 ms
24/09/28 16:05:54 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 1809a5016469:34757 in memory (size: 19.0 KiB, free: 434.4 MiB)
24/09/28 16:05:54 INFO DAGScheduler: Registering RDD 46 (run at AccessController.java:0) as input to shuffle 9
24/09/28 16:05:54 INFO DAGScheduler: Got map stage job 12 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:54 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (run at AccessController.java:0)
24/09/28 16:05:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
24/09/28 16:05:54 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:54 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[46] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 49.1 KiB, free 434.3 MiB)
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 21.2 KiB, free 434.3 MiB)
24/09/28 16:05:54 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 1809a5016469:34757 (size: 21.2 KiB, free: 434.4 MiB)
24/09/28 16:05:54 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[46] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:54 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
24/09/28 16:05:54 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 12) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/28 16:05:54 INFO Executor: Running task 0.0 in stage 27.0 (TID 12)
24/09/28 16:05:54 INFO ShuffleBlockFetcherIterator: Getting 1 (868.0 B) non-empty blocks including 1 (868.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 17.483113 ms
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 7.026004 ms
24/09/28 16:05:54 INFO CodeGenerator: Code generated in 35.991059 ms
24/09/28 16:05:54 INFO Executor: Finished task 0.0 in stage 27.0 (TID 12). 9806 bytes result sent to driver
24/09/28 16:05:54 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 12) in 165 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:54 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
24/09/28 16:05:54 INFO DAGScheduler: ShuffleMapStage 27 (run at AccessController.java:0) finished in 0.176 s
24/09/28 16:05:54 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:54 INFO DAGScheduler: running: Set()
24/09/28 16:05:54 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:54 INFO DAGScheduler: failed: Set()
24/09/28 16:05:54 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.2 MiB)
24/09/28 16:05:54 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 1809a5016469:34757 (size: 3.2 KiB, free: 434.4 MiB)
24/09/28 16:05:54 INFO SparkContext: Created broadcast 24 from broadcast at SparkWrite.java:193
24/09/28 16:05:54 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.location_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/28 16:05:54 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/28 16:05:54 INFO DAGScheduler: Got job 13 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:54 INFO DAGScheduler: Final stage: ResultStage 31 (run at AccessController.java:0)
24/09/28 16:05:54 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)
24/09/28 16:05:54 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:54 INFO DAGScheduler: Submitting ResultStage 31 (ShuffledRowRDD[47] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 10.5 KiB, free 434.2 MiB)
24/09/28 16:05:54 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.2 MiB)
24/09/28 16:05:54 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 1809a5016469:34757 (size: 5.6 KiB, free: 434.4 MiB)
24/09/28 16:05:55 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (ShuffledRowRDD[47] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:55 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
24/09/28 16:05:55 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 13) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/28 16:05:55 INFO Executor: Running task 0.0 in stage 31.0 (TID 13)
24/09/28 16:05:55 INFO ShuffleBlockFetcherIterator: Getting 1 (2.0 KiB) non-empty blocks including 1 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:55 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/db996311-4632-4a1f-a24a-26c6a52faaa9
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 6b5f22f5-344a-4d0a-a44f-640a3d76297e
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Running query with 6b5f22f5-344a-4d0a-a44f-640a3d76297e
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:55 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:55 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:55 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO DAGScheduler: Asked to cancel job group 6b5f22f5-344a-4d0a-a44f-640a3d76297e
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Close statement with 6b5f22f5-344a-4d0a-a44f-640a3d76297e
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 5599aa81-0f85-45bd-b505-3e571e667893
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Running query with 5599aa81-0f85-45bd-b505-3e571e667893
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:55 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/28 16:05:55 INFO HiveMetaStore: 16: get_databases: *
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:05:55 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:55 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:55 INFO HiveMetaStore: 16: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:55 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:55 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:05:55 INFO DAGScheduler: Asked to cancel job group 5599aa81-0f85-45bd-b505-3e571e667893
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Close statement with 5599aa81-0f85-45bd-b505-3e571e667893
24/09/28 16:05:55 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:55 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:55 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:55 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:55 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:55 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:55 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/b6b5d5f5-4e24-46d0-8b1b-dfcaee1e81bc
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 789d43b0-5d6c-434b-923c-bcbb18ef1c46
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Running query with 789d43b0-5d6c-434b-923c-bcbb18ef1c46
24/09/28 16:05:55 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:55 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:55 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:55 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:55 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:55 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:55 INFO DAGScheduler: Asked to cancel job group 789d43b0-5d6c-434b-923c-bcbb18ef1c46
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Close statement with 789d43b0-5d6c-434b-923c-bcbb18ef1c46
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 537fa504-0e30-409f-a3e2-ad985019abb4
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Running query with 537fa504-0e30-409f-a3e2-ad985019abb4
24/09/28 16:05:55 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:55 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 1809a5016469:34757 in memory (size: 21.2 KiB, free: 434.4 MiB)
24/09/28 16:05:55 INFO DAGScheduler: Asked to cancel job group 537fa504-0e30-409f-a3e2-ad985019abb4
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Close statement with 537fa504-0e30-409f-a3e2-ad985019abb4
24/09/28 16:05:55 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:55 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:55 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:55 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:55 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:55 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:55 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/f7ab0740-721b-4075-8ee1-9c817828f862
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 8af1d65e-cef2-4d9f-b8f7-1f260e9f8b1f
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Running query with 8af1d65e-cef2-4d9f-b8f7-1f260e9f8b1f
24/09/28 16:05:55 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:55 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:55 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:55 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:55 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:55 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:55 INFO DAGScheduler: Asked to cancel job group 8af1d65e-cef2-4d9f-b8f7-1f260e9f8b1f
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Close statement with 8af1d65e-cef2-4d9f-b8f7-1f260e9f8b1f
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 8a779122-c636-4d49-8aac-2b705b248834
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Running query with 8a779122-c636-4d49-8aac-2b705b248834
24/09/28 16:05:55 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:55 INFO DAGScheduler: Asked to cancel job group 8a779122-c636-4d49-8aac-2b705b248834
24/09/28 16:05:55 ERROR SparkExecuteStatementOperation: Error executing query with 8a779122-c636-4d49-8aac-2b705b248834, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#1020, tableName#1021, isTemporary#1022, information#1023]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@58051570, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 83ef5e08-92fe-4609-bf69-06dc938261ed
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Running query with 83ef5e08-92fe-4609-bf69-06dc938261ed
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:05:55 INFO DAGScheduler: Asked to cancel job group 83ef5e08-92fe-4609-bf69-06dc938261ed
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Close statement with 83ef5e08-92fe-4609-bf69-06dc938261ed
24/09/28 16:05:55 INFO DAGScheduler: Asked to cancel job group 8a779122-c636-4d49-8aac-2b705b248834
24/09/28 16:05:55 INFO SparkExecuteStatementOperation: Close statement with 8a779122-c636-4d49-8aac-2b705b248834
24/09/28 16:05:55 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:55 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:55 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:55 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:55 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:05:55 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:55 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:55 INFO DataWritingSparkTask: Committed partition 0 (task 13, attempt 0, stage 31.0)
24/09/28 16:05:55 INFO Executor: Finished task 0.0 in stage 31.0 (TID 13). 22621 bytes result sent to driver
24/09/28 16:05:55 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 13) in 957 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:55 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
24/09/28 16:05:55 INFO DAGScheduler: ResultStage 31 (run at AccessController.java:0) finished in 0.979 s
24/09/28 16:05:55 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:05:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
24/09/28 16:05:55 INFO DAGScheduler: Job 13 finished: run at AccessController.java:0, took 0.999002 s
24/09/28 16:05:55 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.location_dim, format=PARQUET) is committing.
24/09/28 16:05:55 INFO SparkWrite: Committing append with 15 new data files to table gold.location_dim
24/09/28 16:05:56 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:56 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/9b051460-9144-41dd-95d7-e9a632149906
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 0fe49ece-0d4e-4a6e-8a70-0d3bcfb10755
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Running query with 0fe49ece-0d4e-4a6e-8a70-0d3bcfb10755
24/09/28 16:05:56 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:05:56 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:56 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:56 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:56 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:56 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:56 INFO DAGScheduler: Asked to cancel job group 0fe49ece-0d4e-4a6e-8a70-0d3bcfb10755
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Close statement with 0fe49ece-0d4e-4a6e-8a70-0d3bcfb10755
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.product_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with c6e9e411-8be7-4aab-a8a0-bfb75cfc98d7
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Running query with c6e9e411-8be7-4aab-a8a0-bfb75cfc98d7
24/09/28 16:05:56 INFO SnapshotProducer: Committed snapshot 7370338723671625081 (MergeAppend)
24/09/28 16:05:56 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.location_dim, snapshotId=7370338723671625081, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.182829931S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=15}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=15}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=19}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=19}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=26621}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=26621}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1727539033730, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/28 16:05:56 INFO SparkWrite: Committed in 187 ms
24/09/28 16:05:56 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.location_dim, format=PARQUET) committed.
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.product_dim"} */

  
    
        create or replace table nessie.gold.product_dim
      
      
    using iceberg
      
      
      
      
      
      

      as
      


WITH src AS (
    SELECT
        src.Category,
        src.size,
        MIN(src.ingestion_date) AS ingestion_date
    FROM 
        nessie.gold.amazon_orders_silver AS src
    
    
    
    
    
    GROUP BY
        src.Category,
        src.size
)

SELECT
    
    
        (ROW_NUMBER() OVER (ORDER BY (SELECT NULL)))
    
 AS id,
    src.*
FROM
    src
  ' with 3832f00c-7a50-46d0-923c-97b21ca60e84
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Running query with 3832f00c-7a50-46d0-923c-97b21ca60e84
24/09/28 16:05:56 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:05:56 INFO NessieIcebergClient: Committed 'gold.location_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=68b57ff3bc29ba272a7232bded2891900ae6f432ca8ee52403a7ea0d717c7df3}', expected commit-id was '15d971ee4a162083674e71a5ed378cfb8f77d8911cb8dfc96f784c6c0e51b335'
24/09/28 16:05:56 INFO BaseMetastoreTableOperations: Successfully committed to table gold.location_dim in 43 ms
24/09/28 16:05:56 INFO DAGScheduler: Asked to cancel job group e721e757-d4db-440c-be1d-2359d1d82f39
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Close statement with e721e757-d4db-440c-be1d-2359d1d82f39
24/09/28 16:05:56 INFO DAGScheduler: Asked to cancel job group f64ded5b-0538-4eee-ad23-ca0f5552fb06
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Close statement with f64ded5b-0538-4eee-ad23-ca0f5552fb06
24/09/28 16:05:56 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:56 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:56 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:56 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:56 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:56 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:56 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:56 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:56 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:56 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:56 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/d6f74694-e02d-4bbf-bdc6-2999be69a349
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with bd2f7301-3ac2-4913-9907-054bf8c0dc92
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Running query with bd2f7301-3ac2-4913-9907-054bf8c0dc92
24/09/28 16:05:56 INFO HiveMetaStore: 4: get_database: default
24/09/28 16:05:56 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:56 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:05:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:56 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:56 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:56 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:56 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:56 INFO DAGScheduler: Asked to cancel job group bd2f7301-3ac2-4913-9907-054bf8c0dc92
24/09/28 16:05:56 INFO SparkExecuteStatementOperation: Close statement with bd2f7301-3ac2-4913-9907-054bf8c0dc92
24/09/28 16:05:56 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:56 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:56 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:56 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/28 16:05:56 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:56 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/28 16:05:56 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:56 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:05:56 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:56 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=68b57ff3bc29ba272a7232bded2891900ae6f432ca8ee52403a7ea0d717c7df3}'
24/09/28 16:05:56 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:05:56 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/28 16:05:56 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/28 16:05:56 INFO SparkScanBuilder: Skipping aggregate pushdown: group by aggregation push down is not supported
24/09/28 16:05:56 INFO V2ScanRelationPushDown: 
Output: Category#1081, Size#1082, Ingestion_Date#1095
         
24/09/28 16:05:56 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 8111912379027843168 created at 2024-09-28T15:59:27.211+00:00 with filter true
24/09/28 16:05:56 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/28 16:05:56 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/28 16:05:56 INFO SparkWrite: Requesting 0 bytes advisory partition size for table gold.product_dim
24/09/28 16:05:56 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table gold.product_dim
24/09/28 16:05:56 INFO SparkWrite: Requesting [] as write ordering for table gold.product_dim
24/09/28 16:05:56 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:05:56 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)
24/09/28 16:05:56 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:56 INFO SparkContext: Created broadcast 26 from broadcast at SparkBatch.java:79
24/09/28 16:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:56 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:05:56 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)
24/09/28 16:05:56 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:56 INFO SparkContext: Created broadcast 27 from broadcast at SparkBatch.java:79
24/09/28 16:05:56 INFO CodeGenerator: Code generated in 48.275158 ms
24/09/28 16:05:56 INFO DAGScheduler: Registering RDD 51 (run at AccessController.java:0) as input to shuffle 10
24/09/28 16:05:56 INFO DAGScheduler: Got map stage job 14 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:56 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (run at AccessController.java:0)
24/09/28 16:05:56 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:05:56 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:56 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[51] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:56 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 41.7 KiB, free 434.2 MiB)
24/09/28 16:05:56 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 434.2 MiB)
24/09/28 16:05:56 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 1809a5016469:34757 (size: 17.8 KiB, free: 434.4 MiB)
24/09/28 16:05:56 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:56 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[51] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:56 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
24/09/28 16:05:56 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 14) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 16223 bytes) 
24/09/28 16:05:56 INFO Executor: Running task 0.0 in stage 32.0 (TID 14)
24/09/28 16:05:56 INFO CodeGenerator: Code generated in 42.681226 ms
24/09/28 16:05:56 INFO CodeGenerator: Code generated in 8.492 ms
24/09/28 16:05:56 INFO CodeGenerator: Code generated in 5.8397 ms
24/09/28 16:05:56 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:05:56 INFO Executor: Finished task 0.0 in stage 32.0 (TID 14). 5443 bytes result sent to driver
24/09/28 16:05:56 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 14) in 176 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:56 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
24/09/28 16:05:56 INFO DAGScheduler: ShuffleMapStage 32 (run at AccessController.java:0) finished in 0.187 s
24/09/28 16:05:56 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:56 INFO DAGScheduler: running: Set()
24/09/28 16:05:56 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:56 INFO DAGScheduler: failed: Set()
24/09/28 16:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:56 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/28 16:05:56 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
24/09/28 16:05:56 INFO CodeGenerator: Code generated in 17.9514 ms
24/09/28 16:05:56 INFO DAGScheduler: Registering RDD 54 (run at AccessController.java:0) as input to shuffle 11
24/09/28 16:05:56 INFO DAGScheduler: Got map stage job 15 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:56 INFO DAGScheduler: Final stage: ShuffleMapStage 34 (run at AccessController.java:0)
24/09/28 16:05:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
24/09/28 16:05:56 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:56 INFO DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[54] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:56 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:57 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 41.1 KiB, free 434.1 MiB)
24/09/28 16:05:57 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 434.1 MiB)
24/09/28 16:05:57 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 1809a5016469:34757 (size: 18.3 KiB, free: 434.3 MiB)
24/09/28 16:05:57 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[54] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:57 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
24/09/28 16:05:57 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 1809a5016469:34757 in memory (size: 5.6 KiB, free: 434.4 MiB)
24/09/28 16:05:57 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 15) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/28 16:05:57 INFO Executor: Running task 0.0 in stage 34.0 (TID 15)
24/09/28 16:05:57 INFO ShuffleBlockFetcherIterator: Getting 1 (882.0 B) non-empty blocks including 1 (882.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/28 16:05:57 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 1809a5016469:34757 in memory (size: 3.2 KiB, free: 434.4 MiB)
24/09/28 16:05:57 INFO CodeGenerator: Code generated in 20.012624 ms
24/09/28 16:05:57 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:05:57 INFO Executor: Finished task 0.0 in stage 34.0 (TID 15). 7937 bytes result sent to driver
24/09/28 16:05:57 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 15) in 47 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:57 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
24/09/28 16:05:57 INFO DAGScheduler: ShuffleMapStage 34 (run at AccessController.java:0) finished in 0.062 s
24/09/28 16:05:57 INFO DAGScheduler: looking for newly runnable stages
24/09/28 16:05:57 INFO DAGScheduler: running: Set()
24/09/28 16:05:57 INFO DAGScheduler: waiting: Set()
24/09/28 16:05:57 INFO DAGScheduler: failed: Set()
24/09/28 16:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/28 16:05:57 INFO CodeGenerator: Code generated in 6.088365 ms
24/09/28 16:05:57 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:05:57 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.2 MiB)
24/09/28 16:05:57 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 1809a5016469:34757 (size: 3.1 KiB, free: 434.4 MiB)
24/09/28 16:05:57 INFO SparkContext: Created broadcast 30 from broadcast at SparkWrite.java:193
24/09/28 16:05:57 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.product_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/28 16:05:57 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/28 16:05:57 INFO DAGScheduler: Got job 16 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:05:57 INFO DAGScheduler: Final stage: ResultStage 37 (run at AccessController.java:0)
24/09/28 16:05:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)
24/09/28 16:05:57 INFO DAGScheduler: Missing parents: List()
24/09/28 16:05:57 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[58] at run at AccessController.java:0), which has no missing parents
24/09/28 16:05:57 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 48.7 KiB, free 434.1 MiB)
24/09/28 16:05:57 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 22.0 KiB, free 434.1 MiB)
24/09/28 16:05:57 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 1809a5016469:34757 (size: 22.0 KiB, free: 434.3 MiB)
24/09/28 16:05:57 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585
24/09/28 16:05:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[58] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:05:57 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
24/09/28 16:05:57 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 16) (1809a5016469, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/28 16:05:57 INFO Executor: Running task 0.0 in stage 37.0 (TID 16)
24/09/28 16:05:57 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/28 16:05:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/28 16:05:57 INFO CodeGenerator: Code generated in 4.230446 ms
24/09/28 16:05:57 INFO CodeGenerator: Code generated in 3.963478 ms
24/09/28 16:05:57 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:05:57 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/28 16:05:57 INFO DataWritingSparkTask: Committed partition 0 (task 16, attempt 0, stage 37.0)
24/09/28 16:05:57 INFO Executor: Finished task 0.0 in stage 37.0 (TID 16). 11926 bytes result sent to driver
24/09/28 16:05:57 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 16) in 57 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:05:57 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
24/09/28 16:05:57 INFO DAGScheduler: ResultStage 37 (run at AccessController.java:0) finished in 0.063 s
24/09/28 16:05:57 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:05:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
24/09/28 16:05:57 INFO DAGScheduler: Job 16 finished: run at AccessController.java:0, took 0.068362 s
24/09/28 16:05:57 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.product_dim, format=PARQUET) is committing.
24/09/28 16:05:57 INFO SparkWrite: Committing append with 1 new data files to table gold.product_dim
24/09/28 16:05:57 INFO SnapshotProducer: Committed snapshot 4461980343317836230 (MergeAppend)
24/09/28 16:05:57 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.product_dim, snapshotId=4461980343317836230, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.062934699S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=10}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=10}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1388}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1388}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1727539033730, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/28 16:05:57 INFO SparkWrite: Committed in 64 ms
24/09/28 16:05:57 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.product_dim, format=PARQUET) committed.
24/09/28 16:05:57 INFO NessieIcebergClient: Committed 'gold.product_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=80e619ea0759566dfabfad6c79edda8b277944d87cb0301a3c2e4d66d0c7d680}', expected commit-id was '68b57ff3bc29ba272a7232bded2891900ae6f432ca8ee52403a7ea0d717c7df3'
24/09/28 16:05:57 INFO BaseMetastoreTableOperations: Successfully committed to table gold.product_dim in 38 ms
24/09/28 16:05:57 INFO DAGScheduler: Asked to cancel job group 3832f00c-7a50-46d0-923c-97b21ca60e84
24/09/28 16:05:57 INFO SparkExecuteStatementOperation: Close statement with 3832f00c-7a50-46d0-923c-97b21ca60e84
24/09/28 16:05:57 INFO DAGScheduler: Asked to cancel job group c6e9e411-8be7-4aab-a8a0-bfb75cfc98d7
24/09/28 16:05:57 INFO SparkExecuteStatementOperation: Close statement with c6e9e411-8be7-4aab-a8a0-bfb75cfc98d7
24/09/28 16:05:57 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:57 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:57 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:57 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:05:57 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:57 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:05:57 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:57 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:05:57 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:05:57 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/d064c37b-bd2d-4494-8ab4-f604d245680d
24/09/28 16:05:57 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 4f0bdbb8-5d5a-49d8-8f55-ba50ddfa6474
24/09/28 16:05:57 INFO SparkExecuteStatementOperation: Running query with 4f0bdbb8-5d5a-49d8-8f55-ba50ddfa6474
24/09/28 16:05:57 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:05:57 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:05:57 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:05:57 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:05:57 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:05:57 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:57 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:57 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:57 INFO DAGScheduler: Asked to cancel job group 4f0bdbb8-5d5a-49d8-8f55-ba50ddfa6474
24/09/28 16:05:57 INFO SparkExecuteStatementOperation: Close statement with 4f0bdbb8-5d5a-49d8-8f55-ba50ddfa6474
24/09/28 16:05:57 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:05:57 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:05:57 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:05:57 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:05:57 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:05:57 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:05:57 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:05:57 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:05 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:06:05 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/6271cdce-0c1f-476a-bc6a-78cc555b9916
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 51344475-ce6f-479c-a241-b68f174e58fb
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Running query with 51344475-ce6f-479c-a241-b68f174e58fb
24/09/28 16:06:05 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:06:05 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:06:05 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:05 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:05 INFO DAGScheduler: Asked to cancel job group 51344475-ce6f-479c-a241-b68f174e58fb
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Close statement with 51344475-ce6f-479c-a241-b68f174e58fb
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 18da065f-da85-4688-a769-09efef903a0c
24/09/28 16:06:05 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 1809a5016469:34757 in memory (size: 17.8 KiB, free: 434.4 MiB)
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Running query with 18da065f-da85-4688-a769-09efef903a0c
24/09/28 16:06:05 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 1809a5016469:34757 in memory (size: 22.0 KiB, free: 434.4 MiB)
24/09/28 16:06:05 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 1809a5016469:34757 in memory (size: 3.1 KiB, free: 434.4 MiB)
24/09/28 16:06:05 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 1809a5016469:34757 in memory (size: 18.3 KiB, free: 434.4 MiB)
24/09/28 16:06:05 INFO HiveMetaStore: 17: get_databases: *
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:06:05 INFO HiveMetaStore: 17: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:06:05 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:05 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/28 16:06:05 INFO DAGScheduler: Asked to cancel job group 18da065f-da85-4688-a769-09efef903a0c
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Close statement with 18da065f-da85-4688-a769-09efef903a0c
24/09/28 16:06:05 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:05 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:05 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:06:05 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:06:05 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:05 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:06:05 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/ade1a93c-7bed-4ab2-9c38-8a8b0d2ead94
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 4da9a3c2-61f1-4e96-8cab-7cb2d954cd8b
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Running query with 4da9a3c2-61f1-4e96-8cab-7cb2d954cd8b
24/09/28 16:06:05 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:06:05 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:06:05 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:05 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:05 INFO DAGScheduler: Asked to cancel job group 4da9a3c2-61f1-4e96-8cab-7cb2d954cd8b
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Close statement with 4da9a3c2-61f1-4e96-8cab-7cb2d954cd8b
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 2e824764-d575-470b-890e-847a2fec06d0
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Running query with 2e824764-d575-470b-890e-847a2fec06d0
24/09/28 16:06:05 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:06:05 INFO DAGScheduler: Asked to cancel job group 2e824764-d575-470b-890e-847a2fec06d0
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Close statement with 2e824764-d575-470b-890e-847a2fec06d0
24/09/28 16:06:05 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:05 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:05 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:06:05 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:06:05 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:05 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:06:05 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/85efa633-17c0-4753-b074-6f379b56f3a3
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with d494a028-06b4-41cf-9510-a570ce756e9b
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Running query with d494a028-06b4-41cf-9510-a570ce756e9b
24/09/28 16:06:05 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:06:05 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:06:05 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:05 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:05 INFO DAGScheduler: Asked to cancel job group d494a028-06b4-41cf-9510-a570ce756e9b
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Close statement with d494a028-06b4-41cf-9510-a570ce756e9b
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 3748c474-3070-40c6-aac1-fb3b7d112ff3
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Running query with 3748c474-3070-40c6-aac1-fb3b7d112ff3
24/09/28 16:06:05 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:06:05 INFO DAGScheduler: Asked to cancel job group 3748c474-3070-40c6-aac1-fb3b7d112ff3
24/09/28 16:06:05 ERROR SparkExecuteStatementOperation: Error executing query with 3748c474-3070-40c6-aac1-fb3b7d112ff3, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#1131, tableName#1132, isTemporary#1133, information#1134]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@429af061, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 37d127e9-df1a-4d15-9599-54eadc575f24
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Running query with 37d127e9-df1a-4d15-9599-54eadc575f24
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/28 16:06:05 INFO DAGScheduler: Asked to cancel job group 37d127e9-df1a-4d15-9599-54eadc575f24
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Close statement with 37d127e9-df1a-4d15-9599-54eadc575f24
24/09/28 16:06:05 INFO DAGScheduler: Asked to cancel job group 3748c474-3070-40c6-aac1-fb3b7d112ff3
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Close statement with 3748c474-3070-40c6-aac1-fb3b7d112ff3
24/09/28 16:06:05 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:05 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:05 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:06:05 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:06:05 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:05 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:06:05 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/83d5519d-68a3-4849-872f-11a77165eb2a
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 4145bf36-3187-4942-81ed-2c70d00c56cd
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Running query with 4145bf36-3187-4942-81ed-2c70d00c56cd
24/09/28 16:06:05 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:06:05 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:06:05 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:06:05 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:06:05 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:05 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:05 INFO DAGScheduler: Asked to cancel job group 4145bf36-3187-4942-81ed-2c70d00c56cd
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Close statement with 4145bf36-3187-4942-81ed-2c70d00c56cd
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.fact_amazon_orders"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240928155821
      ' with 70021aff-9103-439b-b26f-2ece21d327a8
24/09/28 16:06:05 INFO SparkExecuteStatementOperation: Running query with 70021aff-9103-439b-b26f-2ece21d327a8
24/09/28 16:06:06 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.fact_amazon_orders"} */

  
    
        create or replace table nessie.gold.fact_amazon_orders
      
      
    using iceberg
      
      
      
      
      
      

      as
      

SELECT
    src.order_id          AS id,
    date_dim.id           AS date_id,
    curr_dim.id           AS currency_id,
    loc_dim.id            AS location_id,
    prod_dim.id           AS product_id,
    ship_dim.id           AS shipping_id,
    src.qty               AS quantity,
    ROUND(src.amount ,2)  AS amount,
    src.ingestion_date       AS ingestion_date

FROM
    nessie.gold.amazon_orders_silver as src
LEFT JOIN
    nessie.gold.date_dim as date_dim
ON
    src.order_date = date_dim.full_date
LEFT JOIN
    nessie.gold.currency_dim as curr_dim
ON
    src.currency = curr_dim.currency
LEFT JOIN
    nessie.gold.location_dim as loc_dim
ON
    src.ship_country = loc_dim.ship_country AND
    src.ship_state = loc_dim.ship_state AND
    src.ship_city = loc_dim.ship_city   AND
    src.Ship_Postal_Code = loc_dim.Ship_Postal_Code
LEFT JOIN
    nessie.gold.product_dim as prod_dim
ON
    src.category = prod_dim.category AND
    src.size = prod_dim.size
LEFT JOIN
    nessie.gold.shipping_dim as ship_dim
ON
    src.Order_Status        = ship_dim.shipping_status AND
    src.Fulfilment          = ship_dim.Fulfilment AND
    src.ship_service_level  = ship_dim.ship_service_level AND
    src.fulfilled_by        = ship_dim.fulfilled_by 

  ' with 4ad57188-fd7c-44fd-b3aa-9cee988e7680
24/09/28 16:06:06 INFO SparkExecuteStatementOperation: Running query with 4ad57188-fd7c-44fd-b3aa-9cee988e7680
24/09/28 16:06:06 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/28 16:06:06 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/date_dim_45b0aaa6-f7dd-4d57-ae21-a18f02b861e9/metadata/00000-58a69e03-51fb-4c82-b572-bf4b47cbc269.metadata.json
24/09/28 16:06:06 INFO NessieUtil: loadTableMetadata for 'gold.date_dim' from location 's3://warehouse/gold/date_dim_45b0aaa6-f7dd-4d57-ae21-a18f02b861e9/metadata/00000-58a69e03-51fb-4c82-b572-bf4b47cbc269.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=80e619ea0759566dfabfad6c79edda8b277944d87cb0301a3c2e4d66d0c7d680}'
24/09/28 16:06:06 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.date_dim
24/09/28 16:06:06 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/currency_dim_1afcd8b8-1035-475b-8e3e-476cce07c167/metadata/00000-40b37428-1fa8-406f-949b-030acea719cd.metadata.json
24/09/28 16:06:06 INFO NessieUtil: loadTableMetadata for 'gold.currency_dim' from location 's3://warehouse/gold/currency_dim_1afcd8b8-1035-475b-8e3e-476cce07c167/metadata/00000-40b37428-1fa8-406f-949b-030acea719cd.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=80e619ea0759566dfabfad6c79edda8b277944d87cb0301a3c2e4d66d0c7d680}'
24/09/28 16:06:06 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.currency_dim
24/09/28 16:06:06 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/location_dim_074a55db-c850-4969-8d00-e8127d5ecd68/metadata/00000-08726082-6340-4e92-b529-214cadf7d094.metadata.json
24/09/28 16:06:06 INFO NessieUtil: loadTableMetadata for 'gold.location_dim' from location 's3://warehouse/gold/location_dim_074a55db-c850-4969-8d00-e8127d5ecd68/metadata/00000-08726082-6340-4e92-b529-214cadf7d094.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=80e619ea0759566dfabfad6c79edda8b277944d87cb0301a3c2e4d66d0c7d680}'
24/09/28 16:06:06 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.location_dim
24/09/28 16:06:06 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/product_dim_7152f98f-54c0-46f6-a8fa-4b2609ae004c/metadata/00000-685eb525-f62b-4246-be4e-07c581ffa795.metadata.json
24/09/28 16:06:06 INFO NessieUtil: loadTableMetadata for 'gold.product_dim' from location 's3://warehouse/gold/product_dim_7152f98f-54c0-46f6-a8fa-4b2609ae004c/metadata/00000-685eb525-f62b-4246-be4e-07c581ffa795.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=80e619ea0759566dfabfad6c79edda8b277944d87cb0301a3c2e4d66d0c7d680}'
24/09/28 16:06:06 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.product_dim
24/09/28 16:06:06 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/shipping_dim_707e7fa8-b620-434a-a3a4-7208e897a976/metadata/00000-87b1c873-3f68-4093-a786-75677004c61f.metadata.json
24/09/28 16:06:06 INFO NessieUtil: loadTableMetadata for 'gold.shipping_dim' from location 's3://warehouse/gold/shipping_dim_707e7fa8-b620-434a-a3a4-7208e897a976/metadata/00000-87b1c873-3f68-4093-a786-75677004c61f.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=80e619ea0759566dfabfad6c79edda8b277944d87cb0301a3c2e4d66d0c7d680}'
24/09/28 16:06:06 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.shipping_dim
24/09/28 16:06:06 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:06:06 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_1f548c93-d0e0-4be2-bd31-9406d4e30fd4/metadata/00000-62554e23-ace1-40fc-ab73-53192571bd27.gz.metadata.json
24/09/28 16:06:06 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json
24/09/28 16:06:06 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_091d89a0-c6f4-4867-b479-53be7115834a/metadata/00001-1f5d404a-0789-4cdf-8e76-d50f8e5da957.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=80e619ea0759566dfabfad6c79edda8b277944d87cb0301a3c2e4d66d0c7d680}'
24/09/28 16:06:06 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/28 16:06:06 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/28 16:06:06 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/28 16:06:06 INFO SparkScanBuilder: Evaluating completely on Iceberg side: full_date IS NOT NULL
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.date_dim
Pushed Filters: full_date IS NOT NULL
Post-Scan Filters: 
         
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.currency_dim
Pushed Filters: currency IS NOT NULL
Post-Scan Filters: isnotnull(currency#1177)
         
24/09/28 16:06:06 INFO SparkScanBuilder: Evaluating completely on Iceberg side: ship_country IS NOT NULL
24/09/28 16:06:06 INFO SparkScanBuilder: Evaluating completely on Iceberg side: ship_state IS NOT NULL
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.location_dim
Pushed Filters: ship_country IS NOT NULL, ship_state IS NOT NULL, ship_city IS NOT NULL, ship_postal_code IS NOT NULL
Post-Scan Filters: isnotnull(ship_city#1182),isnotnull(ship_postal_code#1183)
         
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.product_dim
Pushed Filters: Category IS NOT NULL, size IS NOT NULL
Post-Scan Filters: isnotnull(Category#1186),isnotnull(size#1187)
         
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.shipping_dim
Pushed Filters: shipping_status IS NOT NULL, Fulfilment IS NOT NULL, ship_service_level IS NOT NULL, fulfilled_by IS NOT NULL
Post-Scan Filters: isnotnull(shipping_status#1190),isnotnull(Fulfilment#1191),isnotnull(ship_service_level#1192),isnotnull(fulfilled_by#1193)
         
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Output: Order_ID#1216, Order_Date#1217, Order_Status#1218, Fulfilment#1219, ship_service_level#1221, Category#1222, Size#1223, Qty#1225, Currency#1226, Amount#1227, Ship_City#1228, Ship_State#1229, Ship_Postal_Code#1230, Ship_Country#1231, Fulfilled_By#1233, Ingestion_Date#1236
         
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Output: id#1168, full_date#1169
         
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Output: id#1176, currency#1177
         
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Output: id#1179, ship_country#1180, ship_state#1181, ship_city#1182, ship_postal_code#1183
         
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Output: id#1185, Category#1186, size#1187
         
24/09/28 16:06:06 INFO V2ScanRelationPushDown: 
Output: id#1189, shipping_status#1190, Fulfilment#1191, ship_service_level#1192, fulfilled_by#1193
         
24/09/28 16:06:06 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 8111912379027843168 created at 2024-09-28T15:59:27.211+00:00 with filter true
24/09/28 16:06:06 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/28 16:06:06 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/28 16:06:06 INFO SnapshotScan: Scanning table nessie.gold.date_dim snapshot 8761048072987420580 created at 2024-09-28T16:05:50.021+00:00 with filter full_date IS NOT NULL
24/09/28 16:06:06 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.date_dim
24/09/28 16:06:06 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.gold.date_dim
24/09/28 16:06:06 INFO SnapshotScan: Scanning table nessie.gold.currency_dim snapshot 5660865275238950643 created at 2024-09-28T16:05:18.460+00:00 with filter currency IS NOT NULL
24/09/28 16:06:06 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.currency_dim
24/09/28 16:06:06 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.gold.currency_dim
24/09/28 16:06:06 INFO SnapshotScan: Scanning table nessie.gold.location_dim snapshot 7370338723671625081 created at 2024-09-28T16:05:56.086+00:00 with filter (((ship_country IS NOT NULL AND ship_state IS NOT NULL) AND ship_city IS NOT NULL) AND ship_postal_code IS NOT NULL)
24/09/28 16:06:06 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.location_dim
24/09/28 16:06:06 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 4 partition(s) for table nessie.gold.location_dim
24/09/28 16:06:06 INFO SnapshotScan: Scanning table nessie.gold.product_dim snapshot 4461980343317836230 created at 2024-09-28T16:05:57.220+00:00 with filter (Category IS NOT NULL AND size IS NOT NULL)
24/09/28 16:06:06 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.product_dim
24/09/28 16:06:06 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.gold.product_dim
24/09/28 16:06:06 INFO SnapshotScan: Scanning table nessie.gold.shipping_dim snapshot 238474889566213799 created at 2024-09-28T16:05:42.352+00:00 with filter (((shipping_status IS NOT NULL AND Fulfilment IS NOT NULL) AND ship_service_level IS NOT NULL) AND fulfilled_by IS NOT NULL)
24/09/28 16:06:06 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.shipping_dim
24/09/28 16:06:06 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.gold.shipping_dim
24/09/28 16:06:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table gold.fact_amazon_orders
24/09/28 16:06:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table gold.fact_amazon_orders
24/09/28 16:06:06 INFO SparkWrite: Requesting [] as write ordering for table gold.fact_amazon_orders
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 32 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.3 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 1809a5016469:34757 (size: 3.6 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 33 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 434.2 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 1809a5016469:34757 (size: 3.4 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 34 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.2 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 1809a5016469:34757 (size: 3.6 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 35 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 434.2 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 1809a5016469:34757 (size: 3.5 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 36 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 434.1 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 1809a5016469:34757 (size: 3.5 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 37 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 434.1 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 1809a5016469:34757 (size: 3.4 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 38 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:06:06 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	scala.collection.immutable.List.map(List.scala:293)
	org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:06 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:06 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:06 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:06 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:06 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/28 16:06:06 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	scala.collection.immutable.List.map(List.scala:293)
	org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.1 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 1809a5016469:34757 (size: 3.6 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 39 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 434.1 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 1809a5016469:34757 (size: 3.5 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 40 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 434.1 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 1809a5016469:34757 (size: 3.5 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 41 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 3.6 KiB, free 434.0 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 1809a5016469:34757 (size: 3.6 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 42 from broadcast at SparkBatch.java:79
24/09/28 16:06:06 INFO CodeGenerator: Code generated in 15.559262 ms
24/09/28 16:06:06 INFO CodeGenerator: Code generated in 9.34208 ms
24/09/28 16:06:06 INFO CodeGenerator: Code generated in 7.303403 ms
24/09/28 16:06:06 INFO CodeGenerator: Code generated in 7.932366 ms
24/09/28 16:06:06 INFO CodeGenerator: Code generated in 13.72049 ms
24/09/28 16:06:06 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:06 INFO DAGScheduler: Got job 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/28 16:06:06 INFO DAGScheduler: Final stage: ResultStage 38 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/28 16:06:06 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:06:06 INFO DAGScheduler: Missing parents: List()
24/09/28 16:06:06 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 14.3 KiB, free 434.0 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 434.0 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 1809a5016469:34757 (size: 5.7 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1585
24/09/28 16:06:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/28 16:06:06 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
24/09/28 16:06:06 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:06 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:06 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 17) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 14075 bytes) 
24/09/28 16:06:06 INFO DAGScheduler: Got job 18 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/28 16:06:06 INFO DAGScheduler: Final stage: ResultStage 39 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/28 16:06:06 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:06:06 INFO DAGScheduler: Missing parents: List()
24/09/28 16:06:06 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[76] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/28 16:06:06 INFO Executor: Running task 0.0 in stage 38.0 (TID 17)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 15.9 KiB, free 434.0 MiB)
24/09/28 16:06:06 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.0 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 1809a5016469:34757 (size: 5.9 KiB, free: 434.4 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1585
24/09/28 16:06:06 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[76] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/28 16:06:06 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
24/09/28 16:06:06 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/28 16:06:06 INFO DAGScheduler: Final stage: ResultStage 40 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/28 16:06:06 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:06:06 INFO DAGScheduler: Missing parents: List()
24/09/28 16:06:06 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[69] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 14.0 KiB, free 434.0 MiB)
24/09/28 16:06:06 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 18) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 14688 bytes) 
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.0 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 1809a5016469:34757 (size: 5.6 KiB, free: 434.3 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585
24/09/28 16:06:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[69] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/28 16:06:06 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
24/09/28 16:06:06 INFO DAGScheduler: Got job 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 4 output partitions
24/09/28 16:06:06 INFO DAGScheduler: Final stage: ResultStage 41 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/28 16:06:06 INFO Executor: Running task 0.0 in stage 39.0 (TID 18)
24/09/28 16:06:06 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:06:06 INFO DAGScheduler: Missing parents: List()
24/09/28 16:06:06 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[70] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/28 16:06:06 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 19) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 15138 bytes) 
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 15.9 KiB, free 433.9 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.9 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 1809a5016469:34757 (size: 5.9 KiB, free: 434.3 MiB)
24/09/28 16:06:06 INFO Executor: Running task 0.0 in stage 40.0 (TID 19)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1585
24/09/28 16:06:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 41 (MapPartitionsRDD[70] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
24/09/28 16:06:06 INFO TaskSchedulerImpl: Adding task set 41.0 with 4 tasks resource profile 0
24/09/28 16:06:06 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/28 16:06:06 INFO DAGScheduler: Final stage: ResultStage 42 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/28 16:06:06 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:06:06 INFO DAGScheduler: Missing parents: List()
24/09/28 16:06:06 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[78] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 14.8 KiB, free 433.9 MiB)
24/09/28 16:06:06 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.9 MiB)
24/09/28 16:06:06 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 1809a5016469:34757 (size: 5.7 KiB, free: 434.3 MiB)
24/09/28 16:06:06 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1585
24/09/28 16:06:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[78] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/28 16:06:06 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
24/09/28 16:06:06 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 20) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 16738 bytes) 
24/09/28 16:06:06 INFO Executor: Running task 0.0 in stage 41.0 (TID 20)
24/09/28 16:06:06 INFO CodeGenerator: Code generated in 25.692647 ms
24/09/28 16:06:06 INFO CodeGenerator: Code generated in 24.787464 ms
24/09/28 16:06:06 INFO CodeGenerator: Code generated in 34.623226 ms
24/09/28 16:06:06 INFO CodeGenerator: Code generated in 15.594951 ms
24/09/28 16:06:06 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:06 INFO Executor: Finished task 0.0 in stage 40.0 (TID 19). 4547 bytes result sent to driver
24/09/28 16:06:06 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 21) (1809a5016469, executor driver, partition 1, PROCESS_LOCAL, 16757 bytes) 
24/09/28 16:06:06 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 19) in 127 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:06:06 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
24/09/28 16:06:06 INFO DAGScheduler: ResultStage 40 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.141 s
24/09/28 16:06:06 INFO Executor: Running task 1.0 in stage 41.0 (TID 21)
24/09/28 16:06:06 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:06 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:06 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:06:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
24/09/28 16:06:07 INFO Executor: Finished task 0.0 in stage 39.0 (TID 18). 4775 bytes result sent to driver
24/09/28 16:06:07 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.168251 s
24/09/28 16:06:07 INFO Executor: Finished task 0.0 in stage 38.0 (TID 17). 4569 bytes result sent to driver
24/09/28 16:06:07 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 22) (1809a5016469, executor driver, partition 2, PROCESS_LOCAL, 16775 bytes) 
24/09/28 16:06:07 INFO Executor: Running task 2.0 in stage 41.0 (TID 22)
24/09/28 16:06:07 INFO TaskSetManager: Starting task 3.0 in stage 41.0 (TID 23) (1809a5016469, executor driver, partition 3, PROCESS_LOCAL, 16404 bytes) 
24/09/28 16:06:07 INFO CodeGenerator: Code generated in 6.884039 ms
24/09/28 16:06:07 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 18) in 187 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:06:07 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
24/09/28 16:06:07 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 17) in 222 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:06:07 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
24/09/28 16:06:07 INFO Executor: Running task 3.0 in stage 41.0 (TID 23)
24/09/28 16:06:07 INFO DAGScheduler: ResultStage 39 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.205 s
24/09/28 16:06:07 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:06:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
24/09/28 16:06:07 INFO DAGScheduler: Job 18 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.209133 s
24/09/28 16:06:07 INFO DAGScheduler: ResultStage 38 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.233 s
24/09/28 16:06:07 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:06:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
24/09/28 16:06:07 INFO DAGScheduler: Job 17 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.239631 s
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 4.0 MiB, free 429.9 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 179.0 B, free 429.9 MiB)
24/09/28 16:06:07 INFO CodeGenerator: Code generated in 14.134283 ms
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 1809a5016469:34757 (size: 179.0 B, free: 434.3 MiB)
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO SparkContext: Created broadcast 48 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO CodeGenerator: Code generated in 10.606055 ms
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 4.0 MiB, free 425.9 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 192.0 B, free 425.9 MiB)
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 4.0 MiB, free 421.9 MiB)
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 453.0 B, free 421.9 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 1809a5016469:34757 (size: 192.0 B, free: 434.3 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 1809a5016469:34757 (size: 453.0 B, free: 434.3 MiB)
24/09/28 16:06:07 INFO SparkContext: Created broadcast 49 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:07 INFO SparkContext: Created broadcast 50 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 32.0 KiB, free 421.9 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 421.9 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO SparkContext: Created broadcast 51 from broadcast at SparkBatch.java:79
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO Executor: Finished task 0.0 in stage 41.0 (TID 20). 4759 bytes result sent to driver
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 32.0 KiB, free 421.8 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 421.8 MiB)
24/09/28 16:06:07 INFO Executor: Finished task 2.0 in stage 41.0 (TID 22). 4861 bytes result sent to driver
24/09/28 16:06:07 INFO Executor: Finished task 3.0 in stage 41.0 (TID 23). 4663 bytes result sent to driver
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO SparkContext: Created broadcast 52 from broadcast at SparkBatch.java:79
24/09/28 16:06:07 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 24) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 14297 bytes) 
24/09/28 16:06:07 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 20) in 312 ms on 1809a5016469 (executor driver) (1/4)
24/09/28 16:06:07 INFO Executor: Running task 0.0 in stage 42.0 (TID 24)
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO TaskSetManager: Finished task 3.0 in stage 41.0 (TID 23) in 163 ms on 1809a5016469 (executor driver) (2/4)
24/09/28 16:06:07 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 22) in 186 ms on 1809a5016469 (executor driver) (3/4)
24/09/28 16:06:07 INFO Executor: Finished task 1.0 in stage 41.0 (TID 21). 4738 bytes result sent to driver
24/09/28 16:06:07 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 21) in 209 ms on 1809a5016469 (executor driver) (4/4)
24/09/28 16:06:07 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
24/09/28 16:06:07 INFO DAGScheduler: ResultStage 41 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.331 s
24/09/28 16:06:07 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:06:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
24/09/28 16:06:07 INFO DAGScheduler: Job 20 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.355006 s
24/09/28 16:06:07 INFO CodeGenerator: Code generated in 8.094559 ms
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 4.0 MiB, free 417.8 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 1460.0 B, free 417.8 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 1809a5016469:34757 (size: 1460.0 B, free: 434.3 MiB)
24/09/28 16:06:07 INFO CodeGenerator: Code generated in 20.185664 ms
24/09/28 16:06:07 INFO SparkContext: Created broadcast 53 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 32.0 KiB, free 417.8 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 417.8 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO SparkContext: Created broadcast 54 from broadcast at SparkBatch.java:79
24/09/28 16:06:07 INFO Executor: Finished task 0.0 in stage 42.0 (TID 24). 4722 bytes result sent to driver
24/09/28 16:06:07 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 24) in 65 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:06:07 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
24/09/28 16:06:07 INFO DAGScheduler: ResultStage 42 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.364 s
24/09/28 16:06:07 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:06:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
24/09/28 16:06:07 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.397558 s
24/09/28 16:06:07 INFO CodeGenerator: Code generated in 9.894268 ms
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 4.0 MiB, free 413.8 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 540.0 B, free 413.8 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 1809a5016469:34757 (size: 540.0 B, free: 434.3 MiB)
24/09/28 16:06:07 INFO SparkContext: Created broadcast 55 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 32.0 KiB, free 413.8 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 413.8 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO SparkContext: Created broadcast 56 from broadcast at SparkBatch.java:79
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 32.0 KiB, free 413.7 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 413.7 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 1809a5016469:34757 (size: 3.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO SparkContext: Created broadcast 57 from broadcast at SparkBatch.java:79
24/09/28 16:06:07 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 1809a5016469:34757 in memory (size: 5.6 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 1809a5016469:34757 in memory (size: 5.7 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 1809a5016469:34757 in memory (size: 5.9 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 1809a5016469:34757 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 1809a5016469:34757 in memory (size: 5.9 KiB, free: 434.4 MiB)
24/09/28 16:06:07 INFO CodeGenerator: Code generated in 48.271689 ms
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 32.0 KiB, free 413.9 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 413.9 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 1809a5016469:34757 (size: 3.2 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO SparkContext: Created broadcast 58 from broadcast at SparkWrite.java:193
24/09/28 16:06:07 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.fact_amazon_orders, format=PARQUET). The input RDD has 1 partitions.
24/09/28 16:06:07 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/28 16:06:07 INFO DAGScheduler: Got job 22 (run at AccessController.java:0) with 1 output partitions
24/09/28 16:06:07 INFO DAGScheduler: Final stage: ResultStage 43 (run at AccessController.java:0)
24/09/28 16:06:07 INFO DAGScheduler: Parents of final stage: List()
24/09/28 16:06:07 INFO DAGScheduler: Missing parents: List()
24/09/28 16:06:07 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[81] at run at AccessController.java:0), which has no missing parents
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 34.4 KiB, free 413.9 MiB)
24/09/28 16:06:07 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 413.9 MiB)
24/09/28 16:06:07 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 1809a5016469:34757 (size: 10.8 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1585
24/09/28 16:06:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[81] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/28 16:06:07 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
24/09/28 16:06:07 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 25) (1809a5016469, executor driver, partition 0, PROCESS_LOCAL, 17044 bytes) 
24/09/28 16:06:07 INFO Executor: Running task 0.0 in stage 43.0 (TID 25)
24/09/28 16:06:07 INFO CodeGenerator: Code generated in 22.038212 ms
24/09/28 16:06:07 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/28 16:06:07 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/28 16:06:07 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/28 16:06:07 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	scala.collection.immutable.List.map(List.scala:293)
	org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:07 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 1809a5016469:34757 in memory (size: 5.7 KiB, free: 434.3 MiB)
24/09/28 16:06:07 INFO DataWritingSparkTask: Committed partition 0 (task 25, attempt 0, stage 43.0)
24/09/28 16:06:07 INFO Executor: Finished task 0.0 in stage 43.0 (TID 25). 7761 bytes result sent to driver
24/09/28 16:06:07 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 25) in 193 ms on 1809a5016469 (executor driver) (1/1)
24/09/28 16:06:07 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
24/09/28 16:06:07 INFO DAGScheduler: ResultStage 43 (run at AccessController.java:0) finished in 0.200 s
24/09/28 16:06:07 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/28 16:06:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
24/09/28 16:06:07 INFO DAGScheduler: Job 22 finished: run at AccessController.java:0, took 0.202868 s
24/09/28 16:06:07 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.fact_amazon_orders, format=PARQUET) is committing.
24/09/28 16:06:07 INFO SparkWrite: Committing append with 1 new data files to table gold.fact_amazon_orders
24/09/28 16:06:07 INFO SnapshotProducer: Committed snapshot 9022233253604967779 (MergeAppend)
24/09/28 16:06:07 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.fact_amazon_orders, snapshotId=9022233253604967779, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.089996621S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=19}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=19}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3126}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=3126}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1727539033730, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/28 16:06:07 INFO SparkWrite: Committed in 90 ms
24/09/28 16:06:07 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.fact_amazon_orders, format=PARQUET) committed.
24/09/28 16:06:07 INFO NessieIcebergClient: Committed 'gold.fact_amazon_orders' against 'Branch{name=amazon_pipeline_sampled_data_1_20240928155821, metadata=null, hash=bb4e38767cd07b95a2b4c1ec02f8414b644fb1061b003100902811241874f6f9}', expected commit-id was '80e619ea0759566dfabfad6c79edda8b277944d87cb0301a3c2e4d66d0c7d680'
24/09/28 16:06:07 INFO BaseMetastoreTableOperations: Successfully committed to table gold.fact_amazon_orders in 57 ms
24/09/28 16:06:07 INFO DAGScheduler: Asked to cancel job group 4ad57188-fd7c-44fd-b3aa-9cee988e7680
24/09/28 16:06:07 INFO SparkExecuteStatementOperation: Close statement with 4ad57188-fd7c-44fd-b3aa-9cee988e7680
24/09/28 16:06:07 INFO DAGScheduler: Asked to cancel job group 70021aff-9103-439b-b26f-2ece21d327a8
24/09/28 16:06:07 INFO SparkExecuteStatementOperation: Close statement with 70021aff-9103-439b-b26f-2ece21d327a8
24/09/28 16:06:07 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:07 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:07 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:07 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:06:07 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:06:07 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:06:07 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:06:07 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/28 16:06:07 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/28 16:06:07 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/bc9f1353-4989-4ca7-9d8c-a8099750c645
24/09/28 16:06:07 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with f22d343e-d600-4c30-8b0c-389910f2d784
24/09/28 16:06:07 INFO SparkExecuteStatementOperation: Running query with f22d343e-d600-4c30-8b0c-389910f2d784
24/09/28 16:06:07 INFO HiveMetaStore: 6: get_database: default
24/09/28 16:06:07 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=get_database: default	
24/09/28 16:06:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/28 16:06:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/28 16:06:07 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/28 16:06:07 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:07 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:07 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:07 INFO DAGScheduler: Asked to cancel job group f22d343e-d600-4c30-8b0c-389910f2d784
24/09/28 16:06:07 INFO SparkExecuteStatementOperation: Close statement with f22d343e-d600-4c30-8b0c-389910f2d784
24/09/28 16:06:07 INFO ObjectStore: ObjectStore, initialize called
24/09/28 16:06:07 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/28 16:06:07 INFO ObjectStore: Initialized ObjectStore
24/09/28 16:06:07 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/28 16:06:07 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/28 16:06:07 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/28 16:06:07 INFO audit: ugi=airflow	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/28 16:06:07 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
