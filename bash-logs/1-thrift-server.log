Exporting variables in /config/dremio.env
Exporting variables in /config/dwh.env
Exporting variables in /config/minio.env
Exporting variables in /config/nessie.env
Exporting variables in /config/paths.env
NESSIE_URI: http://nessie:19120/api/v1
BRANCH_MAIN: main
MINIO_ACCESS_KEY: admin
MINIO_SECRET_KEY: password
MINIO_ENDPOINT: http://172.18.0.3:9000
MINIO_ICEBERG_S3_BUCKET: s3://warehouse
SPARK_HOME: /opt/spark
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/me/.ivy2/cache
The jars for the packages stored in: /home/me/.ivy2/jars
org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency
org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-4bf5cfd3-a1e9-4db2-9337-57de39017b1c;1.0
	confs: [default]
	found org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central
	found org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.92.1 in central
:: resolution report :: resolve 188ms :: artifacts dl 6ms
	:: modules in use:
	org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]
	org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.92.1 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-4bf5cfd3-a1e9-4db2-9337-57de39017b1c
	confs: [default]
	0 artifacts copied, 2 already retrieved (0kB/5ms)
24/09/14 16:52:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/09/14 16:52:23 INFO HiveThriftServer2: Started daemon with process name: 363@459adcdce09e
24/09/14 16:52:23 INFO SignalUtils: Registering signal handler for TERM
24/09/14 16:52:23 INFO SignalUtils: Registering signal handler for HUP
24/09/14 16:52:23 INFO SignalUtils: Registering signal handler for INT
24/09/14 16:52:23 INFO HiveThriftServer2: Starting SparkContext
24/09/14 16:52:23 INFO HiveConf: Found configuration file null
24/09/14 16:52:23 INFO SparkContext: Running Spark version 3.5.2
24/09/14 16:52:23 INFO SparkContext: OS info Linux, 6.5.0-1025-azure, amd64
24/09/14 16:52:23 INFO SparkContext: Java version 11.0.24
24/09/14 16:52:23 INFO ResourceUtils: ==============================================================
24/09/14 16:52:23 INFO ResourceUtils: No custom resources configured for spark.driver.
24/09/14 16:52:23 INFO ResourceUtils: ==============================================================
24/09/14 16:52:23 INFO SparkContext: Submitted application: SparkSQL::172.18.0.6
24/09/14 16:52:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/09/14 16:52:23 INFO ResourceProfile: Limiting resource is cpu
24/09/14 16:52:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/09/14 16:52:23 INFO SecurityManager: Changing view acls to: me
24/09/14 16:52:23 INFO SecurityManager: Changing modify acls to: me
24/09/14 16:52:23 INFO SecurityManager: Changing view acls groups to: 
24/09/14 16:52:23 INFO SecurityManager: Changing modify acls groups to: 
24/09/14 16:52:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: me; groups with view permissions: EMPTY; users with modify permissions: me; groups with modify permissions: EMPTY
24/09/14 16:52:24 INFO Utils: Successfully started service 'sparkDriver' on port 46353.
24/09/14 16:52:24 INFO SparkEnv: Registering MapOutputTracker
24/09/14 16:52:24 INFO SparkEnv: Registering BlockManagerMaster
24/09/14 16:52:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/09/14 16:52:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/09/14 16:52:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/09/14 16:52:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8d525f1c-3058-4b23-80bf-efd8f19a8753
24/09/14 16:52:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/09/14 16:52:24 INFO SparkEnv: Registering OutputCommitCoordinator
24/09/14 16:52:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/09/14 16:52:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/09/14 16:52:24 INFO SparkContext: Added JAR file:///home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar at spark://459adcdce09e:46353/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1726332743764
24/09/14 16:52:24 INFO SparkContext: Added JAR file:///home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar at spark://459adcdce09e:46353/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar with timestamp 1726332743764
24/09/14 16:52:24 INFO SparkContext: Added file file:///home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar at file:///home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1726332743764
24/09/14 16:52:24 INFO Utils: Copying /home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar to /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar
24/09/14 16:52:24 INFO SparkContext: Added file file:///home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar at file:///home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar with timestamp 1726332743764
24/09/14 16:52:24 INFO Utils: Copying /home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar to /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar
24/09/14 16:52:24 INFO Executor: Starting executor ID driver on host 459adcdce09e
24/09/14 16:52:24 INFO Executor: OS info Linux, 6.5.0-1025-azure, amd64
24/09/14 16:52:24 INFO Executor: Java version 11.0.24
24/09/14 16:52:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/09/14 16:52:24 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@faea4da for default.
24/09/14 16:52:24 INFO Executor: Fetching file:///home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1726332743764
24/09/14 16:52:24 INFO Utils: /home/me/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar has been previously copied to /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar
24/09/14 16:52:24 INFO Executor: Fetching file:///home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar with timestamp 1726332743764
24/09/14 16:52:24 INFO Utils: /home/me/.ivy2/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar has been previously copied to /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar
24/09/14 16:52:24 INFO Executor: Fetching spark://459adcdce09e:46353/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar with timestamp 1726332743764
24/09/14 16:52:24 INFO TransportClientFactory: Successfully created connection to 459adcdce09e/172.18.0.6:46353 after 23 ms (0 ms spent in bootstraps)
24/09/14 16:52:24 INFO Utils: Fetching spark://459adcdce09e:46353/jars/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar to /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/fetchFileTemp8821304270766831141.tmp
24/09/14 16:52:24 INFO Utils: /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/fetchFileTemp8821304270766831141.tmp has been previously copied to /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar
24/09/14 16:52:24 INFO Executor: Adding file:/tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/org.projectnessie.nessie-integrations_nessie-spark-extensions-3.5_2.12-0.92.1.jar to class loader default
24/09/14 16:52:24 INFO Executor: Fetching spark://459adcdce09e:46353/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1726332743764
24/09/14 16:52:24 INFO Utils: Fetching spark://459adcdce09e:46353/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar to /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/fetchFileTemp14749801496736931124.tmp
24/09/14 16:52:25 INFO Utils: /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/fetchFileTemp14749801496736931124.tmp has been previously copied to /tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar
24/09/14 16:52:25 INFO Executor: Adding file:/tmp/spark-08f2b459-7224-4475-9f13-a665e3ccead9/userFiles-371aecdb-8af8-4709-a27a-307b0c01a30b/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar to class loader default
24/09/14 16:52:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46053.
24/09/14 16:52:25 INFO NettyBlockTransferService: Server created on 459adcdce09e:46053
24/09/14 16:52:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/09/14 16:52:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 459adcdce09e, 46053, None)
24/09/14 16:52:25 INFO BlockManagerMasterEndpoint: Registering block manager 459adcdce09e:46053 with 434.4 MiB RAM, BlockManagerId(driver, 459adcdce09e, 46053, None)
24/09/14 16:52:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 459adcdce09e, 46053, None)
24/09/14 16:52:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 459adcdce09e, 46053, None)
24/09/14 16:52:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/09/14 16:52:25 INFO SharedState: Warehouse path is 'file:/opt/spark/spark-warehouse'.
24/09/14 16:52:26 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
24/09/14 16:52:26 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/opt/spark/spark-warehouse
24/09/14 16:52:26 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 16:52:26 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 16:52:26 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 16:52:26 INFO ObjectStore: ObjectStore, initialize called
24/09/14 16:52:26 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
24/09/14 16:52:26 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
24/09/14 16:52:30 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
24/09/14 16:52:32 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 16:52:32 INFO ObjectStore: Initialized ObjectStore
24/09/14 16:52:32 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
24/09/14 16:52:32 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.6
24/09/14 16:52:32 INFO HiveMetaStore: Added admin role in metastore
24/09/14 16:52:32 INFO HiveMetaStore: Added public role in metastore
24/09/14 16:52:32 INFO HiveMetaStore: No user is added in admin role, since config is empty
24/09/14 16:52:32 INFO HiveMetaStore: 0: get_database: default
24/09/14 16:52:32 INFO audit: ugi=me	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 16:52:32 INFO HiveUtils: Initializing execution hive, version 2.3.9
24/09/14 16:52:32 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/opt/spark/spark-warehouse
24/09/14 16:52:32 INFO SessionManager: Operation log root directory is created: /tmp/me/operation_logs
24/09/14 16:52:32 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
24/09/14 16:52:32 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
24/09/14 16:52:32 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
24/09/14 16:52:32 INFO AbstractService: Service:OperationManager is inited.
24/09/14 16:52:32 INFO AbstractService: Service:SessionManager is inited.
24/09/14 16:52:32 INFO AbstractService: Service: CLIService is inited.
24/09/14 16:52:32 INFO AbstractService: Service:ThriftBinaryCLIService is inited.
24/09/14 16:52:32 INFO AbstractService: Service: HiveServer2 is inited.
24/09/14 16:52:32 INFO AbstractService: Service:OperationManager is started.
24/09/14 16:52:32 INFO AbstractService: Service:SessionManager is started.
24/09/14 16:52:32 INFO AbstractService: Service: CLIService is started.
24/09/14 16:52:32 INFO AbstractService: Service:ThriftBinaryCLIService is started.
24/09/14 16:52:32 INFO ThriftCLIService: Starting ThriftBinaryCLIService on port 10000 with 5...500 worker threads
24/09/14 16:52:32 INFO AbstractService: Service:HiveServer2 is started.
24/09/14 16:52:32 INFO HiveThriftServer2: HiveThriftServer2 started
24/09/14 17:03:18 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:18 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/8a075591-fa2d-4803-bf77-8f4cfc0de0f9
24/09/14 17:03:18 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with cfcb627f-8362-4b7f-aee5-22f394a66334
24/09/14 17:03:18 INFO SparkExecuteStatementOperation: Running query with cfcb627f-8362-4b7f-aee5-22f394a66334
24/09/14 17:03:20 INFO HiveMetaStore: 1: get_database: global_temp
24/09/14 17:03:20 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: global_temp	
24/09/14 17:03:20 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:20 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:20 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:20 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:20 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:20 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:20 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
24/09/14 17:03:20 INFO HiveMetaStore: 1: get_database: default
24/09/14 17:03:20 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:20 INFO DAGScheduler: Asked to cancel job group cfcb627f-8362-4b7f-aee5-22f394a66334
24/09/14 17:03:20 INFO SparkExecuteStatementOperation: Close statement with cfcb627f-8362-4b7f-aee5-22f394a66334
24/09/14 17:03:20 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 047a1023-6f6a-4374-beb4-a75790f4faf7
24/09/14 17:03:20 INFO SparkExecuteStatementOperation: Running query with 047a1023-6f6a-4374-beb4-a75790f4faf7
24/09/14 17:03:20 INFO HiveMetaStore: 2: get_databases: *
24/09/14 17:03:20 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/14 17:03:20 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:20 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:20 INFO HiveMetaStore: 2: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:20 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:20 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:20 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:20 INFO CodeGenerator: Code generated in 186.192655 ms
24/09/14 17:03:20 INFO CodeGenerator: Code generated in 8.042493 ms
24/09/14 17:03:21 INFO CodeGenerator: Code generated in 10.71492 ms
24/09/14 17:03:21 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/14 17:03:21 INFO DAGScheduler: Asked to cancel job group 047a1023-6f6a-4374-beb4-a75790f4faf7
24/09/14 17:03:21 INFO SparkExecuteStatementOperation: Close statement with 047a1023-6f6a-4374-beb4-a75790f4faf7
24/09/14 17:03:21 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:21 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:21 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:21 INFO HiveMetaStore: 1: Cleaning up thread local RawStore...
24/09/14 17:03:21 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:21 INFO HiveMetaStore: 1: Done cleaning up thread local RawStore
24/09/14 17:03:21 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:21 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:21 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:21 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/c7e772f6-671e-4842-8135-a7129b638bf2
24/09/14 17:03:21 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 2a8eca24-4064-44f6-967a-39b9716910a9
24/09/14 17:03:21 INFO SparkExecuteStatementOperation: Running query with 2a8eca24-4064-44f6-967a-39b9716910a9
24/09/14 17:03:21 INFO HiveMetaStore: 3: get_database: default
24/09/14 17:03:21 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:21 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:21 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:21 INFO HiveMetaStore: 3: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:21 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:21 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:21 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:21 INFO DAGScheduler: Asked to cancel job group 2a8eca24-4064-44f6-967a-39b9716910a9
24/09/14 17:03:21 INFO SparkExecuteStatementOperation: Close statement with 2a8eca24-4064-44f6-967a-39b9716910a9
24/09/14 17:03:21 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 55c12ff5-7d2b-44c1-9dd4-52552ff2587d
24/09/14 17:03:21 INFO SparkExecuteStatementOperation: Running query with 55c12ff5-7d2b-44c1-9dd4-52552ff2587d
24/09/14 17:03:21 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:03:22 INFO DAGScheduler: Asked to cancel job group 55c12ff5-7d2b-44c1-9dd4-52552ff2587d
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Close statement with 55c12ff5-7d2b-44c1-9dd4-52552ff2587d
24/09/14 17:03:22 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:22 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:22 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:22 INFO HiveMetaStore: 3: Cleaning up thread local RawStore...
24/09/14 17:03:22 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:22 INFO HiveMetaStore: 3: Done cleaning up thread local RawStore
24/09/14 17:03:22 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:22 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:22 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:22 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/786caa57-eced-4599-9921-2c1d6b373d39
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 9dd772c1-2c58-4b45-8620-1e05dc1684eb
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Running query with 9dd772c1-2c58-4b45-8620-1e05dc1684eb
24/09/14 17:03:22 INFO HiveMetaStore: 4: get_database: default
24/09/14 17:03:22 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:22 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:22 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:22 INFO HiveMetaStore: 4: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:22 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:22 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:22 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:22 INFO DAGScheduler: Asked to cancel job group 9dd772c1-2c58-4b45-8620-1e05dc1684eb
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Close statement with 9dd772c1-2c58-4b45-8620-1e05dc1684eb
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with b2d6ee74-b9b7-4960-abee-46c911abb763
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Running query with b2d6ee74-b9b7-4960-abee-46c911abb763
24/09/14 17:03:22 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:03:22 INFO DAGScheduler: Asked to cancel job group b2d6ee74-b9b7-4960-abee-46c911abb763
24/09/14 17:03:22 ERROR SparkExecuteStatementOperation: Error executing query with b2d6ee74-b9b7-4960-abee-46c911abb763, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#12, tableName#13, isTemporary#14, information#15]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@2ab17f5f, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 04b7ef86-e25f-45c6-9182-7be9c14e5d7d
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Running query with 04b7ef86-e25f-45c6-9182-7be9c14e5d7d
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/14 17:03:22 INFO DAGScheduler: Asked to cancel job group 04b7ef86-e25f-45c6-9182-7be9c14e5d7d
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Close statement with 04b7ef86-e25f-45c6-9182-7be9c14e5d7d
24/09/14 17:03:22 INFO DAGScheduler: Asked to cancel job group b2d6ee74-b9b7-4960-abee-46c911abb763
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Close statement with b2d6ee74-b9b7-4960-abee-46c911abb763
24/09/14 17:03:22 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:22 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:22 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:22 INFO HiveMetaStore: 4: Cleaning up thread local RawStore...
24/09/14 17:03:22 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:22 INFO HiveMetaStore: 4: Done cleaning up thread local RawStore
24/09/14 17:03:22 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:22 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:22 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:22 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/3f046c15-aa7b-4da0-a136-81a145818730
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with aa8607f4-9291-430a-a5ff-be46411ef377
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Running query with aa8607f4-9291-430a-a5ff-be46411ef377
24/09/14 17:03:22 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:03:22 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:22 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:22 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:22 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:22 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:22 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:22 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:22 INFO DAGScheduler: Asked to cancel job group aa8607f4-9291-430a-a5ff-be46411ef377
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Close statement with aa8607f4-9291-430a-a5ff-be46411ef377
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.amazon_orders_silver"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240914165817
      ' with 646e18ee-13e9-484b-91fc-3dbf47b827bc
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Running query with 646e18ee-13e9-484b-91fc-3dbf47b827bc
24/09/14 17:03:22 INFO CodeGenerator: Code generated in 7.649001 ms
24/09/14 17:03:22 INFO CodeGenerator: Code generated in 9.672666 ms
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.amazon_orders_silver"} */
create or replace view nessie.gold.amazon_orders_silver
  
  
  as
    


SELECT
    *
FROM
    nessie.pipeline.silver_amazon_orders_last_batch
' with 17ef7b49-2401-47fa-b71e-785bdadc4559
24/09/14 17:03:22 INFO SparkExecuteStatementOperation: Running query with 17ef7b49-2401-47fa-b71e-785bdadc4559
24/09/14 17:03:22 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:03:22 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:03:23 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:03:23 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json
24/09/14 17:03:23 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=bcdd00b38776291b9404f381fb14673826d6ebf8832d1ac4c6f8af6be0537a25}'
24/09/14 17:03:23 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/14 17:03:24 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter true
24/09/14 17:03:24 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=nessie.silver.amazon_orders, snapshotId=6957663081544740559, filter=true, schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], projectedFieldNames=[Order_ID, Order_Date, Order_Status, Fulfilment, ORDERS_Channel, ship_service_level, Category, Size, Courier_Status, Qty, Currency, Amount, Ship_City, Ship_State, Ship_Postal_Code, Ship_Country, B2B, Fulfilled_By, New, PendingS, Ingestion_Date], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.259453514S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=7309}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1726332744679, engine-name=spark}}
24/09/14 17:03:24 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Aggregate Functions:
 MAX(Ingestion_Date)
Pushed Group by:
 
         
24/09/14 17:03:24 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Filters: Ingestion_Date IS NOT NULL
Post-Scan Filters: isnotnull(Ingestion_Date#104),(Ingestion_Date#104 = scalar-subquery#40 [])
         
24/09/14 17:03:24 INFO V2ScanRelationPushDown: 
Output: Order_ID#84, Order_Date#85, Order_Status#86, Fulfilment#87, ORDERS_Channel#88, ship_service_level#89, Category#90, Size#91, Courier_Status#92, Qty#93, Currency#94, Amount#95, Ship_City#96, Ship_State#97, Ship_Postal_Code#98, Ship_Country#99, B2B#100, Fulfilled_By#101, New#102, PendingS#103, Ingestion_Date#104
         
24/09/14 17:03:24 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter Ingestion_Date IS NOT NULL
24/09/14 17:03:24 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/14 17:03:24 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/14 17:03:24 INFO NessieIcebergClient: Committed 'gold.amazon_orders_silver' against 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=cef670c59e7f58463d87fdf60dfcc0e6730634bbe12419dc1dafc25338c4a59b}', expected commit-id was 'bcdd00b38776291b9404f381fb14673826d6ebf8832d1ac4c6f8af6be0537a25'
24/09/14 17:03:24 INFO BaseViewOperations: Successfully committed to view gold.amazon_orders_silver in 126 ms
24/09/14 17:03:24 INFO DAGScheduler: Asked to cancel job group 17ef7b49-2401-47fa-b71e-785bdadc4559
24/09/14 17:03:24 INFO SparkExecuteStatementOperation: Close statement with 17ef7b49-2401-47fa-b71e-785bdadc4559
24/09/14 17:03:24 INFO DAGScheduler: Asked to cancel job group 646e18ee-13e9-484b-91fc-3dbf47b827bc
24/09/14 17:03:24 INFO SparkExecuteStatementOperation: Close statement with 646e18ee-13e9-484b-91fc-3dbf47b827bc
24/09/14 17:03:24 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:24 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:24 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:24 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:03:24 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:24 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:03:24 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:24 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:24 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:24 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/f291f5d4-da1e-4549-849c-8ee0188d6917
24/09/14 17:03:24 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 6c2058c4-aec2-4afa-99b2-4af8c433a409
24/09/14 17:03:24 INFO SparkExecuteStatementOperation: Running query with 6c2058c4-aec2-4afa-99b2-4af8c433a409
24/09/14 17:03:24 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:24 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:24 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:24 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:24 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:24 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:24 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:24 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:24 INFO DAGScheduler: Asked to cancel job group 6c2058c4-aec2-4afa-99b2-4af8c433a409
24/09/14 17:03:24 INFO SparkExecuteStatementOperation: Close statement with 6c2058c4-aec2-4afa-99b2-4af8c433a409
24/09/14 17:03:24 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:24 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:24 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:24 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:24 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:24 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:24 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:24 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:39 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:39 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/5d5924a5-32e1-422b-931d-6087ecd7d2fb
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with f940e56d-d077-4d14-98f0-d1e4d7d745a3
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Running query with f940e56d-d077-4d14-98f0-d1e4d7d745a3
24/09/14 17:03:39 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:39 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:39 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:39 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:39 INFO DAGScheduler: Asked to cancel job group f940e56d-d077-4d14-98f0-d1e4d7d745a3
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Close statement with f940e56d-d077-4d14-98f0-d1e4d7d745a3
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with c3da7a3e-fde9-4984-87bd-6d4b17d260cc
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Running query with c3da7a3e-fde9-4984-87bd-6d4b17d260cc
24/09/14 17:03:39 INFO HiveMetaStore: 7: get_databases: *
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/14 17:03:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:39 INFO HiveMetaStore: 7: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:39 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:39 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/14 17:03:39 INFO DAGScheduler: Asked to cancel job group c3da7a3e-fde9-4984-87bd-6d4b17d260cc
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Close statement with c3da7a3e-fde9-4984-87bd-6d4b17d260cc
24/09/14 17:03:39 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:39 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:39 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:39 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:39 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:39 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:39 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/0de9d46f-0f8d-4598-b336-fb2b4ff92046
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 0d10ff9b-d90d-46e7-aef2-b415cbad5244
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Running query with 0d10ff9b-d90d-46e7-aef2-b415cbad5244
24/09/14 17:03:39 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:39 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:39 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:39 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:39 INFO DAGScheduler: Asked to cancel job group 0d10ff9b-d90d-46e7-aef2-b415cbad5244
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Close statement with 0d10ff9b-d90d-46e7-aef2-b415cbad5244
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 2b27a0bf-550b-450f-9771-3d8db28bb436
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Running query with 2b27a0bf-550b-450f-9771-3d8db28bb436
24/09/14 17:03:39 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:03:39 INFO DAGScheduler: Asked to cancel job group 2b27a0bf-550b-450f-9771-3d8db28bb436
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Close statement with 2b27a0bf-550b-450f-9771-3d8db28bb436
24/09/14 17:03:39 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:39 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:39 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:39 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:39 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:39 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:39 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/42082603-6527-436e-be7f-dc7bc33366ae
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 73162d4e-f053-4aa6-92c4-123e848b4de5
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Running query with 73162d4e-f053-4aa6-92c4-123e848b4de5
24/09/14 17:03:39 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:39 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:39 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:39 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:39 INFO DAGScheduler: Asked to cancel job group 73162d4e-f053-4aa6-92c4-123e848b4de5
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Close statement with 73162d4e-f053-4aa6-92c4-123e848b4de5
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with b38fc162-0ede-4cfe-9a9d-12cbe29fc1fe
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Running query with b38fc162-0ede-4cfe-9a9d-12cbe29fc1fe
24/09/14 17:03:39 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:03:39 INFO DAGScheduler: Asked to cancel job group b38fc162-0ede-4cfe-9a9d-12cbe29fc1fe
24/09/14 17:03:39 ERROR SparkExecuteStatementOperation: Error executing query with b38fc162-0ede-4cfe-9a9d-12cbe29fc1fe, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#153, tableName#154, isTemporary#155, information#156]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@dafbd65, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with f01ffcf6-05b6-4727-bf82-65269a1cc32d
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Running query with f01ffcf6-05b6-4727-bf82-65269a1cc32d
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/14 17:03:39 INFO DAGScheduler: Asked to cancel job group f01ffcf6-05b6-4727-bf82-65269a1cc32d
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Close statement with f01ffcf6-05b6-4727-bf82-65269a1cc32d
24/09/14 17:03:39 INFO DAGScheduler: Asked to cancel job group b38fc162-0ede-4cfe-9a9d-12cbe29fc1fe
24/09/14 17:03:39 INFO SparkExecuteStatementOperation: Close statement with b38fc162-0ede-4cfe-9a9d-12cbe29fc1fe
24/09/14 17:03:39 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:39 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:39 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:39 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:39 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:39 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:39 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:40 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/e1d59a69-a2b0-4039-8cfa-47429183e61f
24/09/14 17:03:40 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 4a112fda-cf46-4a1b-8098-e82d56ba0007
24/09/14 17:03:40 INFO SparkExecuteStatementOperation: Running query with 4a112fda-cf46-4a1b-8098-e82d56ba0007
24/09/14 17:03:40 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:40 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:40 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:40 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:40 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:40 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:40 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:40 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:40 INFO DAGScheduler: Asked to cancel job group 4a112fda-cf46-4a1b-8098-e82d56ba0007
24/09/14 17:03:40 INFO SparkExecuteStatementOperation: Close statement with 4a112fda-cf46-4a1b-8098-e82d56ba0007
24/09/14 17:03:40 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.product_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240914165817
      ' with 24df1e3b-9214-40aa-84b4-768bfda840d7
24/09/14 17:03:40 INFO SparkExecuteStatementOperation: Running query with 24df1e3b-9214-40aa-84b4-768bfda840d7
24/09/14 17:03:40 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.product_dim"} */

  
    
        create or replace table nessie.gold.product_dim
      
      
    using iceberg
      
      
      
      
      
      

      as
      







    -- Extract source and target columns from the column mapping dictionary
    
    

    
        
        -- Initial load: Create the table and load all rows
        WITH new_rows AS (
            SELECT DISTINCT
            
                src.Category AS Category , 
            
                src.Size AS Size
            
        FROM nessie.gold.amazon_orders_silver AS src
        )

        SELECT
            ROW_NUMBER() OVER (ORDER BY btch.Category, btch.Size) AS id,
            btch.*
        FROM new_rows AS btch
        
    

  ' with 90a4d1b5-110f-4bbe-ad44-1f93cc70193f
24/09/14 17:03:40 INFO SparkExecuteStatementOperation: Running query with 90a4d1b5-110f-4bbe-ad44-1f93cc70193f
24/09/14 17:03:40 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:03:40 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:03:40 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:03:40 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:03:40 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:03:40 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json
24/09/14 17:03:40 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=cef670c59e7f58463d87fdf60dfcc0e6730634bbe12419dc1dafc25338c4a59b}'
24/09/14 17:03:40 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/14 17:03:40 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/14 17:03:40 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/14 17:03:40 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter true
24/09/14 17:03:40 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=nessie.silver.amazon_orders, snapshotId=6957663081544740559, filter=true, schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], projectedFieldNames=[Order_ID, Order_Date, Order_Status, Fulfilment, ORDERS_Channel, ship_service_level, Category, Size, Courier_Status, Qty, Currency, Amount, Ship_City, Ship_State, Ship_Postal_Code, Ship_Country, B2B, Fulfilled_By, New, PendingS, Ingestion_Date], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.028623348S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=7309}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1726332744679, engine-name=spark}}
24/09/14 17:03:40 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Aggregate Functions:
 MAX(Ingestion_Date)
Pushed Group by:
 
         
24/09/14 17:03:40 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Filters: Ingestion_Date IS NOT NULL
Post-Scan Filters: isnotnull(Ingestion_Date#270),(Ingestion_Date#270 = scalar-subquery#206 [])
         
24/09/14 17:03:40 INFO V2ScanRelationPushDown: 
Output: Category#256, Size#257, Ingestion_Date#270
         
24/09/14 17:03:40 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter Ingestion_Date IS NOT NULL
24/09/14 17:03:40 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/14 17:03:40 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/14 17:03:40 INFO SparkWrite: Requesting 0 bytes advisory partition size for table gold.product_dim
24/09/14 17:03:40 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table gold.product_dim
24/09/14 17:03:40 INFO SparkWrite: Requesting [] as write ordering for table gold.product_dim
24/09/14 17:03:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
24/09/14 17:03:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.4 MiB)
24/09/14 17:03:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:03:40 INFO SparkContext: Created broadcast 0 from broadcast at SparkBatch.java:79
24/09/14 17:03:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/14 17:03:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/14 17:03:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:03:40 INFO SparkContext: Created broadcast 1 from broadcast at SparkBatch.java:79
24/09/14 17:03:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:41 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:41 INFO CodeGenerator: Code generated in 28.488652 ms
24/09/14 17:03:41 INFO CodeGenerator: Code generated in 8.211329 ms
24/09/14 17:03:41 INFO DAGScheduler: Registering RDD 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) as input to shuffle 0
24/09/14 17:03:41 INFO DAGScheduler: Got map stage job 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:03:41 INFO DAGScheduler: Final stage: ShuffleMapStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:03:41 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:03:41 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:41 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:03:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.0 KiB, free 434.3 MiB)
24/09/14 17:03:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.3 MiB)
24/09/14 17:03:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 459adcdce09e:46053 (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:03:41 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/09/14 17:03:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 9763 bytes) 
24/09/14 17:03:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/09/14 17:03:41 INFO CodeGenerator: Code generated in 60.985731 ms
24/09/14 17:03:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1928 bytes result sent to driver
24/09/14 17:03:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 290 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/09/14 17:03:41 INFO DAGScheduler: ShuffleMapStage 0 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.440 s
24/09/14 17:03:41 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:03:41 INFO DAGScheduler: running: Set()
24/09/14 17:03:41 INFO DAGScheduler: waiting: Set()
24/09/14 17:03:41 INFO DAGScheduler: failed: Set()
24/09/14 17:03:41 INFO CodeGenerator: Code generated in 23.885598 ms
24/09/14 17:03:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:03:41 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:03:41 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:03:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
24/09/14 17:03:41 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:41 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:03:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.0 KiB, free 434.3 MiB)
24/09/14 17:03:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.3 MiB)
24/09/14 17:03:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 459adcdce09e:46053 (size: 6.1 KiB, free: 434.4 MiB)
24/09/14 17:03:41 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:41 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/09/14 17:03:41 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:03:41 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
24/09/14 17:03:41 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 459adcdce09e:46053 in memory (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:03:41 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:03:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
24/09/14 17:03:42 INFO CodeGenerator: Code generated in 56.342634 ms
24/09/14 17:03:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 4002 bytes result sent to driver
24/09/14 17:03:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 176 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/09/14 17:03:42 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.219 s
24/09/14 17:03:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:03:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/09/14 17:03:42 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.236334 s
24/09/14 17:03:42 INFO CodeGenerator: Code generated in 64.738984 ms
24/09/14 17:03:42 INFO DAGScheduler: Registering RDD 9 (run at AccessController.java:0) as input to shuffle 1
24/09/14 17:03:42 INFO DAGScheduler: Got map stage job 2 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:03:42 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (run at AccessController.java:0)
24/09/14 17:03:42 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:03:42 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:42 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at run at AccessController.java:0), which has no missing parents
24/09/14 17:03:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 52.6 KiB, free 434.3 MiB)
24/09/14 17:03:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 22.8 KiB, free 434.2 MiB)
24/09/14 17:03:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 459adcdce09e:46053 (size: 22.8 KiB, free: 434.4 MiB)
24/09/14 17:03:42 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:42 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/09/14 17:03:42 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 16504 bytes) 
24/09/14 17:03:42 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 459adcdce09e:46053 in memory (size: 6.1 KiB, free: 434.4 MiB)
24/09/14 17:03:42 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
24/09/14 17:03:42 INFO CodeGenerator: Code generated in 57.025892 ms
24/09/14 17:03:42 INFO CodeGenerator: Code generated in 19.637227 ms
24/09/14 17:03:42 INFO CodeGenerator: Code generated in 14.195835 ms
24/09/14 17:03:42 INFO CodeGenerator: Code generated in 10.322633 ms
24/09/14 17:03:42 INFO VectorizedSparkParquetReaders: Enabling arrow.enable_unsafe_memory_access
24/09/14 17:03:42 INFO VectorizedSparkParquetReaders: Disabling arrow.enable_null_check_for_get
24/09/14 17:03:42 INFO BaseAllocator: Debug mode disabled. Enable with the VM option -Darrow.memory.debug.allocator=true.
24/09/14 17:03:42 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type
24/09/14 17:03:42 INFO CheckAllocator: Using DefaultAllocationManager at memory/DefaultAllocationManagerFactory.class
24/09/14 17:03:42 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:03:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 8215 bytes result sent to driver
24/09/14 17:03:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 904 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/09/14 17:03:43 INFO DAGScheduler: ShuffleMapStage 3 (run at AccessController.java:0) finished in 0.930 s
24/09/14 17:03:43 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:03:43 INFO DAGScheduler: running: Set()
24/09/14 17:03:43 INFO DAGScheduler: waiting: Set()
24/09/14 17:03:43 INFO DAGScheduler: failed: Set()
24/09/14 17:03:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:43 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 29.181383 ms
24/09/14 17:03:43 INFO DAGScheduler: Registering RDD 12 (run at AccessController.java:0) as input to shuffle 2
24/09/14 17:03:43 INFO DAGScheduler: Got map stage job 3 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:03:43 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (run at AccessController.java:0)
24/09/14 17:03:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
24/09/14 17:03:43 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:43 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[12] at run at AccessController.java:0), which has no missing parents
24/09/14 17:03:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 53.0 KiB, free 434.2 MiB)
24/09/14 17:03:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 23.7 KiB, free 434.2 MiB)
24/09/14 17:03:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 459adcdce09e:46053 (size: 23.7 KiB, free: 434.3 MiB)
24/09/14 17:03:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:43 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[12] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:43 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/09/14 17:03:43 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/14 17:03:43 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)
24/09/14 17:03:43 INFO ShuffleBlockFetcherIterator: Getting 1 (801.0 B) non-empty blocks including 1 (801.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:03:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 39.725224 ms
24/09/14 17:03:43 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 10239 bytes result sent to driver
24/09/14 17:03:43 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 459adcdce09e:46053 in memory (size: 22.8 KiB, free: 434.4 MiB)
24/09/14 17:03:43 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 86 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:43 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/09/14 17:03:43 INFO DAGScheduler: ShuffleMapStage 5 (run at AccessController.java:0) finished in 0.097 s
24/09/14 17:03:43 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:03:43 INFO DAGScheduler: running: Set()
24/09/14 17:03:43 INFO DAGScheduler: waiting: Set()
24/09/14 17:03:43 INFO DAGScheduler: failed: Set()
24/09/14 17:03:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 15.397757 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 11.948472 ms
24/09/14 17:03:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/14 17:03:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.2 MiB)
24/09/14 17:03:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 459adcdce09e:46053 (size: 3.1 KiB, free: 434.4 MiB)
24/09/14 17:03:43 INFO SparkContext: Created broadcast 6 from broadcast at SparkWrite.java:193
24/09/14 17:03:43 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.product_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/14 17:03:43 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/14 17:03:43 INFO DAGScheduler: Got job 4 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:03:43 INFO DAGScheduler: Final stage: ResultStage 8 (run at AccessController.java:0)
24/09/14 17:03:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
24/09/14 17:03:43 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:43 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[16] at run at AccessController.java:0), which has no missing parents
24/09/14 17:03:43 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 55.6 KiB, free 434.2 MiB)
24/09/14 17:03:43 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 25.2 KiB, free 434.1 MiB)
24/09/14 17:03:43 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 459adcdce09e:46053 (size: 25.2 KiB, free: 434.3 MiB)
24/09/14 17:03:43 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[16] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:43 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
24/09/14 17:03:43 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 4) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:03:43 INFO Executor: Running task 0.0 in stage 8.0 (TID 4)
24/09/14 17:03:43 INFO ShuffleBlockFetcherIterator: Getting 1 (207.0 B) non-empty blocks including 1 (207.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:03:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 16.278221 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 21.796976 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 7.847791 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 8.96133 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 8.41059 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 7.580053 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 7.743827 ms
24/09/14 17:03:43 INFO CodeGenerator: Code generated in 7.240858 ms
24/09/14 17:03:43 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:03:43 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 459adcdce09e:46053 in memory (size: 23.7 KiB, free: 434.4 MiB)
24/09/14 17:03:43 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/14 17:03:44 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 8.0)
24/09/14 17:03:44 INFO Executor: Finished task 0.0 in stage 8.0 (TID 4). 14182 bytes result sent to driver
24/09/14 17:03:44 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 4) in 462 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:44 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/09/14 17:03:44 INFO DAGScheduler: ResultStage 8 (run at AccessController.java:0) finished in 0.482 s
24/09/14 17:03:44 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:03:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
24/09/14 17:03:44 INFO DAGScheduler: Job 4 finished: run at AccessController.java:0, took 0.494288 s
24/09/14 17:03:44 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.product_dim, format=PARQUET) is committing.
24/09/14 17:03:44 INFO SparkWrite: Committing append with 1 new data files to table gold.product_dim
24/09/14 17:03:44 INFO SnapshotProducer: Committed snapshot 8090811787951346061 (MergeAppend)
24/09/14 17:03:44 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.product_dim, snapshotId=8090811787951346061, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.196614482S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=10}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=10}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1047}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1047}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1726332744679, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/14 17:03:44 INFO SparkWrite: Committed in 204 ms
24/09/14 17:03:44 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.product_dim, format=PARQUET) committed.
24/09/14 17:03:44 INFO NessieIcebergClient: Committed 'gold.product_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=70126dd3c9d067b5b04a99f46d25a347bbd9234f3d994b5a3995c484e92c0b78}', expected commit-id was 'cef670c59e7f58463d87fdf60dfcc0e6730634bbe12419dc1dafc25338c4a59b'
24/09/14 17:03:44 INFO BaseMetastoreTableOperations: Successfully committed to table gold.product_dim in 33 ms
24/09/14 17:03:44 INFO DAGScheduler: Asked to cancel job group 90a4d1b5-110f-4bbe-ad44-1f93cc70193f
24/09/14 17:03:44 INFO SparkExecuteStatementOperation: Close statement with 90a4d1b5-110f-4bbe-ad44-1f93cc70193f
24/09/14 17:03:44 INFO DAGScheduler: Asked to cancel job group 24df1e3b-9214-40aa-84b4-768bfda840d7
24/09/14 17:03:44 INFO SparkExecuteStatementOperation: Close statement with 24df1e3b-9214-40aa-84b4-768bfda840d7
24/09/14 17:03:44 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:44 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:44 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:44 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:44 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:44 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:44 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:44 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:44 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:44 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/59d70f63-28ca-4519-9838-02c3708e7eb2
24/09/14 17:03:44 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 0105dd6d-fd6c-42c5-9e3b-c39546da5db7
24/09/14 17:03:44 INFO SparkExecuteStatementOperation: Running query with 0105dd6d-fd6c-42c5-9e3b-c39546da5db7
24/09/14 17:03:44 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:44 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:44 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:44 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:44 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:44 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:44 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:44 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:44 INFO DAGScheduler: Asked to cancel job group 0105dd6d-fd6c-42c5-9e3b-c39546da5db7
24/09/14 17:03:44 INFO SparkExecuteStatementOperation: Close statement with 0105dd6d-fd6c-42c5-9e3b-c39546da5db7
24/09/14 17:03:44 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:44 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:44 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:44 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:44 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:44 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:44 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:44 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:51 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:51 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/9d26de64-3264-4ea6-b723-7fa0e678b793
24/09/14 17:03:51 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 5d86a6b0-fe09-4d0a-8bcd-25a285fd5910
24/09/14 17:03:51 INFO SparkExecuteStatementOperation: Running query with 5d86a6b0-fe09-4d0a-8bcd-25a285fd5910
24/09/14 17:03:51 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:51 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:51 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:51 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:51 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:51 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:51 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:51 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:52 INFO DAGScheduler: Asked to cancel job group 5d86a6b0-fe09-4d0a-8bcd-25a285fd5910
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Close statement with 5d86a6b0-fe09-4d0a-8bcd-25a285fd5910
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with c411b058-0a9d-4f1d-af40-92f4e77c9bac
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Running query with c411b058-0a9d-4f1d-af40-92f4e77c9bac
24/09/14 17:03:52 INFO HiveMetaStore: 8: get_databases: *
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/14 17:03:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:52 INFO HiveMetaStore: 8: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:52 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:52 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/14 17:03:52 INFO DAGScheduler: Asked to cancel job group c411b058-0a9d-4f1d-af40-92f4e77c9bac
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Close statement with c411b058-0a9d-4f1d-af40-92f4e77c9bac
24/09/14 17:03:52 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:52 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:52 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:52 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:52 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:52 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:52 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/466a3d80-46c8-4424-b9f4-dac8e82b6d8f
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 2bf7b78b-a502-471d-84ea-ac970615abdb
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Running query with 2bf7b78b-a502-471d-84ea-ac970615abdb
24/09/14 17:03:52 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:52 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:52 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:52 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:52 INFO DAGScheduler: Asked to cancel job group 2bf7b78b-a502-471d-84ea-ac970615abdb
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Close statement with 2bf7b78b-a502-471d-84ea-ac970615abdb
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 28da78db-623c-43a3-8155-9889e8813fab
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Running query with 28da78db-623c-43a3-8155-9889e8813fab
24/09/14 17:03:52 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:03:52 INFO DAGScheduler: Asked to cancel job group 28da78db-623c-43a3-8155-9889e8813fab
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Close statement with 28da78db-623c-43a3-8155-9889e8813fab
24/09/14 17:03:52 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:52 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:52 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:52 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:52 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:52 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:52 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/32a284f7-9720-4f3f-8298-9497f968e97f
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 3ccb876a-2e68-4690-8f27-15a0adc0a3a5
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Running query with 3ccb876a-2e68-4690-8f27-15a0adc0a3a5
24/09/14 17:03:52 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:52 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:52 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:52 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:52 INFO DAGScheduler: Asked to cancel job group 3ccb876a-2e68-4690-8f27-15a0adc0a3a5
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Close statement with 3ccb876a-2e68-4690-8f27-15a0adc0a3a5
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 8e57e36a-cfde-40ea-918f-fae5bdf2ef7c
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Running query with 8e57e36a-cfde-40ea-918f-fae5bdf2ef7c
24/09/14 17:03:52 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:03:52 INFO DAGScheduler: Asked to cancel job group 8e57e36a-cfde-40ea-918f-fae5bdf2ef7c
24/09/14 17:03:52 ERROR SparkExecuteStatementOperation: Error executing query with 8e57e36a-cfde-40ea-918f-fae5bdf2ef7c, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#309, tableName#310, isTemporary#311, information#312]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@871c8d2, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 38d48e52-7017-4559-86f5-34d73ee05ac0
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Running query with 38d48e52-7017-4559-86f5-34d73ee05ac0
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/14 17:03:52 INFO DAGScheduler: Asked to cancel job group 38d48e52-7017-4559-86f5-34d73ee05ac0
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Close statement with 38d48e52-7017-4559-86f5-34d73ee05ac0
24/09/14 17:03:52 INFO DAGScheduler: Asked to cancel job group 8e57e36a-cfde-40ea-918f-fae5bdf2ef7c
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Close statement with 8e57e36a-cfde-40ea-918f-fae5bdf2ef7c
24/09/14 17:03:52 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:52 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:52 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:52 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:52 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:52 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:52 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/e6f27b13-41fa-4067-b697-89260f908711
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 6a79ccb3-e9b1-4f19-9283-36c346b1dcf0
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Running query with 6a79ccb3-e9b1-4f19-9283-36c346b1dcf0
24/09/14 17:03:52 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:52 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:52 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:52 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:52 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:52 INFO DAGScheduler: Asked to cancel job group 6a79ccb3-e9b1-4f19-9283-36c346b1dcf0
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Close statement with 6a79ccb3-e9b1-4f19-9283-36c346b1dcf0
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.shipping_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240914165817
      ' with c88ec26f-5c91-445c-b8f4-ed82bea6f905
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Running query with c88ec26f-5c91-445c-b8f4-ed82bea6f905
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.shipping_dim"} */

  
    
        create or replace table nessie.gold.shipping_dim
      
      
    using iceberg
      
      
      
      
      
      

      as
      







    -- Extract source and target columns from the column mapping dictionary
    
    

    
        
        -- Initial load: Create the table and load all rows
        WITH new_rows AS (
            SELECT DISTINCT
            
                src.Order_Status AS shipping_status , 
            
                src.Fulfilment AS Fulfilment , 
            
                src.ship_service_level AS ship_service_level , 
            
                src.fulfilled_by AS fulfilled_by
            
        FROM nessie.gold.amazon_orders_silver AS src
        )

        SELECT
            ROW_NUMBER() OVER (ORDER BY btch.shipping_status, btch.Fulfilment, btch.ship_service_level, btch.fulfilled_by) AS id,
            btch.*
        FROM new_rows AS btch
        
    

  ' with e0ad13e8-868c-46aa-bd2c-799b8f94a4bd
24/09/14 17:03:52 INFO SparkExecuteStatementOperation: Running query with e0ad13e8-868c-46aa-bd2c-799b8f94a4bd
24/09/14 17:03:52 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:03:52 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:03:52 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 459adcdce09e:46053 in memory (size: 3.1 KiB, free: 434.4 MiB)
24/09/14 17:03:52 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 459adcdce09e:46053 in memory (size: 25.2 KiB, free: 434.4 MiB)
24/09/14 17:03:52 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:03:52 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:03:52 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:03:52 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json
24/09/14 17:03:52 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=70126dd3c9d067b5b04a99f46d25a347bbd9234f3d994b5a3995c484e92c0b78}'
24/09/14 17:03:52 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/14 17:03:52 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/14 17:03:52 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/14 17:03:52 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter true
24/09/14 17:03:52 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=nessie.silver.amazon_orders, snapshotId=6957663081544740559, filter=true, schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], projectedFieldNames=[Order_ID, Order_Date, Order_Status, Fulfilment, ORDERS_Channel, ship_service_level, Category, Size, Courier_Status, Qty, Currency, Amount, Ship_City, Ship_State, Ship_Postal_Code, Ship_Country, B2B, Fulfilled_By, New, PendingS, Ingestion_Date], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.021059989S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=7309}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1726332744679, engine-name=spark}}
24/09/14 17:03:52 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Aggregate Functions:
 MAX(Ingestion_Date)
Pushed Group by:
 
         
24/09/14 17:03:52 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Filters: Ingestion_Date IS NOT NULL
Post-Scan Filters: isnotnull(Ingestion_Date#428),(Ingestion_Date#428 = scalar-subquery#364 [])
         
24/09/14 17:03:52 INFO V2ScanRelationPushDown: 
Output: Order_Status#410, Fulfilment#411, ship_service_level#413, Fulfilled_By#425, Ingestion_Date#428
         
24/09/14 17:03:52 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter Ingestion_Date IS NOT NULL
24/09/14 17:03:52 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/14 17:03:52 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/14 17:03:52 INFO SparkWrite: Requesting 0 bytes advisory partition size for table gold.shipping_dim
24/09/14 17:03:52 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table gold.shipping_dim
24/09/14 17:03:52 INFO SparkWrite: Requesting [] as write ordering for table gold.shipping_dim
24/09/14 17:03:52 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/14 17:03:52 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/14 17:03:52 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:03:52 INFO SparkContext: Created broadcast 8 from broadcast at SparkBatch.java:79
24/09/14 17:03:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:52 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/14 17:03:52 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/14 17:03:52 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:03:52 INFO SparkContext: Created broadcast 9 from broadcast at SparkBatch.java:79
24/09/14 17:03:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:52 INFO DAGScheduler: Registering RDD 19 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) as input to shuffle 3
24/09/14 17:03:52 INFO DAGScheduler: Got map stage job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:03:52 INFO DAGScheduler: Final stage: ShuffleMapStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:03:52 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:03:52 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:52 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:03:52 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)
24/09/14 17:03:52 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.2 MiB)
24/09/14 17:03:52 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 459adcdce09e:46053 (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:03:52 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:52 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[19] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:52 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
24/09/14 17:03:52 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 5) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 9763 bytes) 
24/09/14 17:03:52 INFO Executor: Running task 0.0 in stage 9.0 (TID 5)
24/09/14 17:03:52 INFO Executor: Finished task 0.0 in stage 9.0 (TID 5). 1842 bytes result sent to driver
24/09/14 17:03:52 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 5) in 7 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:52 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/09/14 17:03:52 INFO DAGScheduler: ShuffleMapStage 9 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.014 s
24/09/14 17:03:52 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:03:52 INFO DAGScheduler: running: Set()
24/09/14 17:03:52 INFO DAGScheduler: waiting: Set()
24/09/14 17:03:52 INFO DAGScheduler: failed: Set()
24/09/14 17:03:53 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:03:53 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:03:53 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:03:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
24/09/14 17:03:53 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:53 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.0 KiB, free 434.2 MiB)
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.2 MiB)
24/09/14 17:03:53 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 459adcdce09e:46053 (size: 6.1 KiB, free: 434.4 MiB)
24/09/14 17:03:53 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[22] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:53 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
24/09/14 17:03:53 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 6) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:03:53 INFO Executor: Running task 0.0 in stage 11.0 (TID 6)
24/09/14 17:03:53 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:03:53 INFO Executor: Finished task 0.0 in stage 11.0 (TID 6). 3959 bytes result sent to driver
24/09/14 17:03:53 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 6) in 9 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:53 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/09/14 17:03:53 INFO DAGScheduler: ResultStage 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.015 s
24/09/14 17:03:53 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:03:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
24/09/14 17:03:53 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.020302 s
24/09/14 17:03:53 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:53 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 76.211396 ms
24/09/14 17:03:53 INFO DAGScheduler: Registering RDD 26 (run at AccessController.java:0) as input to shuffle 4
24/09/14 17:03:53 INFO DAGScheduler: Got map stage job 7 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:03:53 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (run at AccessController.java:0)
24/09/14 17:03:53 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:03:53 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:53 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[26] at run at AccessController.java:0), which has no missing parents
24/09/14 17:03:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 56.8 KiB, free 434.2 MiB)
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 434.2 MiB)
24/09/14 17:03:53 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 459adcdce09e:46053 (size: 23.8 KiB, free: 434.4 MiB)
24/09/14 17:03:53 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[26] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:53 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
24/09/14 17:03:53 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 7) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 16649 bytes) 
24/09/14 17:03:53 INFO Executor: Running task 0.0 in stage 12.0 (TID 7)
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 38.550192 ms
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 8.985544 ms
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 6.151697 ms
24/09/14 17:03:53 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 459adcdce09e:46053 in memory (size: 6.1 KiB, free: 434.4 MiB)
24/09/14 17:03:53 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 459adcdce09e:46053 in memory (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:03:53 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:03:53 INFO Executor: Finished task 0.0 in stage 12.0 (TID 7). 8172 bytes result sent to driver
24/09/14 17:03:53 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 7) in 172 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:53 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
24/09/14 17:03:53 INFO DAGScheduler: ShuffleMapStage 12 (run at AccessController.java:0) finished in 0.181 s
24/09/14 17:03:53 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:03:53 INFO DAGScheduler: running: Set()
24/09/14 17:03:53 INFO DAGScheduler: waiting: Set()
24/09/14 17:03:53 INFO DAGScheduler: failed: Set()
24/09/14 17:03:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:53 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 90.251954 ms
24/09/14 17:03:53 INFO DAGScheduler: Registering RDD 29 (run at AccessController.java:0) as input to shuffle 5
24/09/14 17:03:53 INFO DAGScheduler: Got map stage job 8 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:03:53 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (run at AccessController.java:0)
24/09/14 17:03:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
24/09/14 17:03:53 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:53 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[29] at run at AccessController.java:0), which has no missing parents
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 57.5 KiB, free 434.2 MiB)
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 24.6 KiB, free 434.2 MiB)
24/09/14 17:03:53 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 459adcdce09e:46053 (size: 24.6 KiB, free: 434.3 MiB)
24/09/14 17:03:53 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:53 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[29] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:53 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
24/09/14 17:03:53 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 8) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/14 17:03:53 INFO Executor: Running task 0.0 in stage 14.0 (TID 8)
24/09/14 17:03:53 INFO ShuffleBlockFetcherIterator: Getting 1 (414.0 B) non-empty blocks including 1 (414.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 33.594675 ms
24/09/14 17:03:53 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 459adcdce09e:46053 in memory (size: 23.8 KiB, free: 434.4 MiB)
24/09/14 17:03:53 INFO Executor: Finished task 0.0 in stage 14.0 (TID 8). 10239 bytes result sent to driver
24/09/14 17:03:53 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 8) in 80 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:53 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/09/14 17:03:53 INFO DAGScheduler: ShuffleMapStage 14 (run at AccessController.java:0) finished in 0.087 s
24/09/14 17:03:53 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:03:53 INFO DAGScheduler: running: Set()
24/09/14 17:03:53 INFO DAGScheduler: waiting: Set()
24/09/14 17:03:53 INFO DAGScheduler: failed: Set()
24/09/14 17:03:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 6.937152 ms
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.2 MiB)
24/09/14 17:03:53 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 459adcdce09e:46053 (size: 3.2 KiB, free: 434.4 MiB)
24/09/14 17:03:53 INFO SparkContext: Created broadcast 14 from broadcast at SparkWrite.java:193
24/09/14 17:03:53 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.shipping_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/14 17:03:53 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/14 17:03:53 INFO DAGScheduler: Got job 9 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:03:53 INFO DAGScheduler: Final stage: ResultStage 17 (run at AccessController.java:0)
24/09/14 17:03:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
24/09/14 17:03:53 INFO DAGScheduler: Missing parents: List()
24/09/14 17:03:53 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[33] at run at AccessController.java:0), which has no missing parents
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 58.7 KiB, free 434.2 MiB)
24/09/14 17:03:53 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 26.0 KiB, free 434.1 MiB)
24/09/14 17:03:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 459adcdce09e:46053 (size: 26.0 KiB, free: 434.3 MiB)
24/09/14 17:03:53 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585
24/09/14 17:03:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[33] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:03:53 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
24/09/14 17:03:53 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 9) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:03:53 INFO Executor: Running task 0.0 in stage 17.0 (TID 9)
24/09/14 17:03:53 INFO ShuffleBlockFetcherIterator: Getting 1 (276.0 B) non-empty blocks including 1 (276.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:03:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 10.968915 ms
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 9.310982 ms
24/09/14 17:03:53 INFO CodeGenerator: Code generated in 7.260034 ms
24/09/14 17:03:53 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:03:53 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/14 17:03:53 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 17.0)
24/09/14 17:03:53 INFO Executor: Finished task 0.0 in stage 17.0 (TID 9). 14384 bytes result sent to driver
24/09/14 17:03:53 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 9) in 161 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:03:53 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
24/09/14 17:03:53 INFO DAGScheduler: ResultStage 17 (run at AccessController.java:0) finished in 0.177 s
24/09/14 17:03:53 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:03:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
24/09/14 17:03:53 INFO DAGScheduler: Job 9 finished: run at AccessController.java:0, took 0.186001 s
24/09/14 17:03:53 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.shipping_dim, format=PARQUET) is committing.
24/09/14 17:03:53 INFO SparkWrite: Committing append with 1 new data files to table gold.shipping_dim
24/09/14 17:03:53 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 459adcdce09e:46053 in memory (size: 24.6 KiB, free: 434.4 MiB)
24/09/14 17:03:53 INFO SnapshotProducer: Committed snapshot 8429945481820994034 (MergeAppend)
24/09/14 17:03:53 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.shipping_dim, snapshotId=8429945481820994034, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.117818732S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=3}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1756}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1756}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1726332744679, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/14 17:03:53 INFO SparkWrite: Committed in 119 ms
24/09/14 17:03:53 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.shipping_dim, format=PARQUET) committed.
24/09/14 17:03:54 INFO NessieIcebergClient: Committed 'gold.shipping_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=b3fb2e1d32b526b2c9502517426501c508017bcf0f9f1b522a295e226d2e54f2}', expected commit-id was '70126dd3c9d067b5b04a99f46d25a347bbd9234f3d994b5a3995c484e92c0b78'
24/09/14 17:03:54 INFO BaseMetastoreTableOperations: Successfully committed to table gold.shipping_dim in 71 ms
24/09/14 17:03:54 INFO DAGScheduler: Asked to cancel job group e0ad13e8-868c-46aa-bd2c-799b8f94a4bd
24/09/14 17:03:54 INFO SparkExecuteStatementOperation: Close statement with e0ad13e8-868c-46aa-bd2c-799b8f94a4bd
24/09/14 17:03:54 INFO DAGScheduler: Asked to cancel job group c88ec26f-5c91-445c-b8f4-ed82bea6f905
24/09/14 17:03:54 INFO SparkExecuteStatementOperation: Close statement with c88ec26f-5c91-445c-b8f4-ed82bea6f905
24/09/14 17:03:54 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:54 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:54 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:54 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:54 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:54 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:54 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:54 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:03:54 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:03:54 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/416babc0-0b7c-4f07-b9f7-0096f175cc30
24/09/14 17:03:54 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 605d8f66-a300-4ae0-9151-7022eff8ea0d
24/09/14 17:03:54 INFO SparkExecuteStatementOperation: Running query with 605d8f66-a300-4ae0-9151-7022eff8ea0d
24/09/14 17:03:54 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:03:54 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:03:54 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:03:54 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:03:54 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:03:54 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:54 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:54 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:54 INFO DAGScheduler: Asked to cancel job group 605d8f66-a300-4ae0-9151-7022eff8ea0d
24/09/14 17:03:54 INFO SparkExecuteStatementOperation: Close statement with 605d8f66-a300-4ae0-9151-7022eff8ea0d
24/09/14 17:03:54 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:03:54 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:03:54 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:03:54 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:03:54 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:03:54 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:03:54 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:03:54 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:02 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:02 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/ef9379e3-9c01-41c2-825b-c016c46d57d1
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 477ebf7d-a2e8-4fa9-ac69-e49eeeb05334
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with 477ebf7d-a2e8-4fa9-ac69-e49eeeb05334
24/09/14 17:04:02 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:02 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:02 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:02 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:02 INFO DAGScheduler: Asked to cancel job group 477ebf7d-a2e8-4fa9-ac69-e49eeeb05334
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Close statement with 477ebf7d-a2e8-4fa9-ac69-e49eeeb05334
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with aa242d35-e0a8-4e07-a15f-6540c84244bd
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with aa242d35-e0a8-4e07-a15f-6540c84244bd
24/09/14 17:04:02 INFO HiveMetaStore: 9: get_databases: *
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:02 INFO HiveMetaStore: 9: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:02 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:02 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/14 17:04:02 INFO DAGScheduler: Asked to cancel job group aa242d35-e0a8-4e07-a15f-6540c84244bd
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Close statement with aa242d35-e0a8-4e07-a15f-6540c84244bd
24/09/14 17:04:02 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:02 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:02 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:02 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:02 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:02 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:02 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/3517f583-6f71-40e7-b78e-93314d0d43bb
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 600567fe-6acb-4c03-b79c-1b933c42fe1a
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with 600567fe-6acb-4c03-b79c-1b933c42fe1a
24/09/14 17:04:02 INFO HiveMetaStore: 6: get_database: default
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:02 INFO HiveMetaStore: 6: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:02 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:02 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:02 INFO DAGScheduler: Asked to cancel job group 600567fe-6acb-4c03-b79c-1b933c42fe1a
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Close statement with 600567fe-6acb-4c03-b79c-1b933c42fe1a
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with ad4516a8-84d8-4494-8f48-1b7a8238bd62
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with ad4516a8-84d8-4494-8f48-1b7a8238bd62
24/09/14 17:04:02 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:04:02 INFO DAGScheduler: Asked to cancel job group ad4516a8-84d8-4494-8f48-1b7a8238bd62
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Close statement with ad4516a8-84d8-4494-8f48-1b7a8238bd62
24/09/14 17:04:02 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:02 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:02 INFO HiveMetaStore: 6: Cleaning up thread local RawStore...
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:02 INFO HiveMetaStore: 6: Done cleaning up thread local RawStore
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:02 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:02 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:02 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/232b66f2-033d-4d14-b265-d18327de23fd
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 59f4103f-0054-4eee-b5be-75d120cf5c45
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with 59f4103f-0054-4eee-b5be-75d120cf5c45
24/09/14 17:04:02 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:02 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:02 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:02 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:02 INFO DAGScheduler: Asked to cancel job group 59f4103f-0054-4eee-b5be-75d120cf5c45
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Close statement with 59f4103f-0054-4eee-b5be-75d120cf5c45
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 9506efbd-9e01-4df0-84b1-275d6389ca67
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with 9506efbd-9e01-4df0-84b1-275d6389ca67
24/09/14 17:04:02 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:04:02 INFO DAGScheduler: Asked to cancel job group 9506efbd-9e01-4df0-84b1-275d6389ca67
24/09/14 17:04:02 ERROR SparkExecuteStatementOperation: Error executing query with 9506efbd-9e01-4df0-84b1-275d6389ca67, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#471, tableName#472, isTemporary#473, information#474]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@33b32b65, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with f1738191-c0a3-42cf-9f5c-5205fca0c4d3
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with f1738191-c0a3-42cf-9f5c-5205fca0c4d3
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/14 17:04:02 INFO DAGScheduler: Asked to cancel job group f1738191-c0a3-42cf-9f5c-5205fca0c4d3
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Close statement with f1738191-c0a3-42cf-9f5c-5205fca0c4d3
24/09/14 17:04:02 INFO DAGScheduler: Asked to cancel job group 9506efbd-9e01-4df0-84b1-275d6389ca67
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Close statement with 9506efbd-9e01-4df0-84b1-275d6389ca67
24/09/14 17:04:02 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:02 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:02 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:02 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:02 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:02 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:02 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/c5b4baa4-bf93-481f-a400-98628c8baa66
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 56e61284-c188-4f12-8224-6267605547fd
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with 56e61284-c188-4f12-8224-6267605547fd
24/09/14 17:04:02 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:02 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:02 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:02 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:02 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:02 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:02 INFO DAGScheduler: Asked to cancel job group 56e61284-c188-4f12-8224-6267605547fd
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Close statement with 56e61284-c188-4f12-8224-6267605547fd
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.currency_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240914165817
      ' with f02f0dfe-55ae-4ea7-b94d-dd06e991ec03
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with f02f0dfe-55ae-4ea7-b94d-dd06e991ec03
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.currency_dim"} */

  
    
        create or replace table nessie.gold.currency_dim
      
      
    using iceberg
      
      
      
      
      
      

      as
      







    -- Extract source and target columns from the column mapping dictionary
    
    

    
        
        -- Initial load: Create the table and load all rows
        WITH new_rows AS (
            SELECT DISTINCT
            
                src.currency AS currency
            
        FROM nessie.gold.amazon_orders_silver AS src
        )

        SELECT
            ROW_NUMBER() OVER (ORDER BY btch.currency) AS id,
            btch.*
        FROM new_rows AS btch
        
    

  ' with cce446cf-f376-42bf-abc2-2e05d5ddcf8f
24/09/14 17:04:02 INFO SparkExecuteStatementOperation: Running query with cce446cf-f376-42bf-abc2-2e05d5ddcf8f
24/09/14 17:04:02 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:04:02 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:04:02 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:04:02 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:04:02 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:04:02 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json
24/09/14 17:04:02 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=b3fb2e1d32b526b2c9502517426501c508017bcf0f9f1b522a295e226d2e54f2}'
24/09/14 17:04:02 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/14 17:04:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 459adcdce09e:46053 in memory (size: 26.0 KiB, free: 434.4 MiB)
24/09/14 17:04:02 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:04:02 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 459adcdce09e:46053 in memory (size: 3.2 KiB, free: 434.4 MiB)
24/09/14 17:04:02 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:04:02 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/14 17:04:02 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/14 17:04:02 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter true
24/09/14 17:04:03 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=nessie.silver.amazon_orders, snapshotId=6957663081544740559, filter=true, schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], projectedFieldNames=[Order_ID, Order_Date, Order_Status, Fulfilment, ORDERS_Channel, ship_service_level, Category, Size, Courier_Status, Qty, Currency, Amount, Ship_City, Ship_State, Ship_Postal_Code, Ship_Country, B2B, Fulfilled_By, New, PendingS, Ingestion_Date], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.018179177S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=7309}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1726332744679, engine-name=spark}}
24/09/14 17:04:03 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Aggregate Functions:
 MAX(Ingestion_Date)
Pushed Group by:
 
         
24/09/14 17:04:03 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Filters: Ingestion_Date IS NOT NULL
Post-Scan Filters: isnotnull(Ingestion_Date#587),(Ingestion_Date#587 = scalar-subquery#523 [])
         
24/09/14 17:04:03 INFO V2ScanRelationPushDown: 
Output: Currency#577, Ingestion_Date#587
         
24/09/14 17:04:03 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter Ingestion_Date IS NOT NULL
24/09/14 17:04:03 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/14 17:04:03 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/14 17:04:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table gold.currency_dim
24/09/14 17:04:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table gold.currency_dim
24/09/14 17:04:03 INFO SparkWrite: Requesting [] as write ordering for table gold.currency_dim
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.4 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:04:03 INFO SparkContext: Created broadcast 16 from broadcast at SparkBatch.java:79
24/09/14 17:04:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:04:03 INFO SparkContext: Created broadcast 17 from broadcast at SparkBatch.java:79
24/09/14 17:04:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:03 INFO DAGScheduler: Registering RDD 36 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) as input to shuffle 6
24/09/14 17:04:03 INFO DAGScheduler: Got map stage job 10 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:04:03 INFO DAGScheduler: Final stage: ShuffleMapStage 18 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:04:03 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:04:03 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:03 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 12.0 KiB, free 434.3 MiB)
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.3 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 459adcdce09e:46053 (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:04:03 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:03 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
24/09/14 17:04:03 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 10) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 9763 bytes) 
24/09/14 17:04:03 INFO Executor: Running task 0.0 in stage 18.0 (TID 10)
24/09/14 17:04:03 INFO Executor: Finished task 0.0 in stage 18.0 (TID 10). 1842 bytes result sent to driver
24/09/14 17:04:03 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 10) in 9 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:03 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
24/09/14 17:04:03 INFO DAGScheduler: ShuffleMapStage 18 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.017 s
24/09/14 17:04:03 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:03 INFO DAGScheduler: running: Set()
24/09/14 17:04:03 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:03 INFO DAGScheduler: failed: Set()
24/09/14 17:04:03 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:04:03 INFO DAGScheduler: Got job 11 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:04:03 INFO DAGScheduler: Final stage: ResultStage 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:04:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
24/09/14 17:04:03 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:03 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 13.0 KiB, free 434.3 MiB)
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.3 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 459adcdce09e:46053 (size: 6.1 KiB, free: 434.4 MiB)
24/09/14 17:04:03 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:03 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
24/09/14 17:04:03 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 11) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:04:03 INFO Executor: Running task 0.0 in stage 20.0 (TID 11)
24/09/14 17:04:03 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/09/14 17:04:03 INFO Executor: Finished task 0.0 in stage 20.0 (TID 11). 3959 bytes result sent to driver
24/09/14 17:04:03 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 11) in 14 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:03 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
24/09/14 17:04:03 INFO DAGScheduler: ResultStage 20 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.022 s
24/09/14 17:04:03 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:04:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
24/09/14 17:04:03 INFO DAGScheduler: Job 11 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.028672 s
24/09/14 17:04:03 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:03 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 62.259247 ms
24/09/14 17:04:03 INFO DAGScheduler: Registering RDD 43 (run at AccessController.java:0) as input to shuffle 7
24/09/14 17:04:03 INFO DAGScheduler: Got map stage job 12 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:03 INFO DAGScheduler: Final stage: ShuffleMapStage 21 (run at AccessController.java:0)
24/09/14 17:04:03 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:04:03 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:03 INFO DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[43] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 51.0 KiB, free 434.2 MiB)
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 22.4 KiB, free 434.2 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 459adcdce09e:46053 (size: 22.4 KiB, free: 434.4 MiB)
24/09/14 17:04:03 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[43] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:03 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
24/09/14 17:04:03 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 12) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 16449 bytes) 
24/09/14 17:04:03 INFO Executor: Running task 0.0 in stage 21.0 (TID 12)
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 25.811806 ms
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 6.700711 ms
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 3.867205 ms
24/09/14 17:04:03 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:04:03 INFO Executor: Finished task 0.0 in stage 21.0 (TID 12). 8129 bytes result sent to driver
24/09/14 17:04:03 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 12) in 89 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:03 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
24/09/14 17:04:03 INFO DAGScheduler: ShuffleMapStage 21 (run at AccessController.java:0) finished in 0.096 s
24/09/14 17:04:03 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:03 INFO DAGScheduler: running: Set()
24/09/14 17:04:03 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:03 INFO DAGScheduler: failed: Set()
24/09/14 17:04:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:03 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 20.93638 ms
24/09/14 17:04:03 INFO DAGScheduler: Registering RDD 46 (run at AccessController.java:0) as input to shuffle 8
24/09/14 17:04:03 INFO DAGScheduler: Got map stage job 13 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:03 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (run at AccessController.java:0)
24/09/14 17:04:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)
24/09/14 17:04:03 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:03 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[46] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 51.5 KiB, free 434.2 MiB)
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 23.3 KiB, free 434.1 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 459adcdce09e:46053 (size: 23.3 KiB, free: 434.3 MiB)
24/09/14 17:04:03 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[46] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:03 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
24/09/14 17:04:03 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 13) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/14 17:04:03 INFO Executor: Running task 0.0 in stage 23.0 (TID 13)
24/09/14 17:04:03 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 18.662809 ms
24/09/14 17:04:03 INFO Executor: Finished task 0.0 in stage 23.0 (TID 13). 10196 bytes result sent to driver
24/09/14 17:04:03 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 13) in 37 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:03 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
24/09/14 17:04:03 INFO DAGScheduler: ShuffleMapStage 23 (run at AccessController.java:0) finished in 0.044 s
24/09/14 17:04:03 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:03 INFO DAGScheduler: running: Set()
24/09/14 17:04:03 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:03 INFO DAGScheduler: failed: Set()
24/09/14 17:04:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 5.565363 ms
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.1 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 459adcdce09e:46053 (size: 3.1 KiB, free: 434.3 MiB)
24/09/14 17:04:03 INFO SparkContext: Created broadcast 22 from broadcast at SparkWrite.java:193
24/09/14 17:04:03 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.currency_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/14 17:04:03 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 459adcdce09e:46053 in memory (size: 6.1 KiB, free: 434.3 MiB)
24/09/14 17:04:03 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/14 17:04:03 INFO DAGScheduler: Got job 14 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:03 INFO DAGScheduler: Final stage: ResultStage 26 (run at AccessController.java:0)
24/09/14 17:04:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
24/09/14 17:04:03 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:03 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[50] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 54.9 KiB, free 434.1 MiB)
24/09/14 17:04:03 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 25.0 KiB, free 434.1 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 459adcdce09e:46053 (size: 25.0 KiB, free: 434.3 MiB)
24/09/14 17:04:03 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[50] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:03 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
24/09/14 17:04:03 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 14) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:04:03 INFO Executor: Running task 0.0 in stage 26.0 (TID 14)
24/09/14 17:04:03 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 459adcdce09e:46053 in memory (size: 22.4 KiB, free: 434.3 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 459adcdce09e:46053 in memory (size: 23.3 KiB, free: 434.4 MiB)
24/09/14 17:04:03 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 459adcdce09e:46053 in memory (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:04:03 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:03 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 6.159713 ms
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 3.996466 ms
24/09/14 17:04:03 INFO CodeGenerator: Code generated in 5.341556 ms
24/09/14 17:04:03 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:03 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/14 17:04:03 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 26.0)
24/09/14 17:04:03 INFO Executor: Finished task 0.0 in stage 26.0 (TID 14). 14010 bytes result sent to driver
24/09/14 17:04:03 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 14) in 108 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:03 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
24/09/14 17:04:03 INFO DAGScheduler: ResultStage 26 (run at AccessController.java:0) finished in 0.114 s
24/09/14 17:04:03 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:04:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
24/09/14 17:04:03 INFO DAGScheduler: Job 14 finished: run at AccessController.java:0, took 0.117720 s
24/09/14 17:04:03 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.currency_dim, format=PARQUET) is committing.
24/09/14 17:04:03 INFO SparkWrite: Committing append with 1 new data files to table gold.currency_dim
24/09/14 17:04:03 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 459adcdce09e:46053 in memory (size: 25.0 KiB, free: 434.4 MiB)
24/09/14 17:04:03 INFO SnapshotProducer: Committed snapshot 1768952174985901472 (MergeAppend)
24/09/14 17:04:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.currency_dim, snapshotId=1768952174985901472, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.176996174S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=1}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=623}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=623}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1726332744679, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/14 17:04:03 INFO SparkWrite: Committed in 177 ms
24/09/14 17:04:03 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.currency_dim, format=PARQUET) committed.
24/09/14 17:04:03 INFO NessieIcebergClient: Committed 'gold.currency_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=0aa345600910f01de9d2c27caf2ebfaac9765429dbf86775e32eee30352f86b4}', expected commit-id was 'b3fb2e1d32b526b2c9502517426501c508017bcf0f9f1b522a295e226d2e54f2'
24/09/14 17:04:03 INFO BaseMetastoreTableOperations: Successfully committed to table gold.currency_dim in 68 ms
24/09/14 17:04:03 INFO DAGScheduler: Asked to cancel job group cce446cf-f376-42bf-abc2-2e05d5ddcf8f
24/09/14 17:04:03 INFO SparkExecuteStatementOperation: Close statement with cce446cf-f376-42bf-abc2-2e05d5ddcf8f
24/09/14 17:04:03 INFO DAGScheduler: Asked to cancel job group f02f0dfe-55ae-4ea7-b94d-dd06e991ec03
24/09/14 17:04:03 INFO SparkExecuteStatementOperation: Close statement with f02f0dfe-55ae-4ea7-b94d-dd06e991ec03
24/09/14 17:04:03 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:03 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:03 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:03 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:03 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:03 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:03 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:03 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:03 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:03 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/43ebe635-36e0-455f-a4bd-1f0bcd66cddc
24/09/14 17:04:03 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with ff8a4972-9d6d-457e-8aec-502c07f43b7b
24/09/14 17:04:03 INFO SparkExecuteStatementOperation: Running query with ff8a4972-9d6d-457e-8aec-502c07f43b7b
24/09/14 17:04:03 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:03 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:03 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:03 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:03 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:03 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:03 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:03 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:03 INFO DAGScheduler: Asked to cancel job group ff8a4972-9d6d-457e-8aec-502c07f43b7b
24/09/14 17:04:03 INFO SparkExecuteStatementOperation: Close statement with ff8a4972-9d6d-457e-8aec-502c07f43b7b
24/09/14 17:04:03 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:03 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:03 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:03 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:03 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:03 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:03 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:03 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:12 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:12 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/86f1091c-b7af-4252-90f1-fca02f76f20e
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with e80058d7-34bd-421f-a339-03ef2cfebb60
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with e80058d7-34bd-421f-a339-03ef2cfebb60
24/09/14 17:04:12 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:12 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:12 INFO DAGScheduler: Asked to cancel job group e80058d7-34bd-421f-a339-03ef2cfebb60
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Close statement with e80058d7-34bd-421f-a339-03ef2cfebb60
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 39589447-4180-4063-aca4-a9dd91d10464
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with 39589447-4180-4063-aca4-a9dd91d10464
24/09/14 17:04:12 INFO HiveMetaStore: 10: get_databases: *
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:12 INFO HiveMetaStore: 10: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/14 17:04:12 INFO DAGScheduler: Asked to cancel job group 39589447-4180-4063-aca4-a9dd91d10464
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Close statement with 39589447-4180-4063-aca4-a9dd91d10464
24/09/14 17:04:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:12 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:12 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:12 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:12 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:12 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/887fd955-1d2b-4248-b9fa-2397add47113
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with ab2fa36c-3857-4be8-875e-1339f52ab8ae
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with ab2fa36c-3857-4be8-875e-1339f52ab8ae
24/09/14 17:04:12 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:12 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:12 INFO DAGScheduler: Asked to cancel job group ab2fa36c-3857-4be8-875e-1339f52ab8ae
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Close statement with ab2fa36c-3857-4be8-875e-1339f52ab8ae
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with ac49590b-d6f3-4c01-b2e7-788aba26b795
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with ac49590b-d6f3-4c01-b2e7-788aba26b795
24/09/14 17:04:12 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:04:12 INFO DAGScheduler: Asked to cancel job group ac49590b-d6f3-4c01-b2e7-788aba26b795
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Close statement with ac49590b-d6f3-4c01-b2e7-788aba26b795
24/09/14 17:04:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:12 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:12 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:12 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:12 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:12 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/ad102c56-8ece-4870-9e32-12d1a0acc1f9
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with cd7b9ec7-4c17-4897-8c0c-1557abb933a9
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with cd7b9ec7-4c17-4897-8c0c-1557abb933a9
24/09/14 17:04:12 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:12 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:12 INFO DAGScheduler: Asked to cancel job group cd7b9ec7-4c17-4897-8c0c-1557abb933a9
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Close statement with cd7b9ec7-4c17-4897-8c0c-1557abb933a9
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with 73164464-84e6-4824-94b3-2d16e9ccfd91
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with 73164464-84e6-4824-94b3-2d16e9ccfd91
24/09/14 17:04:12 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:04:12 INFO DAGScheduler: Asked to cancel job group 73164464-84e6-4824-94b3-2d16e9ccfd91
24/09/14 17:04:12 ERROR SparkExecuteStatementOperation: Error executing query with 73164464-84e6-4824-94b3-2d16e9ccfd91, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#624, tableName#625, isTemporary#626, information#627]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@e1aa595, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 00145efd-7339-4999-b77d-a19ce1498e96
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with 00145efd-7339-4999-b77d-a19ce1498e96
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/14 17:04:12 INFO DAGScheduler: Asked to cancel job group 00145efd-7339-4999-b77d-a19ce1498e96
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Close statement with 00145efd-7339-4999-b77d-a19ce1498e96
24/09/14 17:04:12 INFO DAGScheduler: Asked to cancel job group 73164464-84e6-4824-94b3-2d16e9ccfd91
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Close statement with 73164464-84e6-4824-94b3-2d16e9ccfd91
24/09/14 17:04:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:12 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:12 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:12 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:12 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:12 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/cd188385-a5df-4063-a8e3-5437aca12492
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 795fbae0-0958-4422-8895-0a7b16fbddda
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with 795fbae0-0958-4422-8895-0a7b16fbddda
24/09/14 17:04:12 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:12 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:12 INFO DAGScheduler: Asked to cancel job group 795fbae0-0958-4422-8895-0a7b16fbddda
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Close statement with 795fbae0-0958-4422-8895-0a7b16fbddda
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.date_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240914165817
      ' with 3bb54de8-8b63-4694-bfe7-ced6035abe9d
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with 3bb54de8-8b63-4694-bfe7-ced6035abe9d
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.date_dim"} */

  
    
        create or replace table nessie.gold.date_dim
      
      
    using iceberg
      
      
      partitioned by (MONTH(full_date))
      
      
      

      as
      









    -- Extract source and target columns from the column mapping dictionary
    
    
    

    
    
    WITH new_rows AS 
    (
        SELECT DISTINCT
                src.order_date                           AS     full_date,
                EXTRACT(QUARTER FROM src.order_date)     AS     date_quarter,
                EXTRACT(MONTH   FROM src.order_date)     AS     date_month,
                EXTRACT(WEEK    FROM src.order_date)     AS     date_week,
                EXTRACT(DAY     FROM src.order_date)     AS     date_day,
                EXTRACT(YEAR    FROM src.order_date)     AS     date_year
        FROM nessie.gold.amazon_orders_silver AS src
    )
    SELECT
        ROW_NUMBER() OVER (ORDER BY full_date) AS id,
        src.*
    FROM 
        new_rows AS src' with 39e1e309-2f6a-4daa-9303-51fd58522002
24/09/14 17:04:12 INFO SparkExecuteStatementOperation: Running query with 39e1e309-2f6a-4daa-9303-51fd58522002
24/09/14 17:04:12 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:04:12 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:12 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:12 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:04:12 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:04:12 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:04:12 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:04:12 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json
24/09/14 17:04:12 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=0aa345600910f01de9d2c27caf2ebfaac9765429dbf86775e32eee30352f86b4}'
24/09/14 17:04:12 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/14 17:04:12 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/14 17:04:12 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/14 17:04:12 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter true
24/09/14 17:04:13 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=nessie.silver.amazon_orders, snapshotId=6957663081544740559, filter=true, schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], projectedFieldNames=[Order_ID, Order_Date, Order_Status, Fulfilment, ORDERS_Channel, ship_service_level, Category, Size, Courier_Status, Qty, Currency, Amount, Ship_City, Ship_State, Ship_Postal_Code, Ship_Country, B2B, Fulfilled_By, New, PendingS, Ingestion_Date], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.016041337S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=7309}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1726332744679, engine-name=spark}}
24/09/14 17:04:13 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Aggregate Functions:
 MAX(Ingestion_Date)
Pushed Group by:
 
         
24/09/14 17:04:13 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Filters: Ingestion_Date IS NOT NULL
Post-Scan Filters: isnotnull(Ingestion_Date#745),(Ingestion_Date#745 = scalar-subquery#681 [])
         
24/09/14 17:04:13 INFO V2ScanRelationPushDown: 
Output: Order_Date#726, Ingestion_Date#745
         
24/09/14 17:04:13 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter Ingestion_Date IS NOT NULL
24/09/14 17:04:13 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/14 17:04:13 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/14 17:04:13 INFO SparkWrite: Requesting 402653184 bytes advisory partition size for table gold.date_dim
24/09/14 17:04:13 INFO SparkWrite: Requesting ClusteredDistribution(months(full_date)) as write distribution for table gold.date_dim
24/09/14 17:04:13 INFO SparkWrite: Requesting [] as write ordering for table gold.date_dim
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.3 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:04:13 INFO SparkContext: Created broadcast 24 from broadcast at SparkBatch.java:79
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:04:13 INFO SparkContext: Created broadcast 25 from broadcast at SparkBatch.java:79
24/09/14 17:04:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:13 INFO DAGScheduler: Registering RDD 53 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) as input to shuffle 9
24/09/14 17:04:13 INFO DAGScheduler: Got map stage job 15 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:04:13 INFO DAGScheduler: Final stage: ShuffleMapStage 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:04:13 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:04:13 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:13 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.2 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 459adcdce09e:46053 (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:04:13 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[53] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:13 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
24/09/14 17:04:13 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 15) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 9763 bytes) 
24/09/14 17:04:13 INFO Executor: Running task 0.0 in stage 27.0 (TID 15)
24/09/14 17:04:13 INFO Executor: Finished task 0.0 in stage 27.0 (TID 15). 1842 bytes result sent to driver
24/09/14 17:04:13 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 15) in 7 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:13 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
24/09/14 17:04:13 INFO DAGScheduler: ShuffleMapStage 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.012 s
24/09/14 17:04:13 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:13 INFO DAGScheduler: running: Set()
24/09/14 17:04:13 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:13 INFO DAGScheduler: failed: Set()
24/09/14 17:04:13 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:04:13 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:04:13 INFO DAGScheduler: Final stage: ResultStage 29 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:04:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
24/09/14 17:04:13 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:13 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 13.0 KiB, free 434.2 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.2 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 459adcdce09e:46053 (size: 6.1 KiB, free: 434.4 MiB)
24/09/14 17:04:13 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[56] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:13 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
24/09/14 17:04:13 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 16) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:04:13 INFO Executor: Running task 0.0 in stage 29.0 (TID 16)
24/09/14 17:04:13 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:04:13 INFO Executor: Finished task 0.0 in stage 29.0 (TID 16). 3959 bytes result sent to driver
24/09/14 17:04:13 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 16) in 6 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:13 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
24/09/14 17:04:13 INFO DAGScheduler: ResultStage 29 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.011 s
24/09/14 17:04:13 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:04:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
24/09/14 17:04:13 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.014448 s
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 27.143189 ms
24/09/14 17:04:13 INFO DAGScheduler: Registering RDD 60 (run at AccessController.java:0) as input to shuffle 10
24/09/14 17:04:13 INFO DAGScheduler: Got map stage job 17 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:13 INFO DAGScheduler: Final stage: ShuffleMapStage 30 (run at AccessController.java:0)
24/09/14 17:04:13 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:04:13 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:13 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[60] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 59.1 KiB, free 434.1 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 24.4 KiB, free 434.1 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 459adcdce09e:46053 (size: 24.4 KiB, free: 434.3 MiB)
24/09/14 17:04:13 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[60] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:13 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
24/09/14 17:04:13 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 17) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 16448 bytes) 
24/09/14 17:04:13 INFO Executor: Running task 0.0 in stage 30.0 (TID 17)
24/09/14 17:04:13 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 459adcdce09e:46053 in memory (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 459adcdce09e:46053 in memory (size: 6.1 KiB, free: 434.4 MiB)
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 42.796735 ms
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 9.656305 ms
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 5.48876 ms
24/09/14 17:04:13 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:04:13 INFO Executor: Finished task 0.0 in stage 30.0 (TID 17). 8215 bytes result sent to driver
24/09/14 17:04:13 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 17) in 147 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:13 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
24/09/14 17:04:13 INFO DAGScheduler: ShuffleMapStage 30 (run at AccessController.java:0) finished in 0.154 s
24/09/14 17:04:13 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:13 INFO DAGScheduler: running: Set()
24/09/14 17:04:13 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:13 INFO DAGScheduler: failed: Set()
24/09/14 17:04:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:13 INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 26.437634 ms
24/09/14 17:04:13 INFO DAGScheduler: Registering RDD 63 (run at AccessController.java:0) as input to shuffle 11
24/09/14 17:04:13 INFO DAGScheduler: Got map stage job 18 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:13 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (run at AccessController.java:0)
24/09/14 17:04:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
24/09/14 17:04:13 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:13 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[63] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 61.3 KiB, free 434.1 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 434.1 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 459adcdce09e:46053 (size: 25.4 KiB, free: 434.3 MiB)
24/09/14 17:04:13 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[63] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:13 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
24/09/14 17:04:13 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 18) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/14 17:04:13 INFO Executor: Running task 0.0 in stage 32.0 (TID 18)
24/09/14 17:04:13 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 32.242073 ms
24/09/14 17:04:13 INFO Executor: Finished task 0.0 in stage 32.0 (TID 18). 10196 bytes result sent to driver
24/09/14 17:04:13 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 18) in 55 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:13 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
24/09/14 17:04:13 INFO DAGScheduler: ShuffleMapStage 32 (run at AccessController.java:0) finished in 0.069 s
24/09/14 17:04:13 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:13 INFO DAGScheduler: running: Set()
24/09/14 17:04:13 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:13 INFO DAGScheduler: failed: Set()
24/09/14 17:04:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:13 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 11.911603 ms
24/09/14 17:04:13 INFO DAGScheduler: Registering RDD 68 (run at AccessController.java:0) as input to shuffle 12
24/09/14 17:04:13 INFO DAGScheduler: Got map stage job 19 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:13 INFO DAGScheduler: Final stage: ShuffleMapStage 35 (run at AccessController.java:0)
24/09/14 17:04:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
24/09/14 17:04:13 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:13 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[68] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:13 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 61.7 KiB, free 434.1 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 26.0 KiB, free 434.0 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 459adcdce09e:46053 (size: 26.0 KiB, free: 434.3 MiB)
24/09/14 17:04:13 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[68] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:13 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
24/09/14 17:04:13 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 19) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/14 17:04:13 INFO Executor: Running task 0.0 in stage 35.0 (TID 19)
24/09/14 17:04:13 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 7.596673 ms
24/09/14 17:04:13 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 459adcdce09e:46053 in memory (size: 3.1 KiB, free: 434.3 MiB)
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 4.14824 ms
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 5.980578 ms
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 5.286093 ms
24/09/14 17:04:13 INFO CodeGenerator: Code generated in 8.737932 ms
24/09/14 17:04:13 INFO Executor: Finished task 0.0 in stage 35.0 (TID 19). 11979 bytes result sent to driver
24/09/14 17:04:13 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 19) in 84 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:13 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
24/09/14 17:04:13 INFO DAGScheduler: ShuffleMapStage 35 (run at AccessController.java:0) finished in 0.102 s
24/09/14 17:04:13 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:13 INFO DAGScheduler: running: Set()
24/09/14 17:04:13 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:13 INFO DAGScheduler: failed: Set()
24/09/14 17:04:13 INFO ShufflePartitionsUtil: For shuffle(12), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.0 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 459adcdce09e:46053 (size: 3.2 KiB, free: 434.3 MiB)
24/09/14 17:04:13 INFO SparkContext: Created broadcast 31 from broadcast at SparkWrite.java:193
24/09/14 17:04:13 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.date_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/14 17:04:13 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/14 17:04:13 INFO DAGScheduler: Got job 20 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:13 INFO DAGScheduler: Final stage: ResultStage 39 (run at AccessController.java:0)
24/09/14 17:04:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)
24/09/14 17:04:13 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:13 INFO DAGScheduler: Submitting ResultStage 39 (ShuffledRowRDD[69] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 10.5 KiB, free 434.0 MiB)
24/09/14 17:04:13 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.0 MiB)
24/09/14 17:04:13 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 459adcdce09e:46053 (size: 5.6 KiB, free: 434.3 MiB)
24/09/14 17:04:13 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (ShuffledRowRDD[69] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:13 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
24/09/14 17:04:13 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 20) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:04:13 INFO Executor: Running task 0.0 in stage 39.0 (TID 20)
24/09/14 17:04:13 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:13 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:04:13 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:13 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/14 17:04:15 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 39.0)
24/09/14 17:04:15 INFO Executor: Finished task 0.0 in stage 39.0 (TID 20). 6812 bytes result sent to driver
24/09/14 17:04:15 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 20) in 1299 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:15 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
24/09/14 17:04:15 INFO DAGScheduler: ResultStage 39 (run at AccessController.java:0) finished in 1.304 s
24/09/14 17:04:15 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:04:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
24/09/14 17:04:15 INFO DAGScheduler: Job 20 finished: run at AccessController.java:0, took 1.309376 s
24/09/14 17:04:15 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.date_dim, format=PARQUET) is committing.
24/09/14 17:04:15 INFO SparkWrite: Committing append with 1 new data files to table gold.date_dim
24/09/14 17:04:15 INFO SnapshotProducer: Committed snapshot 7220629113088947840 (MergeAppend)
24/09/14 17:04:15 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.date_dim, snapshotId=7220629113088947840, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.068875937S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=1}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=1835}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=1835}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1726332744679, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/14 17:04:15 INFO SparkWrite: Committed in 69 ms
24/09/14 17:04:15 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.date_dim, format=PARQUET) committed.
24/09/14 17:04:15 INFO NessieIcebergClient: Committed 'gold.date_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=49a13adefa31afeb2302c4c4b3f52742d7b32783bfd4acef5a52c34ff8b35a03}', expected commit-id was '0aa345600910f01de9d2c27caf2ebfaac9765429dbf86775e32eee30352f86b4'
24/09/14 17:04:15 INFO BaseMetastoreTableOperations: Successfully committed to table gold.date_dim in 25 ms
24/09/14 17:04:15 INFO DAGScheduler: Asked to cancel job group 39e1e309-2f6a-4daa-9303-51fd58522002
24/09/14 17:04:15 INFO SparkExecuteStatementOperation: Close statement with 39e1e309-2f6a-4daa-9303-51fd58522002
24/09/14 17:04:15 INFO DAGScheduler: Asked to cancel job group 3bb54de8-8b63-4694-bfe7-ced6035abe9d
24/09/14 17:04:15 INFO SparkExecuteStatementOperation: Close statement with 3bb54de8-8b63-4694-bfe7-ced6035abe9d
24/09/14 17:04:15 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:15 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:15 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:15 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:15 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:15 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:15 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:15 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:15 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:15 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/ad24c14b-c788-4e2c-99a5-c56c0af2b82f
24/09/14 17:04:15 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with ee45e915-f602-481b-994a-20e6ff9a163a
24/09/14 17:04:15 INFO SparkExecuteStatementOperation: Running query with ee45e915-f602-481b-994a-20e6ff9a163a
24/09/14 17:04:15 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:15 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:15 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:15 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:15 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:15 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:15 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:15 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:15 INFO DAGScheduler: Asked to cancel job group ee45e915-f602-481b-994a-20e6ff9a163a
24/09/14 17:04:15 INFO SparkExecuteStatementOperation: Close statement with ee45e915-f602-481b-994a-20e6ff9a163a
24/09/14 17:04:15 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:15 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:15 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:15 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:15 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:15 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:15 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:15 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:16 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 459adcdce09e:46053 in memory (size: 26.0 KiB, free: 434.3 MiB)
24/09/14 17:04:16 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 459adcdce09e:46053 in memory (size: 3.2 KiB, free: 434.3 MiB)
24/09/14 17:04:16 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 459adcdce09e:46053 in memory (size: 5.6 KiB, free: 434.3 MiB)
24/09/14 17:04:16 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 459adcdce09e:46053 in memory (size: 25.4 KiB, free: 434.4 MiB)
24/09/14 17:04:22 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:22 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/292765bd-2706-40dc-a23a-70a8b1594df6
24/09/14 17:04:22 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 36cf3ae7-3b3c-4d33-b611-32da0ae6c61c
24/09/14 17:04:22 INFO SparkExecuteStatementOperation: Running query with 36cf3ae7-3b3c-4d33-b611-32da0ae6c61c
24/09/14 17:04:22 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:22 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:22 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:22 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:22 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:22 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:22 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:22 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:22 INFO DAGScheduler: Asked to cancel job group 36cf3ae7-3b3c-4d33-b611-32da0ae6c61c
24/09/14 17:04:22 INFO SparkExecuteStatementOperation: Close statement with 36cf3ae7-3b3c-4d33-b611-32da0ae6c61c
24/09/14 17:04:22 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 9836a801-918b-4104-9dd1-040cb6da5fcc
24/09/14 17:04:22 INFO SparkExecuteStatementOperation: Running query with 9836a801-918b-4104-9dd1-040cb6da5fcc
24/09/14 17:04:22 INFO HiveMetaStore: 11: get_databases: *
24/09/14 17:04:22 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/14 17:04:22 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:22 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:22 INFO HiveMetaStore: 11: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:22 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:22 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:22 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/14 17:04:23 INFO DAGScheduler: Asked to cancel job group 9836a801-918b-4104-9dd1-040cb6da5fcc
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Close statement with 9836a801-918b-4104-9dd1-040cb6da5fcc
24/09/14 17:04:23 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:23 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:23 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:23 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:23 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:23 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:23 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:23 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:23 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/b23927a5-320d-407b-a587-c08da054034d
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 18dea966-2059-4db4-a710-9bee48c9e171
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Running query with 18dea966-2059-4db4-a710-9bee48c9e171
24/09/14 17:04:23 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:23 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:23 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:23 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:23 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:23 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:23 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:23 INFO DAGScheduler: Asked to cancel job group 18dea966-2059-4db4-a710-9bee48c9e171
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Close statement with 18dea966-2059-4db4-a710-9bee48c9e171
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with 408cae0b-a723-420b-b2d7-6b5bd7cfb7db
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Running query with 408cae0b-a723-420b-b2d7-6b5bd7cfb7db
24/09/14 17:04:23 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:04:23 INFO DAGScheduler: Asked to cancel job group 408cae0b-a723-420b-b2d7-6b5bd7cfb7db
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Close statement with 408cae0b-a723-420b-b2d7-6b5bd7cfb7db
24/09/14 17:04:23 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:23 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:23 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:23 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:23 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:23 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:23 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:23 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:23 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/df84d1d9-c067-4211-bd87-0ba610220836
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with b10d7afc-aca1-4b9c-9db1-18f645fe6b8d
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Running query with b10d7afc-aca1-4b9c-9db1-18f645fe6b8d
24/09/14 17:04:23 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:23 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:23 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:23 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:23 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:23 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:23 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:23 INFO DAGScheduler: Asked to cancel job group b10d7afc-aca1-4b9c-9db1-18f645fe6b8d
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Close statement with b10d7afc-aca1-4b9c-9db1-18f645fe6b8d
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with bfa83c90-1893-4aa8-9780-2d2ef751b4fa
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Running query with bfa83c90-1893-4aa8-9780-2d2ef751b4fa
24/09/14 17:04:23 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:04:23 INFO DAGScheduler: Asked to cancel job group bfa83c90-1893-4aa8-9780-2d2ef751b4fa
24/09/14 17:04:23 ERROR SparkExecuteStatementOperation: Error executing query with bfa83c90-1893-4aa8-9780-2d2ef751b4fa, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#787, tableName#788, isTemporary#789, information#790]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@11664f6, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with ea572fb4-0560-4510-a9d4-6bbc8bdc20e3
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Running query with ea572fb4-0560-4510-a9d4-6bbc8bdc20e3
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/14 17:04:23 INFO DAGScheduler: Asked to cancel job group ea572fb4-0560-4510-a9d4-6bbc8bdc20e3
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Close statement with ea572fb4-0560-4510-a9d4-6bbc8bdc20e3
24/09/14 17:04:23 INFO DAGScheduler: Asked to cancel job group bfa83c90-1893-4aa8-9780-2d2ef751b4fa
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Close statement with bfa83c90-1893-4aa8-9780-2d2ef751b4fa
24/09/14 17:04:23 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:23 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:23 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:23 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:23 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:23 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:23 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:23 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:23 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/c0ca4595-66f7-4d8d-b88b-c7b1804f738d
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with a9e8b893-940e-41e5-b7c6-21c076dc7480
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Running query with a9e8b893-940e-41e5-b7c6-21c076dc7480
24/09/14 17:04:23 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:23 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:23 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:23 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:23 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:23 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:23 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:23 INFO DAGScheduler: Asked to cancel job group a9e8b893-940e-41e5-b7c6-21c076dc7480
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Close statement with a9e8b893-940e-41e5-b7c6-21c076dc7480
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.location_dim"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240914165817
      ' with 9ff94c14-0821-4745-8145-50bdae59d57f
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Running query with 9ff94c14-0821-4745-8145-50bdae59d57f
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.location_dim"} */

  
    
        create or replace table nessie.gold.location_dim
      
      
    using iceberg
      
      
      partitioned by (ship_country,ship_state)
      
      
      

      as
      







    -- Extract source and target columns from the column mapping dictionary
    
    

    
        
        -- Initial load: Create the table and load all rows
        WITH new_rows AS (
            SELECT DISTINCT
            
                src.ship_country AS ship_country , 
            
                src.ship_state AS ship_state , 
            
                src.ship_city AS ship_city , 
            
                src.ship_postal_code AS ship_postal_code
            
        FROM nessie.gold.amazon_orders_silver AS src
        )

        SELECT
            ROW_NUMBER() OVER (ORDER BY btch.ship_country, btch.ship_state, btch.ship_city, btch.ship_postal_code) AS id,
            btch.*
        FROM new_rows AS btch
        
    

  ' with 1ba25a78-25cb-4687-a435-fe868e2fb6a2
24/09/14 17:04:23 INFO SparkExecuteStatementOperation: Running query with 1ba25a78-25cb-4687-a435-fe868e2fb6a2
24/09/14 17:04:23 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:04:23 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:04:23 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:04:23 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:04:23 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:04:23 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json
24/09/14 17:04:23 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=49a13adefa31afeb2302c4c4b3f52742d7b32783bfd4acef5a52c34ff8b35a03}'
24/09/14 17:04:23 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/14 17:04:23 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/14 17:04:23 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/14 17:04:23 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter true
24/09/14 17:04:23 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=nessie.silver.amazon_orders, snapshotId=6957663081544740559, filter=true, schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], projectedFieldNames=[Order_ID, Order_Date, Order_Status, Fulfilment, ORDERS_Channel, ship_service_level, Category, Size, Courier_Status, Qty, Currency, Amount, Ship_City, Ship_State, Ship_Postal_Code, Ship_Country, B2B, Fulfilled_By, New, PendingS, Ingestion_Date], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.014803759S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=7309}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1726332744679, engine-name=spark}}
24/09/14 17:04:23 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Aggregate Functions:
 MAX(Ingestion_Date)
Pushed Group by:
 
         
24/09/14 17:04:23 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Filters: Ingestion_Date IS NOT NULL
Post-Scan Filters: isnotnull(Ingestion_Date#906),(Ingestion_Date#906 = scalar-subquery#842 [])
         
24/09/14 17:04:23 INFO V2ScanRelationPushDown: 
Output: Ship_City#898, Ship_State#899, Ship_Postal_Code#900, Ship_Country#901, Ingestion_Date#906
         
24/09/14 17:04:23 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter Ingestion_Date IS NOT NULL
24/09/14 17:04:23 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/14 17:04:23 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/14 17:04:23 INFO SparkWrite: Requesting 402653184 bytes advisory partition size for table gold.location_dim
24/09/14 17:04:23 INFO SparkWrite: Requesting ClusteredDistribution(ship_country, ship_state) as write distribution for table gold.location_dim
24/09/14 17:04:23 INFO SparkWrite: Requesting [] as write ordering for table gold.location_dim
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)
24/09/14 17:04:23 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:04:23 INFO SparkContext: Created broadcast 33 from broadcast at SparkBatch.java:79
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)
24/09/14 17:04:23 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:04:23 INFO SparkContext: Created broadcast 34 from broadcast at SparkBatch.java:79
24/09/14 17:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:23 INFO DAGScheduler: Registering RDD 72 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) as input to shuffle 13
24/09/14 17:04:23 INFO DAGScheduler: Got map stage job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:04:23 INFO DAGScheduler: Final stage: ShuffleMapStage 40 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:04:23 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:04:23 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:23 INFO DAGScheduler: Submitting ShuffleMapStage 40 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.2 MiB)
24/09/14 17:04:23 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 459adcdce09e:46053 (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:04:23 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:23 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
24/09/14 17:04:23 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 21) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 9763 bytes) 
24/09/14 17:04:23 INFO Executor: Running task 0.0 in stage 40.0 (TID 21)
24/09/14 17:04:23 INFO Executor: Finished task 0.0 in stage 40.0 (TID 21). 1842 bytes result sent to driver
24/09/14 17:04:23 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 21) in 6 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:23 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
24/09/14 17:04:23 INFO DAGScheduler: ShuffleMapStage 40 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.010 s
24/09/14 17:04:23 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:23 INFO DAGScheduler: running: Set()
24/09/14 17:04:23 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:23 INFO DAGScheduler: failed: Set()
24/09/14 17:04:23 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:04:23 INFO DAGScheduler: Got job 22 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:04:23 INFO DAGScheduler: Final stage: ResultStage 42 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:04:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
24/09/14 17:04:23 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:23 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[75] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 13.0 KiB, free 434.1 MiB)
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 434.1 MiB)
24/09/14 17:04:23 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 459adcdce09e:46053 (size: 6.1 KiB, free: 434.3 MiB)
24/09/14 17:04:23 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[75] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:23 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
24/09/14 17:04:23 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 22) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:04:23 INFO Executor: Running task 0.0 in stage 42.0 (TID 22)
24/09/14 17:04:23 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:04:23 INFO Executor: Finished task 0.0 in stage 42.0 (TID 22). 3959 bytes result sent to driver
24/09/14 17:04:23 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 22) in 6 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:23 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
24/09/14 17:04:23 INFO DAGScheduler: ResultStage 42 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.010 s
24/09/14 17:04:23 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:04:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
24/09/14 17:04:23 INFO DAGScheduler: Job 22 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.014910 s
24/09/14 17:04:23 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 459adcdce09e:46053 in memory (size: 6.1 KiB, free: 434.4 MiB)
24/09/14 17:04:23 INFO CodeGenerator: Code generated in 42.613232 ms
24/09/14 17:04:23 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 459adcdce09e:46053 in memory (size: 6.0 KiB, free: 434.4 MiB)
24/09/14 17:04:23 INFO DAGScheduler: Registering RDD 79 (run at AccessController.java:0) as input to shuffle 14
24/09/14 17:04:23 INFO DAGScheduler: Got map stage job 23 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:23 INFO DAGScheduler: Final stage: ShuffleMapStage 43 (run at AccessController.java:0)
24/09/14 17:04:23 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:04:23 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:23 INFO DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[79] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 57.0 KiB, free 434.1 MiB)
24/09/14 17:04:23 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 24.0 KiB, free 434.1 MiB)
24/09/14 17:04:23 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 459adcdce09e:46053 (size: 24.0 KiB, free: 434.3 MiB)
24/09/14 17:04:23 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[79] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:23 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
24/09/14 17:04:23 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 23) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 16644 bytes) 
24/09/14 17:04:23 INFO Executor: Running task 0.0 in stage 43.0 (TID 23)
24/09/14 17:04:23 INFO CodeGenerator: Code generated in 29.042053 ms
24/09/14 17:04:23 INFO CodeGenerator: Code generated in 9.809732 ms
24/09/14 17:04:23 INFO CodeGenerator: Code generated in 6.384652 ms
24/09/14 17:04:23 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:04:24 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:24 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:24 INFO Executor: Finished task 0.0 in stage 43.0 (TID 23). 8129 bytes result sent to driver
24/09/14 17:04:24 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 23) in 162 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:24 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
24/09/14 17:04:24 INFO DAGScheduler: ShuffleMapStage 43 (run at AccessController.java:0) finished in 0.168 s
24/09/14 17:04:24 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:24 INFO DAGScheduler: running: Set()
24/09/14 17:04:24 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:24 INFO DAGScheduler: failed: Set()
24/09/14 17:04:24 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:24 INFO ShufflePartitionsUtil: For shuffle(14), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/09/14 17:04:24 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:04:24 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 459adcdce09e:46053 in memory (size: 24.4 KiB, free: 434.4 MiB)
24/09/14 17:04:24 INFO CodeGenerator: Code generated in 29.524847 ms
24/09/14 17:04:24 INFO DAGScheduler: Registering RDD 82 (run at AccessController.java:0) as input to shuffle 15
24/09/14 17:04:24 INFO DAGScheduler: Got map stage job 24 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:24 INFO DAGScheduler: Final stage: ShuffleMapStage 45 (run at AccessController.java:0)
24/09/14 17:04:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
24/09/14 17:04:24 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:24 INFO DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[82] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:24 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 57.9 KiB, free 434.2 MiB)
24/09/14 17:04:24 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 24.7 KiB, free 434.2 MiB)
24/09/14 17:04:24 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 459adcdce09e:46053 (size: 24.7 KiB, free: 434.3 MiB)
24/09/14 17:04:24 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[82] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:24 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
24/09/14 17:04:24 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 24) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/14 17:04:24 INFO Executor: Running task 0.0 in stage 45.0 (TID 24)
24/09/14 17:04:24 INFO ShuffleBlockFetcherIterator: Getting 1 (2.0 KiB) non-empty blocks including 1 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:04:24 INFO CodeGenerator: Code generated in 13.42097 ms
24/09/14 17:04:24 INFO Executor: Finished task 0.0 in stage 45.0 (TID 24). 10196 bytes result sent to driver
24/09/14 17:04:24 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 24) in 30 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:24 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
24/09/14 17:04:24 INFO DAGScheduler: ShuffleMapStage 45 (run at AccessController.java:0) finished in 0.037 s
24/09/14 17:04:24 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:24 INFO DAGScheduler: running: Set()
24/09/14 17:04:24 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:24 INFO DAGScheduler: failed: Set()
24/09/14 17:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
24/09/14 17:04:24 INFO CodeGenerator: Code generated in 4.721078 ms
24/09/14 17:04:24 INFO DAGScheduler: Registering RDD 87 (run at AccessController.java:0) as input to shuffle 16
24/09/14 17:04:24 INFO DAGScheduler: Got map stage job 25 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:24 INFO DAGScheduler: Final stage: ShuffleMapStage 48 (run at AccessController.java:0)
24/09/14 17:04:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 47)
24/09/14 17:04:24 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:24 INFO DAGScheduler: Submitting ShuffleMapStage 48 (MapPartitionsRDD[87] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:24 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 58.0 KiB, free 434.1 MiB)
24/09/14 17:04:24 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 25.0 KiB, free 434.1 MiB)
24/09/14 17:04:24 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 459adcdce09e:46053 (size: 25.0 KiB, free: 434.3 MiB)
24/09/14 17:04:24 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 48 (MapPartitionsRDD[87] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:24 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0
24/09/14 17:04:24 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 25) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9591 bytes) 
24/09/14 17:04:24 INFO Executor: Running task 0.0 in stage 48.0 (TID 25)
24/09/14 17:04:24 INFO ShuffleBlockFetcherIterator: Getting 1 (868.0 B) non-empty blocks including 1 (868.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:04:24 INFO CodeGenerator: Code generated in 5.264511 ms
24/09/14 17:04:24 INFO CodeGenerator: Code generated in 4.033816 ms
24/09/14 17:04:24 INFO CodeGenerator: Code generated in 4.165302 ms
24/09/14 17:04:24 INFO CodeGenerator: Code generated in 5.186687 ms
24/09/14 17:04:24 INFO Executor: Finished task 0.0 in stage 48.0 (TID 25). 11979 bytes result sent to driver
24/09/14 17:04:24 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 25) in 48 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:24 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
24/09/14 17:04:24 INFO DAGScheduler: ShuffleMapStage 48 (run at AccessController.java:0) finished in 0.057 s
24/09/14 17:04:24 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:04:24 INFO DAGScheduler: running: Set()
24/09/14 17:04:24 INFO DAGScheduler: waiting: Set()
24/09/14 17:04:24 INFO DAGScheduler: failed: Set()
24/09/14 17:04:24 INFO ShufflePartitionsUtil: For shuffle(16), advisory target size: 402653184, actual target size 1048576, minimum partition size: 1048576
24/09/14 17:04:24 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/14 17:04:24 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 434.1 MiB)
24/09/14 17:04:24 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 459adcdce09e:46053 (size: 3.2 KiB, free: 434.3 MiB)
24/09/14 17:04:24 INFO SparkContext: Created broadcast 40 from broadcast at SparkWrite.java:193
24/09/14 17:04:24 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.location_dim, format=PARQUET). The input RDD has 1 partitions.
24/09/14 17:04:24 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/14 17:04:24 INFO DAGScheduler: Got job 26 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:04:24 INFO DAGScheduler: Final stage: ResultStage 52 (run at AccessController.java:0)
24/09/14 17:04:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 51)
24/09/14 17:04:24 INFO DAGScheduler: Missing parents: List()
24/09/14 17:04:24 INFO DAGScheduler: Submitting ResultStage 52 (ShuffledRowRDD[88] at run at AccessController.java:0), which has no missing parents
24/09/14 17:04:24 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 10.5 KiB, free 434.0 MiB)
24/09/14 17:04:24 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.0 MiB)
24/09/14 17:04:24 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 459adcdce09e:46053 (size: 5.6 KiB, free: 434.3 MiB)
24/09/14 17:04:24 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1585
24/09/14 17:04:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (ShuffledRowRDD[88] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:04:24 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0
24/09/14 17:04:24 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 26) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:04:24 INFO Executor: Running task 0.0 in stage 52.0 (TID 26)
24/09/14 17:04:24 INFO ShuffleBlockFetcherIterator: Getting 1 (1884.0 B) non-empty blocks including 1 (1884.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:04:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 459adcdce09e:46053 in memory (size: 24.7 KiB, free: 434.3 MiB)
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 459adcdce09e:46053 in memory (size: 25.0 KiB, free: 434.4 MiB)
24/09/14 17:04:24 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 459adcdce09e:46053 in memory (size: 24.0 KiB, free: 434.4 MiB)
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:04:24 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/14 17:04:25 INFO DataWritingSparkTask: Committed partition 0 (task 26, attempt 0, stage 52.0)
24/09/14 17:04:25 INFO Executor: Finished task 0.0 in stage 52.0 (TID 26). 21492 bytes result sent to driver
24/09/14 17:04:25 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 26) in 1228 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:04:25 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
24/09/14 17:04:25 INFO DAGScheduler: ResultStage 52 (run at AccessController.java:0) finished in 1.233 s
24/09/14 17:04:25 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:04:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
24/09/14 17:04:25 INFO DAGScheduler: Job 26 finished: run at AccessController.java:0, took 1.238489 s
24/09/14 17:04:25 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.location_dim, format=PARQUET) is committing.
24/09/14 17:04:25 INFO SparkWrite: Committing append with 15 new data files to table gold.location_dim
24/09/14 17:04:25 INFO SnapshotProducer: Committed snapshot 2603986048269305278 (MergeAppend)
24/09/14 17:04:25 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.location_dim, snapshotId=2603986048269305278, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.084364864S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=15}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=15}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=19}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=19}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=22041}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=22041}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1726332744679, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/14 17:04:25 INFO SparkWrite: Committed in 84 ms
24/09/14 17:04:25 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.location_dim, format=PARQUET) committed.
24/09/14 17:04:25 INFO NessieIcebergClient: Committed 'gold.location_dim' against 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=c50164e1f5a5e898a551f7524d571b9fbe0772afb87c2501a057e11026e855a3}', expected commit-id was '49a13adefa31afeb2302c4c4b3f52742d7b32783bfd4acef5a52c34ff8b35a03'
24/09/14 17:04:25 INFO BaseMetastoreTableOperations: Successfully committed to table gold.location_dim in 29 ms
24/09/14 17:04:25 INFO DAGScheduler: Asked to cancel job group 1ba25a78-25cb-4687-a435-fe868e2fb6a2
24/09/14 17:04:25 INFO SparkExecuteStatementOperation: Close statement with 1ba25a78-25cb-4687-a435-fe868e2fb6a2
24/09/14 17:04:25 INFO DAGScheduler: Asked to cancel job group 9ff94c14-0821-4745-8145-50bdae59d57f
24/09/14 17:04:25 INFO SparkExecuteStatementOperation: Close statement with 9ff94c14-0821-4745-8145-50bdae59d57f
24/09/14 17:04:25 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:25 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:25 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:25 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:25 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:25 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:25 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:25 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:04:25 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:04:25 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/67e91025-3eaa-48b7-b94a-3274d7e19b86
24/09/14 17:04:25 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 8a0bace7-e764-4804-9ebd-71a52d5db910
24/09/14 17:04:25 INFO SparkExecuteStatementOperation: Running query with 8a0bace7-e764-4804-9ebd-71a52d5db910
24/09/14 17:04:25 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:04:25 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:04:25 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:04:25 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:04:25 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:04:25 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:25 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:25 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:25 INFO DAGScheduler: Asked to cancel job group 8a0bace7-e764-4804-9ebd-71a52d5db910
24/09/14 17:04:25 INFO SparkExecuteStatementOperation: Close statement with 8a0bace7-e764-4804-9ebd-71a52d5db910
24/09/14 17:04:25 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:04:25 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:04:25 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:04:25 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:04:25 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:04:25 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:04:25 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:04:25 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:05:10 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:05:10 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/839878eb-2e4a-4adb-8927-76fe019ee8ac
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 0136e8bd-5393-4579-854e-a03d68592beb
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with 0136e8bd-5393-4579-854e-a03d68592beb
24/09/14 17:05:10 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:05:10 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:05:10 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:10 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:10 INFO DAGScheduler: Asked to cancel job group 0136e8bd-5393-4579-854e-a03d68592beb
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Close statement with 0136e8bd-5393-4579-854e-a03d68592beb
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_schemas"} */

    show databases
  ' with 35f3de00-e5b9-4383-805c-c468d7c2f944
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with 35f3de00-e5b9-4383-805c-c468d7c2f944
24/09/14 17:05:10 INFO HiveMetaStore: 12: get_databases: *
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_databases: *	
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:05:10 INFO HiveMetaStore: 12: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:05:10 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:10 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING>
24/09/14 17:05:10 INFO DAGScheduler: Asked to cancel job group 35f3de00-e5b9-4383-805c-c468d7c2f944
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Close statement with 35f3de00-e5b9-4383-805c-c468d7c2f944
24/09/14 17:05:10 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:10 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:10 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:05:10 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:05:10 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:05:10 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:05:10 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/8fb50316-828c-454b-bf8b-3292983717c7
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 6083688b-718e-4497-8476-48736f8b17e7
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with 6083688b-718e-4497-8476-48736f8b17e7
24/09/14 17:05:10 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:05:10 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:05:10 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:10 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:10 INFO DAGScheduler: Asked to cancel job group 6083688b-718e-4497-8476-48736f8b17e7
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Close statement with 6083688b-718e-4497-8476-48736f8b17e7
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "create__nessie.gold"} */
create schema if not exists nessie.gold
  ' with a2ef19ee-19f9-45b0-bd70-2f5608aa9ad8
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with a2ef19ee-19f9-45b0-bd70-2f5608aa9ad8
24/09/14 17:05:10 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:05:10 INFO DAGScheduler: Asked to cancel job group a2ef19ee-19f9-45b0-bd70-2f5608aa9ad8
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Close statement with a2ef19ee-19f9-45b0-bd70-2f5608aa9ad8
24/09/14 17:05:10 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:10 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:10 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:05:10 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:05:10 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:05:10 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:05:10 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/661d3c0d-12c3-48c8-8c76-facdeee48a93
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 49c73b17-5f6e-4d82-8a33-b0ead7c315b5
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with 49c73b17-5f6e-4d82-8a33-b0ead7c315b5
24/09/14 17:05:10 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:05:10 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:05:10 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:10 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:10 INFO DAGScheduler: Asked to cancel job group 49c73b17-5f6e-4d82-8a33-b0ead7c315b5
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Close statement with 49c73b17-5f6e-4d82-8a33-b0ead7c315b5
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show table extended in nessie.gold like '*'
  ' with f8cb9aa5-3c29-46f8-8a78-13053528dff2
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with f8cb9aa5-3c29-46f8-8a78-13053528dff2
24/09/14 17:05:10 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:05:10 INFO DAGScheduler: Asked to cancel job group f8cb9aa5-3c29-46f8-8a78-13053528dff2
24/09/14 17:05:10 ERROR SparkExecuteStatementOperation: Error executing query with f8cb9aa5-3c29-46f8-8a78-13053528dff2, currentState RUNNING, 
org.apache.spark.sql.AnalysisException: SHOW TABLE EXTENDED is not supported for v2 tables.;
ShowTableExtended *, [namespace#949, tableName#950, isTemporary#951, information#952]
+- ResolvedNamespace org.apache.iceberg.spark.SparkCatalog@1d4b0eb8, [gold]

	at org.apache.spark.sql.errors.QueryCompilationErrors$.commandUnsupportedInV2TableError(QueryCompilationErrors.scala:2093)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:269)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:202)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:225)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "connection_name": "list_None_nessie.gold"} */
show tables in nessie.gold like '*'
  ' with 1f08db75-9b3b-4267-ba1a-c036ce0eb843
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with 1f08db75-9b3b-4267-ba1a-c036ce0eb843
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Result Schema: STRUCT<namespace: STRING, tableName: STRING, isTemporary: BOOLEAN>
24/09/14 17:05:10 INFO DAGScheduler: Asked to cancel job group 1f08db75-9b3b-4267-ba1a-c036ce0eb843
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Close statement with 1f08db75-9b3b-4267-ba1a-c036ce0eb843
24/09/14 17:05:10 INFO DAGScheduler: Asked to cancel job group f8cb9aa5-3c29-46f8-8a78-13053528dff2
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Close statement with f8cb9aa5-3c29-46f8-8a78-13053528dff2
24/09/14 17:05:10 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:10 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:10 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:05:10 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:05:10 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:05:10 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:05:10 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/49e7a9e3-55ed-413f-84fb-0480f24885bb
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 06eb897d-4809-495e-bb37-fa309171ddc5
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with 06eb897d-4809-495e-bb37-fa309171ddc5
24/09/14 17:05:10 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:05:10 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:05:10 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:05:10 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:05:10 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:10 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:10 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:10 INFO DAGScheduler: Asked to cancel job group 06eb897d-4809-495e-bb37-fa309171ddc5
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Close statement with 06eb897d-4809-495e-bb37-fa309171ddc5
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.fact_amazon_orders"} */

        SET spark.sql.catalog.nessie.ref= amazon_pipeline_sampled_data_1_20240914165817
      ' with 55b71887-cea8-40ea-bdd9-b458df373665
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with 55b71887-cea8-40ea-bdd9-b458df373665
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Submitting query '/* {"app": "dbt", "dbt_version": "1.8.6", "profile_name": "amazon_orders", "target_name": "dev", "node_id": "model.amazon_orders.fact_amazon_orders"} */

  
    
        create or replace table nessie.gold.fact_amazon_orders
      
      
    using iceberg
      
      
      
      
      
      

      as
      

SELECT
    src.order_id          AS id,
    date_dim.id           AS date_id,
    curr_dim.id           AS currency_id,
    loc_dim.id            AS location_id,
    prod_dim.id           AS product_id,
    ship_dim.id           AS shipping_id,
    src.qty               AS quantity,
    ROUND(src.amount ,2)  AS amount

FROM
    nessie.gold.amazon_orders_silver as src
LEFT JOIN
    nessie.gold.date_dim as date_dim
ON
    src.order_date = date_dim.full_date
LEFT JOIN
    nessie.gold.currency_dim as curr_dim
ON
    src.currency = curr_dim.currency
LEFT JOIN
    nessie.gold.location_dim as loc_dim
ON
    src.ship_country = loc_dim.ship_country AND
    src.ship_state = loc_dim.ship_state AND
    src.ship_city = loc_dim.ship_city   AND
    src.Ship_Postal_Code = loc_dim.Ship_Postal_Code
LEFT JOIN
    nessie.gold.product_dim as prod_dim
ON
    src.category = prod_dim.category AND
    src.size = prod_dim.size
LEFT JOIN
    nessie.gold.shipping_dim as ship_dim
ON
    src.Order_Status        = ship_dim.shipping_status AND
    src.Fulfilment          = ship_dim.Fulfilment AND
    src.ship_service_level  = ship_dim.ship_service_level AND
    src.fulfilled_by        = ship_dim.fulfilled_by 

  ' with ea1cdca3-ab17-46cf-bc25-e5cb8cf24217
24/09/14 17:05:10 INFO SparkExecuteStatementOperation: Running query with ea1cdca3-ab17-46cf-bc25-e5cb8cf24217
24/09/14 17:05:10 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.aws.s3.S3FileIO
24/09/14 17:05:10 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/date_dim_fd7b12d0-4746-4a3d-98d2-160c34de551a/metadata/00000-4bf5a480-a0a7-4a66-9f70-df38ac7a61a3.metadata.json
24/09/14 17:05:10 INFO NessieUtil: loadTableMetadata for 'gold.date_dim' from location 's3://warehouse/gold/date_dim_fd7b12d0-4746-4a3d-98d2-160c34de551a/metadata/00000-4bf5a480-a0a7-4a66-9f70-df38ac7a61a3.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=c50164e1f5a5e898a551f7524d571b9fbe0772afb87c2501a057e11026e855a3}'
24/09/14 17:05:10 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.date_dim
24/09/14 17:05:10 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/currency_dim_d033912f-ecba-43c1-a349-006cf081c6fa/metadata/00000-ddade254-23fd-487c-a476-a08329d30016.metadata.json
24/09/14 17:05:10 INFO NessieUtil: loadTableMetadata for 'gold.currency_dim' from location 's3://warehouse/gold/currency_dim_d033912f-ecba-43c1-a349-006cf081c6fa/metadata/00000-ddade254-23fd-487c-a476-a08329d30016.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=c50164e1f5a5e898a551f7524d571b9fbe0772afb87c2501a057e11026e855a3}'
24/09/14 17:05:10 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.currency_dim
24/09/14 17:05:10 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/location_dim_8260b7b8-10ff-4ba4-9e93-8155e997deaa/metadata/00000-ef117725-78cf-47ae-8b32-20116c2c5ff9.metadata.json
24/09/14 17:05:10 INFO NessieUtil: loadTableMetadata for 'gold.location_dim' from location 's3://warehouse/gold/location_dim_8260b7b8-10ff-4ba4-9e93-8155e997deaa/metadata/00000-ef117725-78cf-47ae-8b32-20116c2c5ff9.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=c50164e1f5a5e898a551f7524d571b9fbe0772afb87c2501a057e11026e855a3}'
24/09/14 17:05:10 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.location_dim
24/09/14 17:05:10 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/product_dim_42ea235f-c6d9-4f37-8d78-686cf82ef00e/metadata/00000-465d93a2-7106-49a2-9771-2eff363be9c1.metadata.json
24/09/14 17:05:10 INFO NessieUtil: loadTableMetadata for 'gold.product_dim' from location 's3://warehouse/gold/product_dim_42ea235f-c6d9-4f37-8d78-686cf82ef00e/metadata/00000-465d93a2-7106-49a2-9771-2eff363be9c1.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=c50164e1f5a5e898a551f7524d571b9fbe0772afb87c2501a057e11026e855a3}'
24/09/14 17:05:10 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.product_dim
24/09/14 17:05:10 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/gold/shipping_dim_9a7ce33f-aae9-446a-bc63-4f11328734f1/metadata/00000-1a64cd01-00ad-46e9-b460-343e7c340e42.metadata.json
24/09/14 17:05:10 INFO NessieUtil: loadTableMetadata for 'gold.shipping_dim' from location 's3://warehouse/gold/shipping_dim_9a7ce33f-aae9-446a-bc63-4f11328734f1/metadata/00000-1a64cd01-00ad-46e9-b460-343e7c340e42.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=c50164e1f5a5e898a551f7524d571b9fbe0772afb87c2501a057e11026e855a3}'
24/09/14 17:05:10 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.gold.shipping_dim
24/09/14 17:05:10 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:05:10 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/gold/amazon_orders_silver_792634ca-d814-419a-a23c-8d07828b3447/metadata/00000-8e5268b9-718d-4ee2-acb7-88c8193625c8.gz.metadata.json
24/09/14 17:05:10 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:05:10 INFO BaseViewOperations: Refreshing view metadata from new version: s3://warehouse/pipeline/silver_amazon_orders_last_batch_e16917d8-c669-4f04-a941-309ba3456af8/metadata/00000-e52aa396-42a3-4a52-a148-e94cff578bc7.gz.metadata.json
24/09/14 17:05:10 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: s3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json
24/09/14 17:05:10 INFO NessieUtil: loadTableMetadata for 'silver.amazon_orders' from location 's3://warehouse/silver/amazon_orders_a04c8088-4f2d-4f7d-84c2-0fe0fe954bb0/metadata/00001-2b1cee35-2c99-48f8-8d60-ac855aea4ad6.metadata.json' at 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=c50164e1f5a5e898a551f7524d571b9fbe0772afb87c2501a057e11026e855a3}'
24/09/14 17:05:10 INFO BaseMetastoreCatalog: Table loaded by catalog: nessie.silver.amazon_orders
24/09/14 17:05:10 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}
24/09/14 17:05:10 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
24/09/14 17:05:10 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter true
24/09/14 17:05:10 INFO LoggingMetricsReporter: Received metrics report: ScanReport{tableName=nessie.silver.amazon_orders, snapshotId=6957663081544740559, filter=true, schemaId=0, projectedFieldIds=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], projectedFieldNames=[Order_ID, Order_Date, Order_Status, Fulfilment, ORDERS_Channel, ship_service_level, Category, Size, Courier_Status, Qty, Currency, Amount, Ship_City, Ship_State, Ship_Postal_Code, Ship_Country, B2B, Fulfilled_By, New, PendingS, Ingestion_Date], scanMetrics=ScanMetricsResult{totalPlanningDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.015207439S, count=1}, resultDataFiles=CounterResult{unit=COUNT, value=1}, resultDeleteFiles=CounterResult{unit=COUNT, value=0}, totalDataManifests=CounterResult{unit=COUNT, value=1}, totalDeleteManifests=CounterResult{unit=COUNT, value=0}, scannedDataManifests=CounterResult{unit=COUNT, value=1}, skippedDataManifests=CounterResult{unit=COUNT, value=0}, totalFileSizeInBytes=CounterResult{unit=BYTES, value=7309}, totalDeleteFileSizeInBytes=CounterResult{unit=BYTES, value=0}, skippedDataFiles=CounterResult{unit=COUNT, value=0}, skippedDeleteFiles=CounterResult{unit=COUNT, value=0}, scannedDeleteManifests=CounterResult{unit=COUNT, value=0}, skippedDeleteManifests=CounterResult{unit=COUNT, value=0}, indexedDeleteFiles=CounterResult{unit=COUNT, value=0}, equalityDeleteFiles=CounterResult{unit=COUNT, value=0}, positionalDeleteFiles=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82), app-id=local-1726332744679, engine-name=spark}}
24/09/14 17:05:10 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Aggregate Functions:
 MAX(Ingestion_Date)
Pushed Group by:
 
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.silver.amazon_orders
Pushed Filters: Ingestion_Date IS NOT NULL
Post-Scan Filters: isnotnull(Ingestion_Date#1092),(Ingestion_Date#1092 = scalar-subquery#1028 [])
         
24/09/14 17:05:11 INFO SparkScanBuilder: Evaluating completely on Iceberg side: full_date IS NOT NULL
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.date_dim
Pushed Filters: full_date IS NOT NULL
Post-Scan Filters: 
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.currency_dim
Pushed Filters: currency IS NOT NULL
Post-Scan Filters: isnotnull(currency#993)
         
24/09/14 17:05:11 INFO SparkScanBuilder: Evaluating completely on Iceberg side: ship_country IS NOT NULL
24/09/14 17:05:11 INFO SparkScanBuilder: Evaluating completely on Iceberg side: ship_state IS NOT NULL
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.location_dim
Pushed Filters: ship_country IS NOT NULL, ship_state IS NOT NULL, ship_city IS NOT NULL, ship_postal_code IS NOT NULL
Post-Scan Filters: isnotnull(ship_city#997),isnotnull(ship_postal_code#998)
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.product_dim
Pushed Filters: Category IS NOT NULL, Size IS NOT NULL
Post-Scan Filters: isnotnull(Category#1000),isnotnull(Size#1001)
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Pushing operators to nessie.gold.shipping_dim
Pushed Filters: shipping_status IS NOT NULL, Fulfilment IS NOT NULL, ship_service_level IS NOT NULL, fulfilled_by IS NOT NULL
Post-Scan Filters: isnotnull(shipping_status#1003),isnotnull(Fulfilment#1004),isnotnull(ship_service_level#1005),isnotnull(fulfilled_by#1006)
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Output: Order_ID#1072, Order_Date#1073, Order_Status#1074, Fulfilment#1075, ship_service_level#1077, Category#1078, Size#1079, Qty#1081, Currency#1082, Amount#1083, Ship_City#1084, Ship_State#1085, Ship_Postal_Code#1086, Ship_Country#1087, Fulfilled_By#1089, Ingestion_Date#1092
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Output: id#985, full_date#986
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Output: id#992, currency#993
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Output: id#994, ship_country#995, ship_state#996, ship_city#997, ship_postal_code#998
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Output: id#999, Category#1000, Size#1001
         
24/09/14 17:05:11 INFO V2ScanRelationPushDown: 
Output: id#1002, shipping_status#1003, Fulfilment#1004, ship_service_level#1005, fulfilled_by#1006
         
24/09/14 17:05:11 INFO SnapshotScan: Scanning table nessie.silver.amazon_orders snapshot 6957663081544740559 created at 2024-09-14T17:02:37.507+00:00 with filter Ingestion_Date IS NOT NULL
24/09/14 17:05:11 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.silver.amazon_orders
24/09/14 17:05:11 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.silver.amazon_orders
24/09/14 17:05:11 INFO SnapshotScan: Scanning table nessie.gold.date_dim snapshot 7220629113088947840 created at 2024-09-14T17:04:15.114+00:00 with filter full_date IS NOT NULL
24/09/14 17:05:11 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.date_dim
24/09/14 17:05:11 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.gold.date_dim
24/09/14 17:05:11 INFO SnapshotScan: Scanning table nessie.gold.currency_dim snapshot 1768952174985901472 created at 2024-09-14T17:04:03.786+00:00 with filter currency IS NOT NULL
24/09/14 17:05:11 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.currency_dim
24/09/14 17:05:11 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.gold.currency_dim
24/09/14 17:05:11 INFO SnapshotScan: Scanning table nessie.gold.location_dim snapshot 2603986048269305278 created at 2024-09-14T17:04:25.574+00:00 with filter (((ship_country IS NOT NULL AND ship_state IS NOT NULL) AND ship_city IS NOT NULL) AND ship_postal_code IS NOT NULL)
24/09/14 17:05:11 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.location_dim
24/09/14 17:05:11 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 4 partition(s) for table nessie.gold.location_dim
24/09/14 17:05:11 INFO SnapshotScan: Scanning table nessie.gold.product_dim snapshot 8090811787951346061 created at 2024-09-14T17:03:44.243+00:00 with filter (Category IS NOT NULL AND Size IS NOT NULL)
24/09/14 17:05:11 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.product_dim
24/09/14 17:05:11 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.gold.product_dim
24/09/14 17:05:11 INFO SnapshotScan: Scanning table nessie.gold.shipping_dim snapshot 8429945481820994034 created at 2024-09-14T17:03:53.889+00:00 with filter (((shipping_status IS NOT NULL AND Fulfilment IS NOT NULL) AND ship_service_level IS NOT NULL) AND fulfilled_by IS NOT NULL)
24/09/14 17:05:11 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.gold.shipping_dim
24/09/14 17:05:11 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.gold.shipping_dim
24/09/14 17:05:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table gold.fact_amazon_orders
24/09/14 17:05:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table gold.fact_amazon_orders
24/09/14 17:05:11 INFO SparkWrite: Requesting [] as write ordering for table gold.fact_amazon_orders
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.2 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 42 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 434.2 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.4 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 43 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 434.2 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 459adcdce09e:46053 (size: 3.4 KiB, free: 434.4 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 44 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 434.1 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.4 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 45 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 434.1 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 459adcdce09e:46053 (size: 3.4 KiB, free: 434.4 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 46 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 434.1 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.4 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 47 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.0 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 48 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 434.0 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.4 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 49 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 434.0 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.4 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 50 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 433.9 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 459adcdce09e:46053 (size: 3.4 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 51 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 433.9 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 52 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 433.9 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 53 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 433.8 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 54 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 433.8 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 459adcdce09e:46053 (size: 3.4 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 55 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 433.8 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 56 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 32.0 KiB, free 433.7 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 433.7 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 57 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 32.0 KiB, free 433.7 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 433.7 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 459adcdce09e:46053 (size: 3.4 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 58 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 32.0 KiB, free 433.7 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 433.7 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 59 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 32.0 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 459adcdce09e:46053 (size: 3.4 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 60 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 32.0 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 61 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:122)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:36)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	scala.collection.immutable.List.map(List.scala:293)
	org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:699)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:05:11 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.ShowTableExtended.mapChildren(v2Commands.scala:883)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 459adcdce09e:46053 in memory (size: 5.6 KiB, free: 434.3 MiB)
24/09/14 17:05:11 WARN S3FileIO: Unclosed S3FileIO instance created by:
	org.apache.iceberg.aws.s3.S3FileIO.initialize(S3FileIO.java:359)
	org.apache.iceberg.CatalogUtil.loadFileIO(CatalogUtil.java:350)
	org.apache.iceberg.nessie.NessieCatalog.initialize(NessieCatalog.java:132)
	org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:255)
	org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:309)
	org.apache.iceberg.spark.SparkCatalog.buildIcebergCatalog(SparkCatalog.java:154)
	org.apache.iceberg.spark.SparkCatalog.initialize(SparkCatalog.java:751)
	org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)
	org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$catalog$1(CatalogManager.scala:54)
	scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)
	org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:54)
	org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:86)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:51)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)
	org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)
	org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:549)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:32)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:30)
	org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:27)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)
	scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	scala.collection.immutable.List.foldLeft(List.scala:91)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)
	scala.collection.immutable.List.foreach(List.scala:431)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)
	org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:240)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:187)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:236)
	org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:202)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
	org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:223)
	org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:222)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)
	org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)
	org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
	org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)
	org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)
	org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
	org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
	org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)
	org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)
	org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)
	org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:227)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:166)
	scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
	org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:41)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:166)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:161)
	java.base/java.security.AccessController.doPrivileged(Native Method)
	java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:175)
	java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 32.0 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 62 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 32.0 KiB, free 433.5 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 433.5 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 63 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 459adcdce09e:46053 in memory (size: 3.2 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 32.0 KiB, free 433.5 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 433.5 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 64 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 32.0 KiB, free 433.5 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 3.5 KiB, free 433.5 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 459adcdce09e:46053 (size: 3.5 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 65 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 19.216478 ms
24/09/14 17:05:11 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 21.930926 ms
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 16.671938 ms
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 4.52383 ms
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 21.366586 ms
24/09/14 17:05:11 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression
24/09/14 17:05:11 WARN DataSourceV2Strategy: Can't translate true to source filter, unsupported expression
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 459adcdce09e:46053 in memory (size: 3.5 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO DAGScheduler: Got job 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:05:11 INFO DAGScheduler: Final stage: ResultStage 53 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:05:11 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:05:11 INFO DAGScheduler: Missing parents: List()
24/09/14 17:05:11 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[92] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 14.0 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 459adcdce09e:46053 (size: 5.6 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1585
24/09/14 17:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[92] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:05:11 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0
24/09/14 17:05:11 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 27) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 15042 bytes) 
24/09/14 17:05:11 INFO Executor: Running task 0.0 in stage 53.0 (TID 27)
24/09/14 17:05:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO DAGScheduler: Got job 28 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:05:11 INFO DAGScheduler: Final stage: ResultStage 54 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:05:11 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:05:11 INFO DAGScheduler: Missing parents: List()
24/09/14 17:05:11 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[100] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 14.3 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 459adcdce09e:46053 in memory (size: 3.5 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 459adcdce09e:46053 (size: 5.7 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1585
24/09/14 17:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[100] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:05:11 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0
24/09/14 17:05:11 INFO DAGScheduler: Got job 29 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:05:11 INFO DAGScheduler: Final stage: ResultStage 55 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:05:11 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:05:11 INFO DAGScheduler: Missing parents: List()
24/09/14 17:05:11 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[103] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 15.9 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 459adcdce09e:46053 (size: 5.9 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1585
24/09/14 17:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[103] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:05:11 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 15.772918 ms
24/09/14 17:05:11 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 28) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 14005 bytes) 
24/09/14 17:05:11 INFO DAGScheduler: Got job 30 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:05:11 INFO DAGScheduler: Final stage: ResultStage 56 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:05:11 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:05:11 INFO DAGScheduler: Missing parents: List()
24/09/14 17:05:11 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[104] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:05:11 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 29) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 14617 bytes) 
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 14.8 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.6 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 459adcdce09e:46053 (size: 5.7 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585
24/09/14 17:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[104] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:05:11 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0
24/09/14 17:05:11 INFO Executor: Running task 0.0 in stage 54.0 (TID 28)
24/09/14 17:05:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO DAGScheduler: Got job 31 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 4 output partitions
24/09/14 17:05:11 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 30) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 14225 bytes) 
24/09/14 17:05:11 INFO DAGScheduler: Final stage: ResultStage 57 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:05:11 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:05:11 INFO DAGScheduler: Missing parents: List()
24/09/14 17:05:11 INFO Executor: Running task 0.0 in stage 55.0 (TID 29)
24/09/14 17:05:11 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[108] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 15.9 KiB, free 433.5 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.5 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 459adcdce09e:46053 (size: 5.9 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1585
24/09/14 17:05:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 57 (MapPartitionsRDD[108] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
24/09/14 17:05:11 INFO TaskSchedulerImpl: Adding task set 57.0 with 4 tasks resource profile 0
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 5.943117 ms
24/09/14 17:05:11 INFO Executor: Running task 0.0 in stage 56.0 (TID 30)
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 7.08105 ms
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO Executor: Finished task 0.0 in stage 53.0 (TID 27). 4461 bytes result sent to driver
24/09/14 17:05:11 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 31) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 16618 bytes) 
24/09/14 17:05:11 INFO Executor: Running task 0.0 in stage 57.0 (TID 31)
24/09/14 17:05:11 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 27) in 73 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:05:11 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
24/09/14 17:05:11 INFO DAGScheduler: ResultStage 53 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.078 s
24/09/14 17:05:11 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:05:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
24/09/14 17:05:11 INFO DAGScheduler: Job 27 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.081774 s
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 13.768958 ms
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO Executor: Finished task 0.0 in stage 54.0 (TID 28). 4526 bytes result sent to driver
24/09/14 17:05:11 INFO TaskSetManager: Starting task 1.0 in stage 57.0 (TID 32) (459adcdce09e, executor driver, partition 1, PROCESS_LOCAL, 16637 bytes) 
24/09/14 17:05:11 INFO Executor: Running task 1.0 in stage 57.0 (TID 32)
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 28) in 78 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:05:11 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
24/09/14 17:05:11 INFO DAGScheduler: ResultStage 54 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.094 s
24/09/14 17:05:11 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:05:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
24/09/14 17:05:11 INFO DAGScheduler: Job 28 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.096389 s
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 26.786385 ms
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 3.812743 ms
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 8.749004 ms
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 4.0 MiB, free 429.5 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 4.0 MiB, free 425.5 MiB)
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 179.0 B, free 425.5 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 459adcdce09e:46053 (size: 179.0 B, free: 434.3 MiB)
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO SparkContext: Created broadcast 72 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 192.0 B, free 425.5 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 459adcdce09e:46053 (size: 192.0 B, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 71 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO Executor: Finished task 0.0 in stage 55.0 (TID 29). 4726 bytes result sent to driver
24/09/14 17:05:11 INFO TaskSetManager: Starting task 2.0 in stage 57.0 (TID 33) (459adcdce09e, executor driver, partition 2, PROCESS_LOCAL, 16655 bytes) 
24/09/14 17:05:11 INFO Executor: Running task 2.0 in stage 57.0 (TID 33)
24/09/14 17:05:11 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 29) in 120 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:05:11 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool 
24/09/14 17:05:11 INFO DAGScheduler: ResultStage 55 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.130 s
24/09/14 17:05:11 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:05:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished
24/09/14 17:05:11 INFO DAGScheduler: Job 29 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.139431 s
24/09/14 17:05:11 INFO Executor: Finished task 0.0 in stage 56.0 (TID 30). 4729 bytes result sent to driver
24/09/14 17:05:11 INFO TaskSetManager: Starting task 3.0 in stage 57.0 (TID 34) (459adcdce09e, executor driver, partition 3, PROCESS_LOCAL, 16284 bytes) 
24/09/14 17:05:11 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 30) in 118 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:05:11 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
24/09/14 17:05:11 INFO DAGScheduler: ResultStage 56 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.127 s
24/09/14 17:05:11 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:05:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
24/09/14 17:05:11 INFO DAGScheduler: Job 30 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.145459 s
24/09/14 17:05:11 INFO Executor: Running task 3.0 in stage 57.0 (TID 34)
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 4.624779 ms
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 13.626542 ms
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 32.0 KiB, free 425.5 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 421.5 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 4.0 MiB, free 421.5 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 520.0 B, free 417.5 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 4.0 MiB, free 417.5 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 451.0 B, free 417.5 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 73 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 459adcdce09e:46053 (size: 520.0 B, free: 434.3 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 459adcdce09e:46053 (size: 451.0 B, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 74 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO SparkContext: Created broadcast 75 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 32.0 KiB, free 417.5 MiB)
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 417.5 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 76 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 32.0 KiB, free 417.4 MiB)
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 417.4 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO Executor: Finished task 0.0 in stage 57.0 (TID 31). 4712 bytes result sent to driver
24/09/14 17:05:11 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 31) in 180 ms on 459adcdce09e (executor driver) (1/4)
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO SparkContext: Created broadcast 77 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO Executor: Finished task 3.0 in stage 57.0 (TID 34). 4662 bytes result sent to driver
24/09/14 17:05:11 INFO TaskSetManager: Finished task 3.0 in stage 57.0 (TID 34) in 106 ms on 459adcdce09e (executor driver) (2/4)
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO Executor: Finished task 1.0 in stage 57.0 (TID 32). 4731 bytes result sent to driver
24/09/14 17:05:11 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:11 INFO TaskSetManager: Finished task 1.0 in stage 57.0 (TID 32) in 182 ms on 459adcdce09e (executor driver) (3/4)
24/09/14 17:05:11 INFO Executor: Finished task 2.0 in stage 57.0 (TID 33). 4871 bytes result sent to driver
24/09/14 17:05:11 INFO TaskSetManager: Finished task 2.0 in stage 57.0 (TID 33) in 130 ms on 459adcdce09e (executor driver) (4/4)
24/09/14 17:05:11 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
24/09/14 17:05:11 INFO DAGScheduler: ResultStage 57 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.229 s
24/09/14 17:05:11 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:05:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
24/09/14 17:05:11 INFO DAGScheduler: Job 31 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.236557 s
24/09/14 17:05:11 INFO CodeGenerator: Code generated in 4.465621 ms
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 4.0 MiB, free 413.4 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 1469.0 B, free 413.4 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 459adcdce09e:46053 (size: 1469.0 B, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 78 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 32.0 KiB, free 413.4 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 413.4 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 459adcdce09e:46053 (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 79 from broadcast at SparkBatch.java:79
24/09/14 17:05:11 INFO DAGScheduler: Registering RDD 111 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) as input to shuffle 17
24/09/14 17:05:11 INFO DAGScheduler: Got map stage job 32 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:05:11 INFO DAGScheduler: Final stage: ShuffleMapStage 58 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:05:11 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:05:11 INFO DAGScheduler: Missing parents: List()
24/09/14 17:05:11 INFO DAGScheduler: Submitting ShuffleMapStage 58 (MapPartitionsRDD[111] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 12.0 KiB, free 413.4 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 413.4 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 459adcdce09e:46053 (size: 6.0 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1585
24/09/14 17:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 58 (MapPartitionsRDD[111] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:05:11 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0
24/09/14 17:05:11 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 35) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 9763 bytes) 
24/09/14 17:05:11 INFO Executor: Running task 0.0 in stage 58.0 (TID 35)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 459adcdce09e:46053 in memory (size: 5.9 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO Executor: Finished task 0.0 in stage 58.0 (TID 35). 1885 bytes result sent to driver
24/09/14 17:05:11 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 35) in 18 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:05:11 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool 
24/09/14 17:05:11 INFO DAGScheduler: ShuffleMapStage 58 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.025 s
24/09/14 17:05:11 INFO DAGScheduler: looking for newly runnable stages
24/09/14 17:05:11 INFO DAGScheduler: running: Set()
24/09/14 17:05:11 INFO DAGScheduler: waiting: Set()
24/09/14 17:05:11 INFO DAGScheduler: failed: Set()
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 459adcdce09e:46053 in memory (size: 5.9 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
24/09/14 17:05:11 INFO DAGScheduler: Got job 33 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
24/09/14 17:05:11 INFO DAGScheduler: Final stage: ResultStage 60 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
24/09/14 17:05:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 59)
24/09/14 17:05:11 INFO DAGScheduler: Missing parents: List()
24/09/14 17:05:11 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[114] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 13.0 KiB, free 413.4 MiB)
24/09/14 17:05:11 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 413.4 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 459adcdce09e:46053 (size: 6.1 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1585
24/09/14 17:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[114] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
24/09/14 17:05:11 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0
24/09/14 17:05:11 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 36) (459adcdce09e, executor driver, partition 0, NODE_LOCAL, 9602 bytes) 
24/09/14 17:05:11 INFO Executor: Running task 0.0 in stage 60.0 (TID 36)
24/09/14 17:05:11 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/09/14 17:05:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/09/14 17:05:11 INFO Executor: Finished task 0.0 in stage 60.0 (TID 36). 3959 bytes result sent to driver
24/09/14 17:05:11 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 36) in 6 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:05:11 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
24/09/14 17:05:11 INFO DAGScheduler: ResultStage 60 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.012 s
24/09/14 17:05:11 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:05:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
24/09/14 17:05:11 INFO DAGScheduler: Job 33 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.019482 s
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 459adcdce09e:46053 in memory (size: 5.6 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 459adcdce09e:46053 in memory (size: 5.7 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 459adcdce09e:46053 in memory (size: 5.7 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:11 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 459adcdce09e:46053 in memory (size: 3.8 KiB, free: 434.3 MiB)
24/09/14 17:05:12 INFO CodeGenerator: Code generated in 20.230684 ms
24/09/14 17:05:12 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 32.0 KiB, free 413.6 MiB)
24/09/14 17:05:12 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 3.2 KiB, free 413.6 MiB)
24/09/14 17:05:12 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 459adcdce09e:46053 (size: 3.2 KiB, free: 434.3 MiB)
24/09/14 17:05:12 INFO SparkContext: Created broadcast 82 from broadcast at SparkWrite.java:193
24/09/14 17:05:12 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=gold.fact_amazon_orders, format=PARQUET). The input RDD has 1 partitions.
24/09/14 17:05:12 INFO SparkContext: Starting job: run at AccessController.java:0
24/09/14 17:05:12 INFO DAGScheduler: Got job 34 (run at AccessController.java:0) with 1 output partitions
24/09/14 17:05:12 INFO DAGScheduler: Final stage: ResultStage 61 (run at AccessController.java:0)
24/09/14 17:05:12 INFO DAGScheduler: Parents of final stage: List()
24/09/14 17:05:12 INFO DAGScheduler: Missing parents: List()
24/09/14 17:05:12 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[117] at run at AccessController.java:0), which has no missing parents
24/09/14 17:05:12 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 34.8 KiB, free 413.5 MiB)
24/09/14 17:05:12 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 10.9 KiB, free 413.5 MiB)
24/09/14 17:05:12 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 459adcdce09e:46053 (size: 10.9 KiB, free: 434.3 MiB)
24/09/14 17:05:12 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1585
24/09/14 17:05:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[117] at run at AccessController.java:0) (first 15 tasks are for partitions Vector(0))
24/09/14 17:05:12 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks resource profile 0
24/09/14 17:05:12 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 37) (459adcdce09e, executor driver, partition 0, PROCESS_LOCAL, 17325 bytes) 
24/09/14 17:05:12 INFO Executor: Running task 0.0 in stage 61.0 (TID 37)
24/09/14 17:05:12 INFO CodeGenerator: Code generated in 19.453574 ms
24/09/14 17:05:12 INFO CodecPool: Got brand-new compressor [.zstd]
24/09/14 17:05:12 INFO CodecPool: Got brand-new decompressor [.zstd]
24/09/14 17:05:12 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/09/14 17:05:12 INFO DataWritingSparkTask: Committed partition 0 (task 37, attempt 0, stage 61.0)
24/09/14 17:05:12 INFO Executor: Finished task 0.0 in stage 61.0 (TID 37). 7729 bytes result sent to driver
24/09/14 17:05:12 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 37) in 123 ms on 459adcdce09e (executor driver) (1/1)
24/09/14 17:05:12 INFO DAGScheduler: ResultStage 61 (run at AccessController.java:0) finished in 0.127 s
24/09/14 17:05:12 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
24/09/14 17:05:12 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool 
24/09/14 17:05:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished
24/09/14 17:05:12 INFO DAGScheduler: Job 34 finished: run at AccessController.java:0, took 0.130498 s
24/09/14 17:05:12 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.fact_amazon_orders, format=PARQUET) is committing.
24/09/14 17:05:12 INFO SparkWrite: Committing append with 1 new data files to table gold.fact_amazon_orders
24/09/14 17:05:12 INFO SnapshotProducer: Committed snapshot 5271531706074489737 (MergeAppend)
24/09/14 17:05:12 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=gold.fact_amazon_orders, snapshotId=5271531706074489737, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.079874516S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=19}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=19}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2787}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=2787}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.2, app-id=local-1726332744679, engine-name=spark, iceberg-version=Apache Iceberg 1.5.2 (commit cbb853073e681b4075d7c8707610dceecbee3a82)}}
24/09/14 17:05:12 INFO SparkWrite: Committed in 81 ms
24/09/14 17:05:12 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=gold.fact_amazon_orders, format=PARQUET) committed.
24/09/14 17:05:12 INFO NessieIcebergClient: Committed 'gold.fact_amazon_orders' against 'Branch{name=amazon_pipeline_sampled_data_1_20240914165817, metadata=null, hash=f8f64e9852f67b17f3af979ac6bd4ab52e675f66c7f327dce031903e1559a9a4}', expected commit-id was 'c50164e1f5a5e898a551f7524d571b9fbe0772afb87c2501a057e11026e855a3'
24/09/14 17:05:12 INFO BaseMetastoreTableOperations: Successfully committed to table gold.fact_amazon_orders in 23 ms
24/09/14 17:05:12 INFO DAGScheduler: Asked to cancel job group ea1cdca3-ab17-46cf-bc25-e5cb8cf24217
24/09/14 17:05:12 INFO SparkExecuteStatementOperation: Close statement with ea1cdca3-ab17-46cf-bc25-e5cb8cf24217
24/09/14 17:05:12 INFO DAGScheduler: Asked to cancel job group 55b71887-cea8-40ea-bdd9-b458df373665
24/09/14 17:05:12 INFO SparkExecuteStatementOperation: Close statement with 55b71887-cea8-40ea-bdd9-b458df373665
24/09/14 17:05:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:12 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:05:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:05:12 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:05:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:05:12 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/09/14 17:05:12 INFO ThriftCLIService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
24/09/14 17:05:12 INFO HiveSessionImpl: Operation log session directory is created: /tmp/me/operation_logs/661ccfd2-a4f0-4571-a0e7-a70ddd9de50c
24/09/14 17:05:12 INFO SparkExecuteStatementOperation: Submitting query 'USE `default`' with 548af5fd-9edf-4a8e-95e2-37f2b94c0071
24/09/14 17:05:12 INFO SparkExecuteStatementOperation: Running query with 548af5fd-9edf-4a8e-95e2-37f2b94c0071
24/09/14 17:05:12 INFO HiveMetaStore: 5: get_database: default
24/09/14 17:05:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=get_database: default	
24/09/14 17:05:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
24/09/14 17:05:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
24/09/14 17:05:12 INFO HiveMetaStore: 5: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
24/09/14 17:05:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:12 INFO DAGScheduler: Asked to cancel job group 548af5fd-9edf-4a8e-95e2-37f2b94c0071
24/09/14 17:05:12 INFO SparkExecuteStatementOperation: Close statement with 548af5fd-9edf-4a8e-95e2-37f2b94c0071
24/09/14 17:05:12 INFO ObjectStore: ObjectStore, initialize called
24/09/14 17:05:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
24/09/14 17:05:12 INFO ObjectStore: Initialized ObjectStore
24/09/14 17:05:12 INFO HiveMetaStore: 5: Cleaning up thread local RawStore...
24/09/14 17:05:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Cleaning up thread local RawStore...	
24/09/14 17:05:12 INFO HiveMetaStore: 5: Done cleaning up thread local RawStore
24/09/14 17:05:12 INFO audit: ugi=default	ip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
24/09/14 17:05:12 ERROR TThreadPoolServer: Thrift error occurred during processing of message.
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374)
	at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451)
	at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:433)
	at org.apache.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:52)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
